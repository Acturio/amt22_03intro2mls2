[["index.html", "AMAT- Ciencia de Datos y Machine Learning 2 BIENVENIDA Objetivo Instructores Ciencia de Datos en R Estructura del curso actual Duración y evaluación del curso Recursos y dinámica de clase", " AMAT- Ciencia de Datos y Machine Learning 2 Karina Lizette Gamboa Puente Oscar Arturo Bringas López BIENVENIDA Objetivo Desarrollar conocimiento y habilidades para implementar modelos complejos de Machine Learning a través de un flujo de trabajo limpio, ordenado y sistematizado a mediante las librerías en R más novedosas que han sido desarrolladas hasta el momento. Al finalizar este curso, el participante será capáz de combinar distintas clases de modelos para dar una solución compleja y precisa a problemas predictivos. Aprenderá a cuantificar los problemas éticos asociados al sesgo o inequidad producidos por modelos de machine learning, así como su interpretación en el mundo productivo. Finalmente, se estudiará la manera de desarrollar un diseño de experimento para implementarse en el ámbito empresarial de modo que el participante pueda tomar mejores decisiones para contribuir en su ambiente laboral. Se asume que el alumno tiene conocimientos generales de estadística, bases matemáticas y de programación básica en R y que cuenta con los conocimientos teóricos básicos de machine learning y prácticos con tidymodels. Instructores ACT. ARTURO BRINGAS LinkedIn: arturo-bringas Email: act.arturo.b@ciencias.unam.mx Actuario, egresado de la Facultad de Ciencias y Maestría en Ciencia de Datos, ITAM. Experiencia en modelos predictivos y de clasificación de machine learning aplicado a seguros, deportes y movilidad internacional. Es jefe de departamento en Investigación Aplicada y Opinión de la UNAM, donde realiza estudios estadísticos de impacto social. Es consultor para empresas y organizaciones como GNP, El Universal, UNAM, Sinnia, la Organización de las Naciones Unidas Contra la Droga y el Delito (UNODC), entre otros. Actualmente es profesor de ciencia de datos y machine learning en AMAT y se desempeña como consultor independiente en diferentes proyectos contribuyendo a empresas en temas de machine learning, estadística, series de tiempo, visualización de datos y análisis geoespacial. ACT. KARINA LIZETTE GAMBOA LinkedIn: KaLizzyGam Email: lizzygamboa@ciencias.unam.mx Actuaria, egresada de la Facultad de Ciencias, UNAM, candidata a Maestra en Ciencia de Datos por el ITAM. Experiencia en áreas de analítica predictiva e inteligencia del negocio. Lead y Senior Data Scientist en consultoría en diferentes sectores como tecnología, asegurador, financiero y bancario. Experta en entendimiento de negocio para la correcta implementación de algoritmos de inteligencia y explotación de datos. Actualmente se desarrolla como Arquitecta de Soluciones Analíticas en Merama, startup mexicana clasificada como uno de los nuevos unicornios de Latinoamérica. Senior Data Science en CLOSTER y como profesora del diplomado de Metodología de la Investigación Social por la UNAM así como instructora de cursos de Ciencia de Datos en AMAT. Empresas anteriores: GNP, Activer Banco y Casa de Bolsa, PlayCity Casinos, RakenDataGroup Consulting, entre otros. Ciencia de Datos en R Estructura del curso actual Alcances del curso Al finalizar el módulo, el participante sabrá plantear un proyecto de ciencia de datos, desde sus requerimientos hasta sus implementación comercial. Sabrá crear flujos de trabajo limpios y ordenados para crear poderosos modelos de Machine Learning. Podrá comparar múltiples modelos y seleccionar el que más aportación realice a su negocio considerando la ética alrededor del sesgo e inequidad producida por modelos. Profundizará su conocimiento en la interpretación de modelos complejos y aprenderá a cuantificar el beneficio comercial de la implementación de modelos. Requisitos: Computadora con al menos 4Gb Ram. Instalación de R con al menos versión 4.1.0 Instalación de Rstudio con al menos versión 1.4 Data Science &amp; Machine Learning (Aprendizaje Supervisado I) Temario: 1.- Machine Learning (10 HRS) Regresión polinomial Regresión con CPA Imputación SVM Boosting 2. Flujos de trabajo y ensamblajes (8 HRS) Pipelines Workflowsets Comparación de modelos Stacking 3. Sesgo e inequidad de modelos (4 HRS) Cuantificación de sesgo Cuantificación de inequidad 4. Interpretación de modelos (4 HRS) LIME DALExtra 5. Aplicación a negocios (6 HRS) Diseño de experimentos en campañas de retención Valuación de implementación de modelos Duración y evaluación del curso El programa tiene una duración de 32 hrs. Las clases serán impartidas los días domingo, de 9:00 am a 1:00 pm Serán asignados ejercicios que el participante deberá resolver entre una semana y otra. Al final del curso se solicitará un proyecto final, el cual deberá ser entregado para ser acreedor a la constancia de participación. Recursos y dinámica de clase En esta clase estaremos usando: R da click aquí si aún no lo descargas RStudio da click aquí también Zoom Clases Pulgar arriba: Voy bien, estoy entendiendo! Pulgar abajo: Eso no quedó muy claro Mano arriba: Quiero participar/preguntar ó Ya estoy listo para iniciar Grupo de WhatsApp El chismecito está aquí Google Drive Notas de clase Revisame si quieres aprender Finalmente, se dejarán ejercicios que serán clave para el éxito del aprendizaje de los capítulos, por lo que se trabajará en equipo para lograr adquirir el mayor aprendizaje. En este grupo se trabajará en equipos de 3 y 4 personas para enriquecer el aprendizaje y participación de todos. "],["repaso.html", "Capítulo 1 Repaso 1.1 Machine Learning 1.2 Tipos de aprendizaje 1.3 Errores: Sesgo vs varianza 1.4 Partición de datos 1.5 Recetas y tratamiento de datos", " Capítulo 1 Repaso En los cursos anteriores, hemos hablando a cerca del proceso completo de Ciencia de Datos, para poder empezar con la segunda parte del curso de Analisis Supervisado, valele la pena hacer un breve repaso de lo que hemos estudiado hasta el momento. 1.1 Machine Learning Machine Learning o –aprendizaje automático– es una rama de la inteligencia artificial que permite que las máquinas aprendan de los patrones existentes en los datos. Se usan métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. (Está enfocado en la programación de máquinas para aprender de los patrones existentes en datos principalmente estructurados y anticiparse al futuro) 1.2 Tipos de aprendizaje Platicamos en el módulo pasado que al hablar de Machine Learning, existen distintos tipos de aprendizaje, siendo los más comúnes: Aprendizaje supervisado Aprendizaje no supervisado Otreos ejemplos de especialidades son Aprendizaje profundo Aprendizaje por refuerzo La diferencia entre el análisis supervisado y el no supervisado es la etiqueta, es decir, en el análisis supervisado tenemos una etiqueta “correcta” y el objetivo de los algoritmos es predecir esta etiqueta. 1.2.1 Aprendizaje supervisado Conocemos la respuesta correcta de antemano. Esta respuesta correcta fue “etiquetada” por un humano (la mayoría de las veces, en algunas circunstancias puede ser generada por otro algoritmo). Debido a que conocemos la respuesta correcta, existen muchas métricas de desempeño del modelo para verificar que nuestro algoritmo está haciendo las cosas “bien.” 1.2.1.1 Tipos de aprendizaje supervisado (Regresión vs clasificación) Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Los algoritmos de clasificación se usan cuando el resultado deseado es una etiqueta discreta, es decir, clasifican un elemento dentro de diversas clases. En un problema de regresión, la variable target o variable a predecir es un valor numérico. 1.2.2 Aprendizaje no supervisado Aquí no tenemos la respuesta correcta de antemano ¿cómo podemos saber que el algoritmo está bien o mal? Estadísticamente podemos verificar que el algoritmo está bien Siempre tenemos que verificar con el cliente si los resultados que estamos obteniendo tienen sentido de negocio. Por ejemplo, número de grupos y características 1.3 Errores: Sesgo vs varianza En el mundo de Machine Learning cuando desarrollamos un modelo nos esforzamos para hacer que sea lo más preciso, ajustando los parámetros, pero la realidad es que no se puede construir un modelo 100% preciso ya que nunca pueden estar libres de errores. Comprender cómo las diferentes fuentes de error generan sesgo y varianza nos ayudará a mejorar el proceso de ajuste de datos, lo que resulta en modelos más precisos, adicionalmente también evitará el error de sobreajuste y falta de ajuste. Error por sesgo: Es la diferencia entre la predicción esperada de nuestro modelo y los valores verdaderos. Aunque al final nuestro objetivo es siempre construir modelos que puedan predecir datos muy cercanos a los valores verdaderos, no siempre es tan fácil porque algunos algoritmos son simplemente demasiado rígidos para aprender señales complejas del conjunto de datos. Imagina ajustar una regresión lineal a un conjunto de datos que tiene un patrón no lineal, no importa cuántas observaciones más recopiles, una regresión lineal no podrá modelar las curvas en esos datos. Esto se conoce como underfitting. Error por varianza: Se refiere a la cantidad que la estimación de la función objetivo cambiará si se utiliza diferentes datos de entrenamiento. La función objetivo se estima a partir de los datos de entrenamiento mediante un algoritmo de Machine Learning, por lo que deberíamos esperar que el algoritmo tenga alguna variación. Idealmente no debería cambiar demasiado de un conjunto de datos de entrenamiento a otro. Los algoritmos de Machine Learning que tienen una gran varianza están fuertemente influenciados por los detalles de los datos de entrenamiento, esto significa que los detalles de la capacitación influyen en el número y los tipos de parámetros utilizados para caracterizar la función de mapeo. Error irreducible: El error irreducible no se puede reducir, independientemente de qué algoritmo se usa. También se le conoce como ruido y, por lo general, proviene por factores como variables desconocidas que influyen en el mapeo de las variables de entrada a la variable de salida, un conjunto de características incompleto o un problema mal enmarcado. Acá es importante comprender que no importa cuán bueno hagamos nuestro modelo, nuestros datos tendrán cierta cantidad de ruido o un error irreductible que no se puede eliminar. 1.4 Partición de datos Cuando hay una gran cantidad de datos disponibles, una estrategia inteligente es asignar subconjuntos específicos de datos para diferentes tareas, en lugar de asignar la mayor cantidad posible solo a la estimación de los parámetros del modelo. Si el conjunto inicial de datos no es lo suficientemente grande, habrá cierta superposición de cómo y cuándo se asignan nuestros datos, y es importante contar con una metodología sólida para la partición de datos. 1.4.1 Métodos comunes para particionar datos El enfoque principal para la validación del modelo es dividir el conjunto de datos existente en dos conjuntos distintos: Entrenamiento: Este conjunto suele contener la mayoría de los datos, los cuales sirven para la construcción de modelos donde se pueden ajustar diferentes modelos, se investigan estrategias de ingeniería de características, etc. La mayor parte del proceso de modelado se utiliza este conjunto. Prueba: La otra parte de las observaciones se coloca en este conjunto. Estos datos se mantienen en reserva hasta que se elijan uno o dos modelos como los de mejor rendimiento. El conjunto de prueba se utiliza como árbitro final para determinar la eficiencia del modelo, por lo que es fundamental mirar el conjunto de prueba una sola vez. Supongamos que asignamos el \\(80\\%\\) de los datos al conjunto de entrenamiento y el \\(20\\%\\) restante a las pruebas. El método más común es utilizar un muestreo aleatorio simple. El paquete rsample tiene herramientas para realizar divisiones de datos como esta; la función initial_split() fue creada para este propósito. library(tidymodels) data(ames) tidymodels_prefer() # Fijar un número aleatorio con para que los resultados puedan ser reproducibles set.seed(123) # Partición 80/20 de los datos ames_split &lt;- initial_split(ames, prop = 0.80) ames_split ## &lt;Analysis/Assess/Total&gt; ## &lt;2344/586/2930&gt; La información impresa denota la cantidad de datos en el conjunto de entrenamiento \\((n = 2,344)\\), la cantidad en el conjunto de prueba \\((n = 586)\\) y el tamaño del grupo original de muestras \\((n = 2,930)\\). El objeto ames_split es un objeto rsplit y solo contiene la información de partición; para obtener los conjuntos de datos resultantes, aplicamos dos funciones más: ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) dim(ames_train) ## [1] 2344 74 No hay un porcentaje de división óptimo para el conjunto de entrenamiento y prueba. Los porcentajes de división más comunes comunes son: Entrenamiento: \\(80\\%\\), Prueba: \\(20\\%\\) Entrenamiento: \\(67\\%\\), Prueba: \\(33\\%\\) Entrenamiento: \\(50\\%\\), Prueba: \\(50\\%\\) 1.4.2 Conjunto de validación El conjunto de validación se definió originalmente cuando los investigadores se dieron cuenta de que medir el rendimiento del conjunto de entrenamiento conducía a resultados que eran demasiado optimistas. Esto llevó a modelos que se sobreajustaban, lo que significa que se desempeñaron muy bien en el conjunto de entrenamiento pero mal en el conjunto de prueba. Para combatir este problema, se retuvo un pequeño conjunto de datos de validación y se utilizó para medir el rendimiento del modelo mientras este está siendo entrenado. Una vez que la tasa de error del conjunto de validación comenzara a aumentar, la capacitación se detendría. En otras palabras, el conjunto de validación es un medio para tener una idea aproximada de qué tan bien se desempeñó el modelo antes del conjunto de prueba. Por otra parte esta primera particíón de datos evolucionó hasta la manera en que usualmente se hacen con más de una validación: 1.5 Recetas y tratamiento de datos La ingenería de datos y procesamiento de datos es parte vital del desarrollo de un buen modelo. En este curso analizaremos distintos métodos de machine learning que permitirán predecir una respuesta numérica o categórica. Usaremos el lenguaje de programación R para dicho procesamiento. Hay varios pasos que se deben de seguir para crear un modelo útil: Recopilación de datos. Limpieza de datos. Creación de nuevas variables. Estimación de parámetros. Selección y ajuste del modelo. Evaluación del rendimiento. Al comienzo de un proyecto, generalmente hay un conjunto finito de datos disponibles para todas estas tareas. OJO: A medida que los datos se reutilizan para múltiples tareas, aumentan los riesgos de agregar sesgos o grandes efectos de errores metodológicos. 1.5.1 Pre-procesamiento de datos Como punto de partida para nuestro flujo de trabajo de aprendizaje automático, necesitaremos datos de entrada. En la mayoría de los casos, estos datos se cargarán y almacenarán en forma de data frames o tibbles en R. Incluirán una o varias variables predictoras y, en caso de aprendizaje supervisado, también incluirán un resultado conocido. Sin embargo, no todos los modelos pueden lidiar con diferentes problemas de datos y, a menudo, necesitamos transformar los datos para obtener el mejor rendimiento posible del modelo. Este proceso se denomina pre-procesamiento y puede incluir una amplia gama de pasos, como: Dicotomización de variables Near Zero Value (nzv) o Varianza Cero Imputaciones Des-correlacionar Normalizar Transformar Creación de nuevas variables Interacciones En la tabla, \\(\\checkmark\\) indica que el método es obligatorio para el modelo y \\(\\times\\) indica que no lo es. El símbolo \\(\\circ\\) significa que la técnica puede ayudar al modelo, pero no es obligatorio. 1.5.2 Recetas Una receta es una serie de pasos o instrucciones para el procesamiento de datos. A diferencia del método de fórmula dentro de una función de modelado, la receta define los pasos sin ejecutarlos inmediatamente; es sólo una especificación de lo que se debe hacer. La estructura de una receta sigue los siguientes pasos: Inicialización recipe() Transformación step_[...]() Preparación prep() Aplicación bake(), juice() La siguiente sección explica la estructura y flujo de transformaciones: receta &lt;- recipe(response ~ X1 + X2 + X3 + ... + Xn, data = dataset ) %&gt;% step_1(...) %&gt;% step_2(...) %&gt;% step_3(...) %&gt;% ... step_4(...) %&gt;% prep() receta_aplicacion &lt;- bake(receta, new_data = new_dataset) juice(receta_aplicacion) 1.5.2.1 Transformaciones generales En cuanto a las transformaciones posibles, existe una gran cantidad de funciones que soportan este proceso. En esta sección se muestran algunas de las transformación más comunes y aquí La guía completa de las familia de funciones step : step_select(): Selecciona un subconjunto de variables específicas en el conjunto de datos. step_mutate(): Crea una nueva variable o modifica una existente usando dplyr::mutate(). step_mutate_at(): Lee una especificación de un paso de receta que modificará las variables seleccionadas usando una función común a través de dplyr::mutate_at(). step_filter(): Crea una especificación de un paso de receta que eliminará filas usando dplyr::filter(). step_arrange(): Ordena el conjunto de datos de acuerdo con una o más variables. step_rm(): Crea una especificación de un paso de receta que eliminará las variables según su nombre, tipo o función. step_nzv(): Realiza una selección de variables eliminando todas aquellas cuya varianza se encuentre cercana a cero. step_naomit(): Elimina todos los renglones que tengan alguna variable con valores perdidos. step_normalize(): Centra y escala las variables numéricas especificadas, generando una transformación a una distribución normal estándar. step_range(): Transforma el rango de un conjunto de variables numéricas al especificado. step_interact(): Crea un nuevo conjunto de variables basadas en la interacción entre dos variables. step_ratio(): Crea una nueva variable a partir del cociente entre dos variables. all_predictors(): Selecciona a todos los predictores del conjunto de entrenamineto para aplicarles alguna de las funciones mencionadas. all_numeric_predictors(): Selecciona a todos los predictores numéricos del conjunto de entrenamineto para aplicarles alguna de las funciones mencionadas. all_nominal_predictors(): Selecciona a todos los predictores nominales del conjunto de entrenamineto para aplicarles alguna de las funciones mencionadas. "],["feature-engineering.html", "Capítulo 2 Feature Engineering 2.1 Regresión polinomial 2.2 Análisis de Componentes Principales 2.3 Imputación KNN 2.4 Ejercicios", " Capítulo 2 Feature Engineering 2.1 Regresión polinomial Es común encontrar en la literatura este tema junto con la regresión múltiple, sin embargo, el alcance de esta transformación va a más allá de la regresión lineal, por lo que estudiaremos este tema como parte de la ingeniería de variables y no como un modelo lineal exclusivamente. El objetivo de la transformación polinomial dentro de la ingeniería de variables es crear nuevas variables que puedan explicar la relación entre la variable de respuesta y explicativa a través de un polinomio de grado k. En el siguiente gráfico se representa mediante una línea roja a la regresión lineal y mediante una curva azul al polinomio que relaciona a las variables independiente y dependiente. Es evidente que existe un mejor ajuste cuando se considera un polinomio de grado k en vez de la componente lineal. Este método ofrece mayor flexibilidad para el ajuste de un modelo. Será importante mediar posteriormente entre el sesgo y la varianza de forma que podamos tener un mejor ajuste sin caer en el sobreajuste. La fórmula que expresa la relación entre variable de respuesta y explicativas es ahora: \\[Y_i \\sim X_1 + X_1^2\\] Es importante mencionar que cuando se ajusta un modelo polinomial de segundo orden, se deben mantener ambas variables en el model (la original y la cuadrática). Cuando se tenga un modelo polinomial de grado k, se deberán conservar los k elementos que componen el polinomio: \\[Y\\sim X_1+X_1^2 + ... + X_1^k\\] Estas transformaciones son posibles realizarlas a múltiples variables que conforman el conjunto de datos que sirve de insumo para el modelo. A través de los pasos secuenciales en las recetas podemos integrar esta tarea a través de la función step_poly( ). Veamos un ejemplo: library(tidymodels) n = 100 set.seed(12345) original_data &lt;- tibble(x = seq(-10, 10, lengt=n)) %&gt;% mutate(y = x^3 + rnorm(n, mean = 0, sd = 100)) poly_recipe &lt;- recipe(y ~ x, data = original_data) %&gt;% step_poly(x, degree = 3) %&gt;% prep() juice(poly_recipe) ## # A tibble: 100 × 4 ## y x_poly_1 x_poly_2 x_poly_3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -941. -0.171 0.217 -0.249 ## 2 -870. -0.168 0.204 -0.219 ## 3 -895. -0.165 0.191 -0.190 ## 4 -874. -0.161 0.178 -0.163 ## 5 -716. -0.158 0.166 -0.137 ## 6 -908. -0.154 0.154 -0.113 ## 7 -616. -0.151 0.142 -0.0904 ## 8 -661. -0.147 0.131 -0.0690 ## 9 -618. -0.144 0.119 -0.0489 ## 10 -640. -0.140 0.108 -0.0302 ## # … with 90 more rows juice(poly_recipe) %&gt;% lm(y ~ x_poly_1 + x_poly_2 + x_poly_3, data = .) ## ## Call: ## lm(formula = y ~ x_poly_1 + x_poly_2 + x_poly_3, data = .) ## ## Coefficients: ## (Intercept) x_poly_1 x_poly_2 x_poly_3 ## 24.52 3643.95 -34.12 1541.84 juice(poly_recipe) %&gt;% mutate( x = seq(-10, 10, lengt=n), y_est = 24.52 + 3643.95 * x_poly_1 -34.12 * x_poly_2 + 1541.84 * x_poly_3) %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_smooth(aes(y = y_est)) + ggtitle(&quot;Regresión polinomial&quot;) En el ejemplo anterior puede mostrarse que es a través de la ponderación de los términos polinomiales que se logra estimar a la variable de respuesta Y. Este mismo procedimiento puede usarse en la receta de la ingeniería de datos para realizar la predicción de una variable de respuesta mediante cualquier otro algoritmo predictivo. 2.2 Análisis de Componentes Principales El análisis PCA (por sus siglas en inglés) es una técnica de reducción de dimensión útil tanto para el proceso de análisis exploratorio, el inferencial y predictivo. Es una técnica ampliamente usada en muchos estudios, pues permite sintetizar la información relevante y desechar aquello que no aporta tanto. Es particularmente útil en el caso de conjuntos de datos “amplios” en donde las variables están correlacionadas entre sí y donde se tienen muchas variables para cada observación. En los conjuntos de datos donde hay muchas variables presentes, no es fácil trazar los datos en su formato original, lo que dificulta tener una idea de las tendencias presentes en ellos. PCA permite ver la estructura general de los datos, identificando qué observaciones son similares entre sí y cuáles son diferentes. Esto puede permitirnos identificar grupos de muestras que son similares y determinar qué variables hacen a un grupo diferente de otro. La idea detrás de esta técnica es la siguiente: Se desean crear nuevas variables llamadas Componentes Principales, las cuales son creadas como combinación lineal (transformación lineal) de las variables originales, por lo que cada una de las variables nuevas contiene parcialmente información de todas las variables originales. \\[Z_1 = a_{11}X_1 +a_{12}X_2 + ... + a_{1p}X_p\\] \\[Z_2 = a_{21}X_1 +a_{22}X_2 + ... + a_{2p}X_p\\] \\[...\\] \\[Z_p = a_{p1}X_1 +a_{p2}X_2 + ... + a_{pp}X_p\\] Se desea que la primer componente principal capture la mayor varianza posible de todo el conjunto de datos. \\[\\forall i \\in 2,...,p \\quad Var(Z_1)&gt;Var(Z_i)\\] La segunda componente principal deberá SER INDEPENDIENTE de la primera y deberá abarcar la mayor varianza posible del restante. Esta condición se debe cumplir para toda componente i, de tal forma que las nuevas componentes creadas son independientes entre sí y acumulan la mayor proporción de varianza en las primeras de ellas, dejando la mínima proporción de varianza a las últimas componentes. \\[Z_1 \\perp\\!\\!\\!\\perp Z_2 \\quad \\&amp; \\quad Var(Z_1)&gt;Var(Z_2)&gt;Var(Z_i)\\] El punto anterior permite desechar unas cuantas componentes (las últimas) sin perder mucha varianza. ¡¡ RECORDAR !! A través de CPA se logra retener la mayor cantidad de varianza útil pero usando menos componentes que el número de variables originales. Para que este proceso sea efectivo, debe existir ALTA correlación entre las variables originales. Cuando muchas variables se correlacionan entre sí, todas contribuirán fuertemente al mismo componente principal. Cada componente principal suma un cierto porcentaje de la variación total en el conjunto de datos. Cuando sus variables iniciales estén fuertemente correlacionadas entre sí y podrá aproximar la mayor parte de la complejidad de su conjunto de datos con solo unos pocos componentes principales. Agregar componentes adicionales hace que la estimación del conjunto de datos total sea más precisa, pero también más difícil de manejar. 2.2.1 Eigenvalores y eigenvectores Los vectores propios y los valores propios vienen en pares: cada vector propio tiene un valor propio correspondiente. Los vectores propios son la ponderación que permite crear la combinación lineal de las variables para conformar cada componente principal, mientras que el valor propio es la varianza asociada a cada componente principal. Desde un punto de vista geométrico, el eigenvector es la dirección del vector determinado por la componente principal y el eigenvalor es la magnitud de dicho vector. El valor propio de una componente es la varianza de este. La suma acumulada de los primeros \\(j\\) eigenvalores representa la varianza acumulada de las primeras \\(j\\) componentes principales El número de valores propios y vectores propios que existe es igual al número de dimensiones que tiene el conjunto de datos. 2.2.2 Implementación en R library(sf) library(magrittr) library(tidymodels) indice_marg &lt;- st_read(&#39;data/IMEF_2010.dbf&#39;, quiet = TRUE) glimpse(indice_marg) ## Rows: 32 ## Columns: 16 ## $ CVE_ENT &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;1… ## $ AÑO &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20… ## $ POB_TOT &lt;int&gt; 1184996, 3155070, 637026, 822441, 2748391, 650555, 4796580, 34… ## $ ANALF &lt;dbl&gt; 3.274040, 2.600783, 3.234464, 8.370643, 2.645050, 5.157943, 17… ## $ SPRIM &lt;dbl&gt; 14.754823, 12.987567, 14.273833, 22.541207, 12.168029, 18.4761… ## $ OVSDE &lt;dbl&gt; 1.0649743, 0.4322072, 0.9436751, 6.4196750, 1.0916308, 0.68577… ## $ OVSEE &lt;dbl&gt; 0.62347891, 0.94517891, 2.84464884, 2.59080046, 0.53707721, 0.… ## $ OVSAE &lt;dbl&gt; 0.9854257, 3.5616214, 7.0865085, 9.7378176, 1.3908497, 1.17060… ## $ VHAC &lt;dbl&gt; 30.33066, 29.05839, 31.73806, 45.96720, 30.26891, 31.32052, 53… ## $ OVPT &lt;dbl&gt; 1.761813, 3.398537, 5.814081, 4.500699, 1.423701, 4.691477, 15… ## $ PL_5000 &lt;dbl&gt; 25.1626166, 10.3491523, 15.6188287, 30.8755279, 12.1486353, 14… ## $ PO2SM &lt;dbl&gt; 33.64880, 21.86970, 23.29986, 45.51076, 30.04270, 32.04402, 69… ## $ IM &lt;dbl&gt; -0.91086057, -1.14014880, -0.68128749, 0.43357139, -1.14000448… ## $ GM &lt;chr&gt; &quot;Bajo&quot;, &quot;Muy bajo&quot;, &quot;Bajo&quot;, &quot;Alto&quot;, &quot;Muy bajo&quot;, &quot;Bajo&quot;, &quot;Muy a… ## $ LUGAR &lt;int&gt; 28, 30, 23, 10, 29, 26, 2, 21, 32, 15, 14, 1, 6, 27, 22, 8, 19… ## $ NOM_ENT &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Baja California&quot;, &quot;Baja California Sur&quot;, &quot;C… indice_marg %&gt;% dplyr::count(GM, sort = TRUE) ## GM n ## 1 Medio 9 ## 2 Alto 8 ## 3 Bajo 8 ## 4 Muy bajo 4 ## 5 Muy alto 3 pca_recipe &lt;- recipe(IM ~ ., data = indice_marg) %&gt;% update_role(NOM_ENT, GM, new_role = &quot;id&quot;) %&gt;% step_normalize(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% step_pca(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM, num_comp=9, res=&quot;res&quot;) %&gt;% step_rm(LUGAR, AÑO, POB_TOT) %&gt;% prep() juice(pca_recipe) ## # A tibble: 32 × 13 ## CVE_ENT GM NOM_ENT IM PC1 PC2 PC3 PC4 PC5 PC6 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Bajo Aguasc… -0.911 -2.34 -0.227 0.372 0.492 0.264 0.0764 ## 2 02 Muy b… Baja C… -1.14 -2.93 0.595 -0.0597 -0.492 0.291 -0.0508 ## 3 03 Bajo Baja C… -0.681 -1.75 1.37 -0.683 -0.400 -0.304 0.160 ## 4 04 Alto Campec… 0.434 1.12 -0.819 -0.151 -0.271 -0.929 0.178 ## 5 05 Muy b… Coahui… -1.14 -2.93 -0.144 0.157 -0.133 0.0419 -0.0786 ## 6 06 Bajo Colima -0.779 -2.00 0.0316 0.552 -0.136 0.320 -0.729 ## 7 07 Muy a… Chiapas 2.32 5.96 0.132 1.36 -0.0122 -0.673 -0.471 ## 8 08 Bajo Chihua… -0.520 -1.34 1.05 -1.27 0.633 -0.646 -0.387 ## 9 09 Muy b… Distri… -1.48 -3.81 0.110 0.159 -0.453 0.205 -0.355 ## 10 10 Medio Durango 0.0525 0.135 0.675 -1.50 0.929 -0.448 0.146 ## # … with 22 more rows, and 3 more variables: PC7 &lt;dbl&gt;, PC8 &lt;dbl&gt;, PC9 &lt;dbl&gt; Veamos los pasos de esta receta: Primero, debemos decirle a la receta qué datos se usan para predecir la variable de respuesta. Se actualiza el rol de las variables nombre de entidad y grado de marginación con la función NOM_ENT, ya que es una variable que queremos mantener por conveniencia como identificador de filas, pero no son un predictor ni variable de respuesta. Necesitamos centrar y escalar los predictores numéricos, porque estamos a punto de implementar PCA. Finalmente, usamos step_pca() para realizar el análisis de componentes principales. La función prep() es la que realiza toda la preparación de la receta. Una vez que hayamos hecho eso, podremos explorar los resultados del PCA. Comencemos por ver cómo resultó el PCA. Podemos ordenar los resultados mediante la función tidy(), incluido el paso de PCA, que es el segundo paso. Luego hagamos una visualización para ver cómo se ven los componentes. A continuación se muestran la desviación estándar, porcentaje de varianza y porcentaje de varianza acumulada que aporta cada componente principal. summary(pca_recipe$steps[[2]]$res) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.572 0.82085 0.7920 0.64640 0.52101 0.44069 0.31797 ## Proportion of Variance 0.735 0.07487 0.0697 0.04643 0.03016 0.02158 0.01123 ## Cumulative Proportion 0.735 0.80990 0.8796 0.92603 0.95619 0.97777 0.98900 ## PC8 PC9 ## Standard deviation 0.25660 0.18201 ## Proportion of Variance 0.00732 0.00368 ## Cumulative Proportion 0.99632 1.00000 Podemos observar que en la primera componente principal, las \\(9\\) variables que utilizó el Consejo Nacional de Población para obtener el Índice de Marginación 2010 aportan de manera positiva en el primer componente principal. library(tidytext) tidied_pca &lt;- tidy(pca_recipe, 2) tidied_pca %&gt;% filter(component %in% paste0(&quot;PC&quot;, 1:4)) %&gt;% group_by(component) %&gt;% top_n(9, abs(value)) %&gt;% ungroup() %&gt;% mutate(terms = reorder_within(terms, abs(value), component)) %&gt;% ggplot(aes(abs(value), terms, fill = value &gt; 0)) + geom_col() + facet_wrap(~component, scales = &quot;free_y&quot;) + scale_y_reordered() + labs( x = &quot;Absolute value of contribution&quot;, y = NULL, fill = &quot;Positive?&quot; )+ theme_minimal() Notamos que las \\(9\\) variables aportan entre el \\(25\\%\\) y el \\(35\\%\\) a la primera componente principal. 2.2.3 Representación gráfica library(ggrepel) juice(pca_recipe) %&gt;% mutate(GM = factor(GM, levels = c(&quot;Muy alto&quot;, &quot;Alto&quot;, &quot;Medio&quot;, &quot;Bajo&quot;, &quot;Muy bajo&quot;)), ordered = T) %&gt;% ggplot(aes(PC1, PC2, label = NOM_ENT)) + geom_point(aes(color = GM), alpha = 0.7, size = 2) + geom_text_repel() + ggtitle(&quot;Grado de marginación de entidades&quot;) Finalmente, podemos observar como (de izquierda a derecha) los estados con grado de marginación Muy bajo, Bajo, Medio, Alto y Muy Alto respectivamente. juice(pca_recipe) %&gt;% ggplot(aes(x = IM, y = PC1)) + geom_smooth(method = &quot;lm&quot;) + geom_point(size = 2) + ggtitle(&quot;Comparación: Índice Marginación Vs PCA CP1&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 2.2.4 ¿Cuántas componentes retener? Existe en la literatura basta información sobre el número de componentes a retener en un análisis de PCA. El siguiente gráfico lleva por nombre gráfico de codo y muestra el porcentaje de varianza explicado por cada componente principal. library(factoextra) library(FactoMineR) res.pca &lt;- indice_marg %&gt;% select(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% as.data.frame() %&gt;% set_rownames(indice_marg$NOM_ENT) %&gt;% PCA(graph=FALSE) fviz_eig(res.pca, addlabels=TRUE, ylim=c(0, 100)) El grafico anterior muestra que hay una diferencia muy grande entre la varianza retenida por la 1er componente principal y el resto de las variables. Dependiendo del objetivo del analisis podra elegirse el numero adecuado de componentes a retener, no obstante, la literatura sugiere retener 1 o 2 componentes principales. Regresando al tema de feature engineering, es posible realizar el proceso de componentes principales y elegir una de las dos opciones siguientes: Especificar el número de componentes a retener Indicar el porcentaje de varianza a alcanzar La segunda opción elegirá tantas componentes como sean necesarias hasta alcanzar el hiperparámetro mínimo indicado. A continuación se ejemplifica: Caso 1: pca_recipe &lt;- recipe(IM ~ ., data = indice_marg) %&gt;% update_role(NOM_ENT, GM, new_role = &quot;id&quot;) %&gt;% step_normalize(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% step_pca(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM,num_comp=2) %&gt;% step_rm(LUGAR, AÑO, POB_TOT) %&gt;% prep() juice(pca_recipe) ## # A tibble: 32 × 6 ## CVE_ENT GM NOM_ENT IM PC1 PC2 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Bajo Aguascalientes -0.911 -2.34 -0.227 ## 2 02 Muy bajo Baja California -1.14 -2.93 0.595 ## 3 03 Bajo Baja California Sur -0.681 -1.75 1.37 ## 4 04 Alto Campeche 0.434 1.12 -0.819 ## 5 05 Muy bajo Coahuila de Zaragoza -1.14 -2.93 -0.144 ## 6 06 Bajo Colima -0.779 -2.00 0.0316 ## 7 07 Muy alto Chiapas 2.32 5.96 0.132 ## 8 08 Bajo Chihuahua -0.520 -1.34 1.05 ## 9 09 Muy bajo Distrito Federal -1.48 -3.81 0.110 ## 10 10 Medio Durango 0.0525 0.135 0.675 ## # … with 22 more rows Caso 2: pca_recipe &lt;- recipe(IM ~ ., data = indice_marg) %&gt;% update_role(NOM_ENT, GM, new_role = &quot;id&quot;) %&gt;% step_normalize(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% step_pca(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM,threshold=0.90) %&gt;% step_rm(LUGAR, AÑO, POB_TOT) %&gt;% prep() juice(pca_recipe) ## # A tibble: 32 × 8 ## CVE_ENT GM NOM_ENT IM PC1 PC2 PC3 PC4 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Bajo Aguascalientes -0.911 -2.34 -0.227 0.372 0.492 ## 2 02 Muy bajo Baja California -1.14 -2.93 0.595 -0.0597 -0.492 ## 3 03 Bajo Baja California Sur -0.681 -1.75 1.37 -0.683 -0.400 ## 4 04 Alto Campeche 0.434 1.12 -0.819 -0.151 -0.271 ## 5 05 Muy bajo Coahuila de Zaragoza -1.14 -2.93 -0.144 0.157 -0.133 ## 6 06 Bajo Colima -0.779 -2.00 0.0316 0.552 -0.136 ## 7 07 Muy alto Chiapas 2.32 5.96 0.132 1.36 -0.0122 ## 8 08 Bajo Chihuahua -0.520 -1.34 1.05 -1.27 0.633 ## 9 09 Muy bajo Distrito Federal -1.48 -3.81 0.110 0.159 -0.453 ## 10 10 Medio Durango 0.0525 0.135 0.675 -1.50 0.929 ## # … with 22 more rows Así es como usaremos el análisis de componentes principales para mejorar la estructura de variables que sirven de input para cualquiera de los modelos posteriores. Continuaremos con un paso más de pre-procesamiento antes de comenzar a aprender nuevos modelos. 2.3 Imputación KNN Antes de aprender el uso de la función de imputación, recordaremos brevemente como funciona el algoritmo de K-Nearest-Neighbor (KNN) KNN es un algoritmo de aprendizaje supervisado que podemos usar tanto para regresión como clasificación. Es un algoritmo fácil de interpretar y que permite ser flexible en el balance entre sesgo y varianza (dependiendo de los hiper-parámetros seleccionados). El algoritmo de K vecinos más cercanos realiza comparaciones entre un nuevo elemento y las observaciones anteriores que ya cuentan con etiqueta. La esencia de este algoritmo está en etiquetar a un nuevo elemento de manera similar a como están etiquetados aquellos K elementos que más se le parecen. Veremos este proceso para cada uno de los posibles casos: 2.3.1 Ventajas y limitaciones del Clasificador KNN Ventajas: KNN no hace ninguna suposición subyacente sobre los datos. Con la adición de más puntos de datos, el clasificador evoluciona constantemente y es capaz de adaptarse rápidamente a los cambios en el conjunto de datos de entrada. Le da al usuario la flexibilidad de elegir la métrica de medida de distancia. Desventajas: KNN es muy sensible a los valores atípicos. No funciona con datos faltantes. A medida que crece el conjunto de datos, la clasificación se vuelve más lenta. Existe la llamada maldición de la dimensionalidad. Este algoritmo es altamente usado para imputación de datos faltantes, ¿tiene lógica, cierto?, con recipes podemos aplicar un paso con la función: step_impute_knn antes llamada step_knnimpute(), podemos observar la documentación de la función en el siguiente enlace. 2.3.2 Implementación en R Veámos cómo implementar la imputación por KNN para datos faltantes en R: step_impute_knn( recipe, ..., ### Variables a imputar neighbors = 5, impute_with = imp_vars(all_predictors()), id = rand_id(&quot;impute_knn&quot;) ) Acerca de los parámetros: neighbors: Número de vecinos impute_with: Una llamada a imp_vars para especificar qué variables se usan para imputar las variables. Si una columna se incluye en ambas listas para ser imputada y para ser un predictor de imputación, se eliminará de esta última y no se usará para imputarse a sí misma. id: Una cadena de caracteres que es exclusiva de este paso para identificarlo. La función utiliza el conjunto de entrenamiento para imputar cualquier otro conjunto de datos. La única función de distancia disponible es la distancia de Gower, que se puede utilizar para combinaciones de datos nominales y numéricos. Acerca de Gower El coeficiente de similitud de Gower propuesto en 1971 permite la manipulación simultánea de variables cuantitativas y cualitativas en una base de datos, mediante la aplicación de este coeficiente se logra hallar la similitud entre individuos a los cuales se les han medido una serie de características en común. Una similaridad alta, es decir cercana a 1, indicara gran homogeneidad entre los individuos; por el contrario, una similaridad cercana a cero indica que los individuos son diferentes Vamos a utilizar los datos de biomass de la libreria modeldata que contiene un conjunto de datos donde diferentes combustibles de biomasa se caracterizan por la cantidad de ciertas moléculas (carbono, hidrógeno, oxígeno, nitrógeno y azufre) y el poder calorífico superior correspondiente (HHV). En esta base hemos retirado valores aleatoriamente sobre dos variables para realizar el ejercicio. library(recipes) library(modeldata) library(DataExplorer) data(biomass) biomass_te_whole &lt;- as_tibble(biomass) # induce some missing data at random set.seed(19735) carb_missing &lt;- sample(1:nrow(biomass_te_whole), 75) nitro_missing &lt;- sample(1:nrow(biomass_te_whole), 75) biomass_te_whole$carbon[carb_missing] &lt;- NA biomass_te_whole$nitrogen[nitro_missing] &lt;- NA biomass_te_whole[&quot;carb_imputed&quot;] &lt;- &quot;No&quot; biomass_te_whole$carb_imputed[carb_missing] &lt;- &quot;Yes&quot; biomass_te_whole[&quot;nitro_imputed&quot;] &lt;- &quot;No&quot; biomass_te_whole$nitro_imputed[nitro_missing] &lt;- &quot;Yes&quot; biomass_tr &lt;- biomass_te_whole %&gt;% filter( dataset == &#39;Training&#39;) biomass_te &lt;- biomass_te_whole %&gt;% filter( dataset == &quot;Testing&quot;) biomass_te_whole[nitro_missing,] ## # A tibble: 75 × 10 ## sample dataset carbon hydrogen oxygen nitrogen sulfur HHV carb_imputed ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Red Alder … Traini… 49.6 6.06 43.8 NA 0.07 19.3 No ## 2 Cereals, H… Traini… 44.8 5 42.5 NA 0.13 18.4 No ## 3 Sewage Slu… Traini… 28.3 4.07 17.5 NA 1.25 12.1 No ## 4 Palm Fibre Testing 47.5 6.01 36.4 NA 0.3 19.2 No ## 5 Rice Straw Traini… 35.7 4.62 39.1 NA 0 14.8 No ## 6 Prune Pits Traini… 49.3 6.59 41.8 NA 0.07 20.1 No ## 7 Ryegrass S… Traini… 46.7 5.8 41.9 NA 0.2 18.5 No ## 8 Peanut Hul… Traini… 45.8 5.46 39.6 NA 0.12 18.6 No ## 9 Cereals, H… Traini… NA 4.9 43.1 NA 0.12 18.2 Yes ## 10 Erco Char Traini… 65.9 2.57 12.8 NA 0.1 24.2 No ## # … with 65 more rows, and 1 more variable: nitro_imputed &lt;chr&gt; biomass_tr %&gt;% DataExplorer::plot_missing( title = &quot;Train&quot; ) biomass_te %&gt;% DataExplorer::plot_missing( title = &quot;Test&quot; ) En la primera opción vamos a aplicar el paso de immputación indicando qué columnas quieren ser imputadas y con cuáles variables queremos que se haga el proceso. recipe_esp &lt;- recipe( HHV ~ carbon + hydrogen + oxygen + nitrogen + sulfur, data = biomass_tr) %&gt;% step_impute_knn( carbon, nitrogen, impute_with= imp_vars(hydrogen, oxygen), neighbors = 3) %&gt;% prep() imputed_esp_train &lt;- bake(recipe_esp, biomass_tr) imputed_esp_test &lt;- bake(recipe_esp, biomass_te) ## Probando la receta con un test imputed_esp_train %&gt;% DataExplorer::plot_missing( title = &quot;Imputacion Train&quot;) imputed_esp_test %&gt;% DataExplorer::plot_missing(title = &quot;Imputacion Test&quot;) Sin embargo, siempre podemos pedirle al modelo que haga la imputación de todas las variables que tengan nulos, con toda la información de las demás variables disponibles (no nulas). recipe &lt;- recipe( HHV ~ carbon + hydrogen + oxygen + nitrogen + sulfur, data = biomass_tr) %&gt;% step_impute_knn(all_predictors(), neighbors = 3) %&gt;% prep() imputed &lt;- bake(recipe, biomass_te) %&gt;% bind_cols(biomass_te %&gt;% select(carb_imputed, nitro_imputed)) # prueba con test imputed %&gt;% DataExplorer::plot_missing() biomass %&gt;% filter(dataset == &#39;Testing&#39;) %&gt;% select(nitrogen) %&gt;% bind_cols(imputed[,c(4, 8)]) %&gt;% ggplot(aes(x = `nitrogen...2`, y = `nitrogen...1`, color = nitro_imputed)) + geom_point() + ggtitle(&quot;Comparación de datos reales vs imputados (Nitrógeno)&quot;) + xlab(&quot;Imputado&quot;) + ylab(&quot;Real&quot;) biomass %&gt;% filter(dataset == &#39;Testing&#39;) %&gt;% select(carbon) %&gt;% bind_cols(imputed[,c(1, 7)]) %&gt;% ggplot(aes(x = `carbon...2`, y = `carbon...1`, color = carb_imputed)) + geom_point() + ggtitle(&quot;Comparación de datos reales vs imputados (Carbono)&quot;) + xlab(&quot;Imputado&quot;) + ylab(&quot;Real&quot;) 2.4 Ejercicios Cada equipo estará a cargo de desarrollar una receta de feature engineering utilizando los pasos vistos en el curso pasado y el actual. Tendrán 15 días para probar distintas estrategias que mejoren las predicciones del precio de ventas. Se deberá entregar y explicar el código creado por el equipo. Este código servirá para los ejercicios de optimización de modelos en los siguientes capítulos. "],["support-vector-machine-svm-svr.html", "Capítulo 3 Support Vector Machine (SVM / SVR) 3.1 Maximum Margin Classifier 3.2 Support Vector Classifiers 3.3 Support Vector Machine 3.4 El truco del Kernel 3.5 Support Vector Regression 3.6 Ventajas y desventajas 3.7 Ajuste del modelo con R 3.8 Ejercicios", " Capítulo 3 Support Vector Machine (SVM / SVR) Es común encontrar en la literatura el nombre de SVM para referirse tanto al caso de regresión como al de clasificación, no obstante, SVR se refiere particularmente a Suport Vector Regression. Support vector machine, llamado SVM, es un algoritmo de aprendizaje supervisado que se puede utilizar para problemas de clasificación y regresión. Se utiliza para conjuntos de datos más pequeños, ya que tarda demasiado en procesarse. El principal objetivo de esta técnica es encontrar el Hiperplano de Separación Óptima, también conocido como Boundary Decision, el cual será el margen de clasificación más grande que podamos ajustar para separar a las clases involucradas, limitando las veces que una observación viola dicho margen. Para entender este algoritmo es necesario entender 3 conceptos principales: Maximum margin classifiers Support vector classifiers Support vector machines Estudiemos cada uno de estos principios. 3.1 Maximum Margin Classifier A menudo se generalizan con máquinas de vectores de soporte, pero SVM tiene muchos más parámetros en comparación. El clasificador de margen máximo considera un hiperplano con ancho de separación máxima para clasificar los datos. Sin embargo, se pueden dibujar infinitos hiperplanos en un conjunto de datos por lo que es importante elegir el hiperplano ideal para la clasificación. En un espacio n-dimensional, un hiperplano es un subespacio de la dimensión n-1. Es decir, si los datos tienen un espacio bidimensional, entonces el hiperplano puede ser una línea recta que divide el espacio de datos en dos mitades y pasa por la siguiente ecuacion: \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2=0\\] Las observaciones que caen en el hiperplano sigue la ecuación anterior. Las observaciones que caen en la región por encima o por debajo del hiperplano sigue las siguientes ecuaciones: \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2&gt;0\\] \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2&lt;0\\] El clasificador de margen máximo a menudo falla en la situación de casos no separables en los que no puede asignar un hiperplano diferente para clasificar datos no separables. Para tales casos, un clasificador de vectores de soporte viene al rescate. Del diagrama anterior, podemos suponer infinitos hiperplanos (izquierda). El clasificador de margen máximo viene con un solo hiperplano que divide los datos como en la gráfica de la derecha. Los datos que tocan los hiperplanos positivo y negativo se denominan vectores de soporte. 3.2 Support Vector Classifiers Los vectores de soporte son las observaciones que están más cerca del hiperplano e influyen en la posición y orientación del hiperplano. Este tipo de clasificador puede considerarse como una versión extendida del clasificador de margen máximo. Cuando tratamos con datos de la vida real, encontramos que la mayoría de las observaciones están en clases superpuestas. Es por eso que se implementan clasificadores de vectores de soporte. Usando estos vectores de soporte, maximizamos el margen del clasificador. Eliminar los vectores de soporte cambiará la posición del hiperplano. Estos son los puntos que nos ayudan a construir nuestro SVM. Consideremos un parámetro de ajuste C. Entendamos con el siguiente diagrama. Podemos ver en el gráfico de la izquierda que los valores más altos de C generaron más errores que se consideran una violación o infracción. El diagrama de la derecha muestra un valor más bajo de C y no brinda suficientes posibilidades de infracción al reducir el ancho del margen. Puede considerarse al parámetro C como el monto de regularización, tal que: Si C es bajo, el margen será más amplio y tendremos un mayor número de violaciones al margen, pero el modelo generalizará mejor Si C es alto, nuestro margen será menos amplio y tendrá menos violaciones. Sin embargo, no generalizará bien. Este modelo es sensible a cambios en la escala de datos de entrada, por lo que será importante estandarizar las variables antes de usar este modelo. 3.3 Support Vector Machine El enfoque de la máquina de vectores de soporte se considera durante una decisión no lineal y los datos no son separables por un clasificador de vectores de soporte, independientemente de la función de costo. Cuando es casi imposible separar clases de manera no lineal, aplicamos el truco llamado truco del kernel el cual ayuda a manejar la separación de los datos. En el gráfico anterior, los datos que eran inseparables en una dimensión se separaron una vez que se transformaron a un espacio de dos dimensiones después de aplicar una transformación mediante kernel polinomial de segundo grado. Ahora veamos cómo manejar los datos bidimensionales linealmente inseparables. En datos bidimensionales, el núcleo polinomial de segundo grado se aplica utilizando un plano lineal después de transformarlo a dimensiones superiores. 3.4 El truco del Kernel Las funciones Kernel son métodos con los que se utilizan clasificadores lineales como SVM para clasificar puntos de datos separables no linealmente. Esto se hace representando los puntos de datos en un espacio de mayor dimensión que su original. Por ejemplo, los datos 1D se pueden representar como datos 2D en el espacio, los datos 2D se pueden representar como datos 3D, etcétera. El truco del kernel ofrece una forma de calcular las relaciones entre los puntos de datos utilizando funciones del kernel y representar los datos de una manera más eficiente con menos cómputo. Los modelos que utilizan esta técnica se denominan “modelos kernelizados”. Hay varias funciones que utiliza SVM para realizar esta tarea. Algunos de los más comunes son: El núcleo lineal: Se utiliza para datos lineales. Esto simplemente representa los puntos de datos usando una relación lineal. \\[K(x, y)=(x^T \\cdot y)\\] \\[f(x)=w^T \\cdot x + b\\] Esta formulación se presenta como solución al problema de optimización sobre w: \\[min_{w\\in R^d} \\frac{1}{2}\\parallel w \\parallel ^2+ C\\sum_{i}^{N}{max(0, 1-y_i f(x_i))}\\] \\[s.a. \\quad y_i f(x_i) \\geq 1 - max(0, 1-y_i f(x_i))\\] En donde \\(1-y_i f(x_i)\\) es la distancia de \\(x_i\\) al correspondiente margen de la clase si \\(x_i\\) se encuentra en el lado equivocado del margen y cero en caso contrario. De esta forma, los puntos que se encuentran lejos del margen del lado equivocado obtendrán una mayor penalización. Dar click en la siguiente liga para mayor entendimiento del problema de optimización. Función de núcleo polinomial: Transforma los puntos de datos mediante el uso del producto escalar y la transformación de los datos en una “dimensión n,” n podría ser cualquier valor de 2, 3, etcétera, es decir, la transformación será un producto al cuadrado o superior. Por lo tanto, representar datos en un espacio de mayor dimensión utilizando los nuevos puntos transformados. \\[K(x, y)=(c+ x^T \\cdot y)^p\\] Cuando se emplea \\(p=1\\) y \\(c=0\\), el resultado es el mismo que el de un kernel lineal. Si \\(p&gt;1\\), se generan límites de decisión no lineales, aumentando la no linealidad a medida que aumenta p. No suele ser recomendable emplear valores de p mayores 5 por problemas de overfitting. La función de base radial (RBF): Esta función se comporta como un “modelo de vecino más cercano ponderado.” Transforma los datos representándolos en dimensiones infinitas, La función Radial puede ser de Gauss o de Laplace. Esto depende de un hiperparámetro conocido como gamma \\(\\gamma\\). Cuanto menor sea el valor del hiperparámetro, menor será el sesgo y mayor la varianza. Mientras que un valor más alto de hiperparámetro da un sesgo más alto y menor varianza. Este es el núcleo más utilizado. \\[K(x, y)=exp(-\\gamma \\parallel x - y\\parallel^2)=exp(-\\frac{\\parallel x-y \\parallel ^2}{2\\sigma²})\\] \\[f(x)=w^T \\cdot \\phi(x) + b\\] Se realiza un mapeo de x a \\(\\phi(x)\\) en donde los datos son separables Es recomendable probar el kernel RBF. Este kernel tiene dos ventajas: que solo tiene dos hiperparámetros que optimizar (\\(\\gamma\\) y la penalización \\(C\\) común a todos los SVM) y que su flexibilidad puede ir desde un clasificador lineal a uno muy complejo. La función sigmoide: También conocida como función tangente hiperbólica (Tanh), encuentra más aplicación en redes neuronales como función de activación. Esta función mapea los valores de entrada al intervalo [-1, 1]. \\[K(x, y)= tanh(\\kappa x\\cdot y-\\delta)\\] ¿Por qué se llama un “truco del kernel?” SVM vuelve a representar los puntos de datos no lineales utilizando cualquiera de las funciones del kernel de una manera que parece que los datos se han transformado, luego encuentra el hiperplano de separación óptimo, sin embargo, en realidad, los puntos de datos siguen siendo los mismos, en realidad no se han transformado. Es por eso que se llama un ‘truco del kernel.’ 3.5 Support Vector Regression El problema de la regresión es encontrar una función que aproxime la relación de un dominio de datos de entrada a números reales con base en una muestra de entrenamiento. Veamos cómo funciona SVR en realidad. Consideremos las dos líneas rojas como el límite de decisión y la línea verde como el hiperplano. Nuestro objetivo, cuando avanzamos con SVR, es básicamente considerar los puntos que están dentro de la línea límite de decisión. Nuestra línea de mejor ajuste es el hiperplano que tiene un número máximo de puntos. Lo primero que entenderemos será el límite de decisión. Consideremos estas líneas como si estuvieran a cualquier distancia, digamos ‘a,’ del hiperplano. Entonces, estas son las líneas que dibujamos a la distancia ‘+a’ y ‘-a’ del hiperplano. Esta ‘a’ en el texto se conoce básicamente como épsilon y representa el margen. Suponiendo que la ecuación del hiperplano es la siguiente: \\[Y_i = W^TX + b\\] Entonces estas ecuaciones se transforman en la siguiente forma: \\(W^TX + b = +a\\) \\(W^TX + b = -a\\) Por lo tanto, cualquier hiperplano que satisfaga nuestra SVR debería satisfacer: \\(-a &lt; Y- WX+b &lt; +a\\) Nuestro objetivo principal aquí es decidir un límite de decisión a una distancia ‘a’ del hiperplano original, de modo que los puntos de datos más cercanos al hiperplano o los vectores de soporte estén dentro de esa línea límite. Vamos a tomar solo aquellos puntos que están dentro del límite de decisión y tienen la menor tasa de error, o están dentro del margen de tolerancia. Esto nos da un mejor modelo de ajuste. 3.6 Ventajas y desventajas Ventajas Es un modelo que ajusta bien con pocos datos Son flexibles en datos no estructurados, estructurados y semiestructurados. La función Kernel alivia las complejidades en casi cualquier tipo de datos. Se observa menos sobreajuste en comparación con otros modelos. Desventajas El tiempo de entrenamiento es mayor cuando se calculan grandes conjuntos de datos. Los hiperparámetros suelen ser un desafío al interpretar su impacto. La interpretación general es difícil (black box). 3.7 Ajuste del modelo con R Usaremos las recetas antes implementadas para ajustar tanto el modelo de regresión como el de clasificación. Exploraremos un conjunto de hiperparámetros para elegir el mejor modelo. Recordemos los pasos a seguir al ajustar un modelo Separación inicial de datos ( test, train ) Pre-procesamiento e ingeniería de variables Selección de tipo de modelo con hiperparámetros iniciales Inicialización de workflow o pipeline Creación de grid search Entrenamiento de modelos con hiperparámetros definidos (salvar los modelos entrenados) Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) Selección de modelo a usar Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) Validar poder predictivo con datos de prueba. 3.7.1 Implementación de SVR en R A continuación, revisaremos paso por paso este procedimiento usando SVM como modelo. Los datos corresponden a nuestro ya conocido problema predictivo de precio de casas. Se puede encontrar los datos y documentación en el siguiente enlace Paso 1: Separación inicial de datos ( test, train ) library(tidymodels) data(ames) set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) ames_folds &lt;- vfold_cv(ames_train) Contando con datos de entrenamiento, procedemos a realizar el feature engineering para extraer las mejores características que permitirán realizar las estimaciones en el modelo. Paso 2: Pre-procesamiento e ingeniería de variables receta_casas &lt;- recipe(Sale_Price ~ . , data = ames_train) %&gt;% step_unknown(Alley) %&gt;% step_rename(Year_Remod = Year_Remod_Add) %&gt;% step_rename(ThirdSsn_Porch = Three_season_porch) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod, TotalSF = Gr_Liv_Area + Total_Bsmt_SF, AvgRoomSF = Gr_Liv_Area / TotRms_AbvGrd, Pool = if_else(Pool_Area &gt; 0, 1, 0), Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_predictors(), -all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~ Second_Flr_SF:First_Flr_SF) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% step_rm( First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath, Bsmt_Half_Bath, Kitchen_AbvGr, BsmtFin_Type_1_Unf, Total_Bsmt_SF, Kitchen_AbvGr, Pool_Area, Gr_Liv_Area, Sale_Type_Oth, Sale_Type_VWD ) %&gt;% prep() receta_casas ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 73 ## ## Training data contained 2197 data points and no missing data. ## ## Operations: ## ## Unknown factor level assignment for Alley [trained] ## Variable renaming for Year_Remod [trained] ## Variable renaming for ThirdSsn_Porch [trained] ## Ratios from Bedroom_AbvGr, Gr_Liv_Area [trained] ## Variable mutation for ~Year_Sold - Year_Remod, ~Gr_Liv_Area + To... [trained] ## Re-order factor level to ref_level for Exter_Cond [trained] ## Centering and scaling for Lot_Frontage, Lot_Area, Year_Built, Year_Remod,... [trained] ## Dummy variables from MS_SubClass, MS_Zoning, Street, Alley, Lot_Shape, Land_Co... [trained] ## Interactions with Second_Flr_SF:First_Flr_SF [trained] ## Interactions with (Bsmt_Cond_Fair + Bsmt_Cond_Good + Bsmt_Cond_No_Ba... [trained] ## Variables removed First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath... [trained] Recordemos que la función recipe() solo son los pasos a seguir, necesitamos usar la función prep() que nos devuelve una receta actualizada con las estimaciones y la función juice() que nos devuelve la matriz de diseño. Una vez que la receta de transformación de datos está lista, procedemos a implementar el pipeline del modelo de interés. Existen diversas funciones dentro de tidymodels para implementar estos modelos, entra las cuales se encuentran: Base lineal: svm_lineal() Base polinomial: svm_poly() Base radial: svm_rbf() Paso 3: Selección de tipo de modelo con hiperparámetros iniciales svm_model &lt;- svm_rbf( mode = &quot;regression&quot;, cost = tune(), rbf_sigma = tune(), margin = tune()) %&gt;% set_engine(&quot;kernlab&quot;) svm_model ## Radial Basis Function Support Vector Machine Specification (regression) ## ## Main Arguments: ## cost = tune() ## rbf_sigma = tune() ## margin = tune() ## ## Computational engine: kernlab Paso 4: Inicialización de workflow o pipeline svm_workflow &lt;- workflow() %&gt;% add_recipe(receta_casas) %&gt;% add_model(svm_model) svm_workflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: svm_rbf() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 11 Recipe Steps ## ## • step_unknown() ## • step_rename() ## • step_rename() ## • step_ratio() ## • step_mutate() ## • step_relevel() ## • step_normalize() ## • step_dummy() ## • step_interact() ## • step_interact() ## • ... ## • and 1 more step. ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Radial Basis Function Support Vector Machine Specification (regression) ## ## Main Arguments: ## cost = tune() ## rbf_sigma = tune() ## margin = tune() ## ## Computational engine: kernlab Paso 5: Creación de grid search svm_parameters_set &lt;- svm_workflow %&gt;% hardhat::extract_parameter_set_dials() %&gt;% update( cost = cost(c(0, 0.5)), rbf_sigma = rbf_sigma(c(-3, 3)), margin = svm_margin(c(-2, 2)) ) set.seed(123) svm_grid &lt;- svm_parameters_set %&gt;% grid_max_entropy(size = 100) svm_grid ## # A tibble: 100 × 3 ## cost rbf_sigma margin ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.22 0.504 0.571 ## 2 1.41 1.22 0.329 ## 3 1.33 0.111 1.38 ## 4 1.35 0.0935 -1.65 ## 5 1.04 0.937 1.48 ## 6 1.14 0.0154 -1.91 ## 7 1.39 70.2 -1.98 ## 8 1.07 1.16 -0.487 ## 9 1.17 1.34 0.948 ## 10 1.02 0.0303 0.230 ## # … with 90 more rows ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) Paso 6: Entrenamiento de modelos con hiperparámetros definidos library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) svm1 &lt;- Sys.time() svm_tune_result &lt;- tune_grid( svm_workflow, resamples = ames_folds, grid = svm_grid, metrics = metric_set(rmse, mae, mape), control = ctrl_grid ) svm2 &lt;- Sys.time(); svm2 - svm1 stopCluster(cluster) svm_tune_result %&gt;% saveRDS(&quot;models/svm_model_reg.rds&quot;) Podemos obtener las métricas de cada fold con el siguiente código: svm_tune_result &lt;- readRDS(&quot;models/svm_model_reg.rds&quot;) unnest(svm_tune_result, .metrics) ## # A tibble: 3,000 × 11 ## splits id cost rbf_sigma margin .metric .estimator .estimate ## &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &lt;split [1977/220]&gt; Fold01 1.04 0.00515 0.543 rmse standard 41851. ## 2 &lt;split [1977/220]&gt; Fold01 1.04 0.00515 0.543 mae standard 29737. ## 3 &lt;split [1977/220]&gt; Fold01 1.04 0.00515 0.543 mape standard 19.8 ## 4 &lt;split [1977/220]&gt; Fold01 1.12 688. 1.55 rmse standard 98784. ## 5 &lt;split [1977/220]&gt; Fold01 1.12 688. 1.55 mae standard 86126. ## 6 &lt;split [1977/220]&gt; Fold01 1.12 688. 1.55 mape standard 63.4 ## 7 &lt;split [1977/220]&gt; Fold01 1.15 0.0145 -0.643 rmse standard 41706. ## 8 &lt;split [1977/220]&gt; Fold01 1.15 0.0145 -0.643 mae standard 28971. ## 9 &lt;split [1977/220]&gt; Fold01 1.15 0.0145 -0.643 mape standard 19.7 ## 10 &lt;split [1977/220]&gt; Fold01 1.20 103. -1.51 rmse standard 98356. ## # … with 2,990 more rows, and 3 more variables: .config &lt;chr&gt;, .notes &lt;list&gt;, ## # .predictions &lt;list&gt; Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) collect_metrics(svm_tune_result) ## # A tibble: 300 × 9 ## cost rbf_sigma margin .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.04 0.00515 0.543 mae standard 27905. 10 604. Preprocesso… ## 2 1.04 0.00515 0.543 mape standard 17.2 10 0.579 Preprocesso… ## 3 1.04 0.00515 0.543 rmse standard 39638. 10 1268. Preprocesso… ## 4 1.12 688. 1.55 mae standard 79477. 10 903. Preprocesso… ## 5 1.12 688. 1.55 mape standard 59.3 10 1.23 Preprocesso… ## 6 1.12 688. 1.55 rmse standard 93686. 10 1127. Preprocesso… ## 7 1.15 0.0145 -0.643 mae standard 27913. 10 657. Preprocesso… ## 8 1.15 0.0145 -0.643 mape standard 17.7 10 0.600 Preprocesso… ## 9 1.15 0.0145 -0.643 rmse standard 39121. 10 1424. Preprocesso… ## 10 1.20 103. -1.51 mae standard 78820. 10 912. Preprocesso… ## # … with 290 more rows En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos: svm_tune_result %&gt;% autoplot() svm_tune_result %&gt;% show_best(n = 10, metric = &quot;mape&quot;) ## # A tibble: 10 × 9 ## cost rbf_sigma margin .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.24 0.0175 0.226 mape standard 14.3 10 0.558 Preprocessor1… ## 2 1.13 0.0211 0.319 mape standard 14.8 10 0.569 Preprocessor1… ## 3 1.38 0.0308 0.320 mape standard 14.9 10 0.574 Preprocessor1… ## 4 1.13 0.00549 -0.0512 mape standard 14.9 10 0.584 Preprocessor1… ## 5 1.04 0.00398 -0.0321 mape standard 15.3 10 0.611 Preprocessor1… ## 6 1.10 0.00303 0.0327 mape standard 15.5 10 0.623 Preprocessor1… ## 7 1.17 0.00383 0.353 mape standard 16.1 10 0.615 Preprocessor1… ## 8 1.06 0.00165 0.0561 mape standard 16.2 10 0.628 Preprocessor1… ## 9 1.28 0.00380 -0.388 mape standard 16.2 10 0.609 Preprocessor1… ## 10 1.36 0.00299 0.409 mape standard 16.6 10 0.619 Preprocessor1… Paso 8: Selección de modelo a usar # Selección del mejor modelo según la métrica MAPE svm_regression_best_model &lt;- select_best(svm_tune_result, metric = &quot;mape&quot;) svm_regression_best_model ## # A tibble: 1 × 4 ## cost rbf_sigma margin .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.24 0.0175 0.226 Preprocessor1_Model075 # Selección del modelo más regularizado a menos de una desviación estandar, según la métrica MAPE svm_regression_best_1se_model &lt;- svm_tune_result %&gt;% select_by_one_std_err(metric = &quot;mape&quot;, &quot;mape&quot;) svm_regression_best_1se_model ## # A tibble: 1 × 11 ## cost rbf_sigma margin .metric .estimator mean n std_err .config .best ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.13 0.0211 0.319 mape standard 14.8 10 0.569 Preproces… 14.3 ## # … with 1 more variable: .bound &lt;dbl&gt; Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) # Modelo final svm_regression_final_model &lt;- svm_workflow %&gt;% finalize_workflow(svm_regression_best_1se_model) %&gt;% parsnip::fit(data = ames_train) svm_regression_final_model ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: svm_rbf() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 11 Recipe Steps ## ## • step_unknown() ## • step_rename() ## • step_rename() ## • step_ratio() ## • step_mutate() ## • step_relevel() ## • step_normalize() ## • step_dummy() ## • step_interact() ## • step_interact() ## • ... ## • and 1 more step. ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: eps-svr (regression) ## parameter : epsilon = 0.318837530910969 cost C = 1.1300031747186 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 0.0210599956279017 ## ## Number of Support Vectors : 709 ## ## Objective Function Value : -221.4799 ## Training error : 0.149207 Como hemos hablado anteriormente, este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo library(vip) ames_importance &lt;- svm_regression_final_model %&gt;% extract_fit_parsnip() %&gt;% vi( method = &quot;permute&quot;, nsim = 10, target = &quot;Sale_Price&quot;, metric = &quot;rmse&quot;, pred_wrapper = kernlab::predict, train = juice(receta_casas) ) ames_importance %&gt;% saveRDS(&quot;models/vip_ames_svm.rds&quot;) ames_importance &lt;- readRDS(&quot;models/vip_ames_svm.rds&quot;) ames_importance ## # A tibble: 274 × 3 ## Variable Importance StDev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Year_Built 13445. 595. ## 2 Garage_Area 8866. 409. ## 3 TotRms_AbvGrd 8788. 393. ## 4 Fireplaces 5900. 475. ## 5 Mas_Vnr_Area 5731. 186. ## 6 Full_Bath 4544. 339. ## 7 Garage_Cars 3639. 368. ## 8 Lot_Area 2981. 261. ## 9 Bedroom_AbvGr 2564. 296. ## 10 BsmtFin_SF_1 2425. 156. ## # … with 264 more rows ames_importance %&gt;% mutate(Variable = forcats::fct_reorder(Variable, Importance)) %&gt;% slice_max(Importance, n = 20) %&gt;% ggplot(aes(Importance, Variable, color = Variable)) + geom_errorbar(aes(xmin = Importance - StDev, xmax = Importance + StDev), alpha = 0.5, size = 1.3) + geom_point(size = 3) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Variable Importance Measure&quot;) Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: # Predicciones results &lt;- predict(svm_regression_final_model, ames_test) %&gt;% dplyr::bind_cols(truth = ames_test$Sale_Price) %&gt;% dplyr::rename(pred_svm_reg = .pred, Sale_Price = truth) head(results) ## # A tibble: 6 × 2 ## pred_svm_reg Sale_Price ## &lt;dbl&gt; &lt;int&gt; ## 1 143363. 105000 ## 2 183725. 185000 ## 3 179908. 180400 ## 4 127895. 141000 ## 5 217114. 210000 ## 6 196308. 216000 results %&gt;% yardstick::metrics(Sale_Price, pred_svm_reg) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 36200. ## 2 rsq standard 0.798 ## 3 mae standard 24681. Es posible definir nuestro propio conjunto de metricas que deseamos reportar creando este objeto: multi_metric &lt;- metric_set(rmse, rsq, mae, mape, ccc) multi_metric(results, truth = Sale_Price, estimate = pred_svm_reg) %&gt;% mutate(.estimate = round(.estimate, 2)) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 36200. ## 2 rsq standard 0.8 ## 3 mae standard 24681. ## 4 mape standard 14.8 ## 5 ccc standard 0.89 results %&gt;% ggplot(aes(x = pred_svm_reg, y = Sale_Price)) + geom_point() + geom_abline(color = &quot;red&quot;) + xlab(&quot;Prediction&quot;) + ylab(&quot;Observation&quot;) + ggtitle(&quot;Comparisson&quot;) 3.7.2 Implementación de SVM en R Es turno de revisar la implementación de SVM con nuestro bien conocido problema de predicción de cancelación de servicios de telecomunicaciones. Los datos se encuentran disponibles en el siguiente enlace: Los pasos para implementar en R este modelo predictivo son los mismos, cambiando únicamente las especificaciones del tipo de modelo, pre-procesamiento e hiper-parámetros. library(tidyverse) library(tidymodels) library(readr) telco &lt;- read_csv(&quot;data/Churn.csv&quot;) glimpse(telco) ## Rows: 7,043 ## Columns: 21 ## $ customerID &lt;chr&gt; &quot;7590-VHVEG&quot;, &quot;5575-GNVDE&quot;, &quot;3668-QPYBK&quot;, &quot;7795-CFOCW… ## $ gender &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;,… ## $ SeniorCitizen &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Partner &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Dependents &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;… ## $ tenure &lt;dbl&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 58, 49, 2… ## $ PhoneService &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ MultipleLines &lt;chr&gt; &quot;No phone service&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No phone service&quot;, &quot;… ## $ InternetService &lt;chr&gt; &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;Fiber optic&quot;, &quot;Fiber opt… ## $ OnlineSecurity &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;… ## $ OnlineBackup &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;N… ## $ DeviceProtection &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… ## $ TechSupport &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ StreamingTV &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Ye… ## $ StreamingMovies &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Contract &lt;chr&gt; &quot;Month-to-month&quot;, &quot;One year&quot;, &quot;Month-to-month&quot;, &quot;One … ## $ PaperlessBilling &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ PaymentMethod &lt;chr&gt; &quot;Electronic check&quot;, &quot;Mailed check&quot;, &quot;Mailed check&quot;, &quot;… ## $ MonthlyCharges &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, 29.7… ## $ TotalCharges &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820.50, 1949… ## $ Churn &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… Paso 1: Separación inicial de datos ( test, train ) set.seed(1234) telco_split &lt;- initial_split(telco, prop = .70) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) telco_folds &lt;- vfold_cv(telco_train) telco_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [4437/493]&gt; Fold01 ## 2 &lt;split [4437/493]&gt; Fold02 ## 3 &lt;split [4437/493]&gt; Fold03 ## 4 &lt;split [4437/493]&gt; Fold04 ## 5 &lt;split [4437/493]&gt; Fold05 ## 6 &lt;split [4437/493]&gt; Fold06 ## 7 &lt;split [4437/493]&gt; Fold07 ## 8 &lt;split [4437/493]&gt; Fold08 ## 9 &lt;split [4437/493]&gt; Fold09 ## 10 &lt;split [4437/493]&gt; Fold10 Paso 2: Pre-procesamiento e ingeniería de variables binner &lt;- function(x) { x &lt;- cut(x, breaks = c(0, 12, 24, 36,48,60,72), include.lowest = TRUE) as.numeric(x) } telco_rec &lt;- recipe(Churn ~ ., data = telco_train) %&gt;% update_role(customerID, new_role = &quot;id variable&quot;) %&gt;% step_num2factor( tenure, transform = binner, levels = c(&quot;0-1 year&quot;, &quot;1-2 years&quot;, &quot;2-3 years&quot;, &quot;3-4 years&quot;, &quot;4-5 years&quot;, &quot;5-6 years&quot;)) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_rm(customerID, skip=T) %&gt;% prep() telco_rec ## Recipe ## ## Inputs: ## ## role #variables ## id variable 1 ## outcome 1 ## predictor 19 ## ## Training data contained 4930 data points and 10 incomplete rows. ## ## Operations: ## ## Factor variables from tenure [trained] ## Centering and scaling for SeniorCitizen, MonthlyCharges, TotalCharges [trained] ## Dummy variables from gender, Partner, Dependents, tenure, PhoneService, Multip... [trained] ## Median imputation for SeniorCitizen, MonthlyCharges, TotalCharges, ge... [trained] ## Variables removed customerID [trained] Paso 3: Selección de tipo de modelo con hiperparámetros iniciales svm_class_model &lt;- svm_rbf( mode = &quot;classification&quot;, cost = tune(), rbf_sigma = tune(), margin = tune()) %&gt;% set_engine(&quot;kernlab&quot;) svm_class_model ## Radial Basis Function Support Vector Machine Specification (classification) ## ## Main Arguments: ## cost = tune() ## rbf_sigma = tune() ## margin = tune() ## ## Computational engine: kernlab Paso 4: Inicialización de workflow o pipeline svm_class_workflow &lt;- workflow() %&gt;% add_recipe(telco_rec) %&gt;% add_model(svm_class_model) svm_class_workflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: svm_rbf() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 5 Recipe Steps ## ## • step_num2factor() ## • step_normalize() ## • step_dummy() ## • step_impute_median() ## • step_rm() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Radial Basis Function Support Vector Machine Specification (classification) ## ## Main Arguments: ## cost = tune() ## rbf_sigma = tune() ## margin = tune() ## ## Computational engine: kernlab Paso 5: Creación de grid search svm_class_parameters_set &lt;- svm_class_workflow %&gt;% hardhat::extract_parameter_set_dials() %&gt;% update( cost = cost(c(0, 0.5)), rbf_sigma = rbf_sigma(c(-3, 3)), margin = svm_margin(c(-2, 2)) ) set.seed(123) svm_class_grid &lt;- svm_class_parameters_set %&gt;% grid_max_entropy(size = 100) svm_class_grid ## # A tibble: 100 × 3 ## cost rbf_sigma margin ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.22 0.504 0.571 ## 2 1.41 1.22 0.329 ## 3 1.33 0.111 1.38 ## 4 1.35 0.0935 -1.65 ## 5 1.04 0.937 1.48 ## 6 1.14 0.0154 -1.91 ## 7 1.39 70.2 -1.98 ## 8 1.07 1.16 -0.487 ## 9 1.17 1.34 0.948 ## 10 1.02 0.0303 0.230 ## # … with 90 more rows ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) Paso 6: Entrenamiento de modelos con hiperparámetros definidos library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) svm1 &lt;- Sys.time() svm_tune_class_result &lt;- tune_grid( svm_class_workflow, resamples = telco_folds, grid = svm_class_grid, metrics = metric_set(roc_auc, pr_auc), control = ctrl_grid ) svm2 &lt;- Sys.time(); svm2 - svm1 stopCluster(cluster) svm_tune_class_result %&gt;% saveRDS(&quot;models/svm_model_class.rds&quot;) svm_tune_class_result &lt;- readRDS(&quot;models/svm_model_class.rds&quot;) unnest(svm_tune_class_result, .metrics) ## # A tibble: 1,966 × 11 ## splits id cost rbf_sigma margin .metric .estimator .estimate ## &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &lt;split [4437/493]&gt; Fold01 1.22 0.504 0.571 roc_auc binary 0.760 ## 2 &lt;split [4437/493]&gt; Fold01 1.22 0.504 0.571 pr_auc binary 0.854 ## 3 &lt;split [4437/493]&gt; Fold01 1.41 1.22 0.329 roc_auc binary 0.735 ## 4 &lt;split [4437/493]&gt; Fold01 1.41 1.22 0.329 pr_auc binary 0.851 ## 5 &lt;split [4437/493]&gt; Fold01 1.33 0.111 1.38 roc_auc binary 0.756 ## 6 &lt;split [4437/493]&gt; Fold01 1.33 0.111 1.38 pr_auc binary 0.859 ## 7 &lt;split [4437/493]&gt; Fold01 1.35 0.0935 -1.65 roc_auc binary 0.762 ## 8 &lt;split [4437/493]&gt; Fold01 1.35 0.0935 -1.65 pr_auc binary 0.866 ## 9 &lt;split [4437/493]&gt; Fold01 1.04 0.937 1.48 roc_auc binary 0.751 ## 10 &lt;split [4437/493]&gt; Fold01 1.04 0.937 1.48 pr_auc binary 0.859 ## # … with 1,956 more rows, and 3 more variables: .config &lt;chr&gt;, .notes &lt;list&gt;, ## # .predictions &lt;list&gt; Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) collect_metrics(svm_tune_class_result) ## # A tibble: 200 × 9 ## cost rbf_sigma margin .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.22 0.504 0.571 pr_auc binary 0.867 10 0.00769 Preprocessor1_… ## 2 1.22 0.504 0.571 roc_auc binary 0.767 10 0.00534 Preprocessor1_… ## 3 1.41 1.22 0.329 pr_auc binary 0.868 10 0.00710 Preprocessor1_… ## 4 1.41 1.22 0.329 roc_auc binary 0.764 10 0.00778 Preprocessor1_… ## 5 1.33 0.111 1.38 pr_auc binary 0.878 10 0.00588 Preprocessor1_… ## 6 1.33 0.111 1.38 roc_auc binary 0.776 10 0.00505 Preprocessor1_… ## 7 1.35 0.0935 -1.65 pr_auc binary 0.880 10 0.00569 Preprocessor1_… ## 8 1.35 0.0935 -1.65 roc_auc binary 0.779 10 0.00499 Preprocessor1_… ## 9 1.04 0.937 1.48 pr_auc binary 0.871 10 0.00637 Preprocessor1_… ## 10 1.04 0.937 1.48 roc_auc binary 0.769 10 0.00654 Preprocessor1_… ## # … with 190 more rows En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos. svm_tune_class_result %&gt;% autoplot() svm_tune_class_result %&gt;% show_best(n = 10, metric = &quot;roc_auc&quot;) ## # A tibble: 10 × 9 ## cost rbf_sigma margin .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.23 0.00686 -1.20 roc_auc binary 0.808 7 0.00593 Preprocessor1_… ## 2 1.02 0.0303 0.230 roc_auc binary 0.805 9 0.00529 Preprocessor1_… ## 3 1.05 0.00823 0.904 roc_auc binary 0.805 9 0.00624 Preprocessor1_… ## 4 1.14 0.0215 1.11 roc_auc binary 0.804 10 0.00505 Preprocessor1_… ## 5 1.14 0.0154 -1.91 roc_auc binary 0.803 8 0.00625 Preprocessor1_… ## 6 1.19 0.0306 -1.45 roc_auc binary 0.803 10 0.00517 Preprocessor1_… ## 7 1.06 0.0350 -1.54 roc_auc binary 0.803 10 0.00517 Preprocessor1_… ## 8 1.22 0.0360 0.552 roc_auc binary 0.803 10 0.00517 Preprocessor1_… ## 9 1.36 0.0368 0.596 roc_auc binary 0.803 10 0.00515 Preprocessor1_… ## 10 1.11 0.00676 -1.42 roc_auc binary 0.802 9 0.00670 Preprocessor1_… Paso 8: Selección de modelo a usar # Selección del mejor modelo según la métrica ROC AUC svm_classification_best_model &lt;- select_best(svm_tune_class_result, metric = &quot;roc_auc&quot;) svm_classification_best_model ## # A tibble: 1 × 4 ## cost rbf_sigma margin .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.23 0.00686 -1.20 Preprocessor1_Model043 # Selección del modelo más regularizado a menos de una desviación estandar, según la métrica ROC AUC svm_classification_best_1se_model &lt;- svm_tune_class_result %&gt;% select_by_one_std_err(metric = &quot;roc_auc&quot;, &quot;roc_auc&quot;) svm_classification_best_1se_model ## # A tibble: 1 × 11 ## cost rbf_sigma margin .metric .estimator mean n std_err .config .best ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.14 0.0154 -1.91 roc_auc binary 0.803 8 0.00625 Preproces… 0.808 ## # … with 1 more variable: .bound &lt;dbl&gt; Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) # Modelo final set.seed(1352) svm_classification_final_model &lt;- svm_class_workflow %&gt;% finalize_workflow(svm_classification_best_model) %&gt;% parsnip::fit(data = telco_train) svm_classification_final_model ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: svm_rbf() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 5 Recipe Steps ## ## • step_num2factor() ## • step_normalize() ## • step_dummy() ## • step_impute_median() ## • step_rm() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1.22744145128527 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 0.00685742851317484 ## ## Number of Support Vectors : 2580 ## ## Objective Function Value : -3090.757 ## Training error : 0.223327 ## Probability model included. Como hemos hablado anteriormente, este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo. churn_importance &lt;- svm_classification_final_model %&gt;% extract_fit_parsnip() %&gt;% vi( method = &quot;permute&quot;, nsim = 10, target = &quot;Churn&quot;, metric = &quot;auc&quot;, reference_class = &quot;Yes&quot;, pred_wrapper = kernlab::predict, train = juice(telco_rec) ) churn_importance %&gt;% saveRDS(&quot;models/vip_telco_svm.rds&quot;) churn_importance &lt;- readRDS(&quot;models/vip_telco_svm.rds&quot;) churn_importance ## # A tibble: 34 × 3 ## Variable Importance StDev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TotalCharges 0.0637 0.00487 ## 2 MonthlyCharges 0.0539 0.00983 ## 3 SeniorCitizen 0.0219 0.00515 ## 4 gender_Male 0 0 ## 5 Partner_Yes 0 0 ## 6 Dependents_Yes 0 0 ## 7 tenure_X1.2.years 0 0 ## 8 tenure_X2.3.years 0 0 ## 9 tenure_X3.4.years 0 0 ## 10 tenure_X4.5.years 0 0 ## # … with 24 more rows churn_importance %&gt;% mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% ggplot(aes(Importance, Variable, color = Variable)) + geom_errorbar(aes(xmin = Importance - StDev, xmax = Importance + StDev), alpha = 0.5, size = 1.3) + geom_point(size = 3) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Variable Importance Measure&quot;) Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: # Predicciones telco_test &lt;- testing(telco_split) results_cla &lt;- predict(svm_classification_final_model, telco_test, type = &quot;prob&quot;) %&gt;% dplyr::bind_cols(truth = telco_test$Churn) %&gt;% mutate(truth = factor(truth, levels = c(&#39;No&#39;, &#39;Yes&#39;), labels = c(&#39;No&#39;, &#39;Yes&#39;))) head(results_cla) ## # A tibble: 6 × 3 ## .pred_No .pred_Yes truth ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.854 0.146 No ## 2 0.247 0.753 Yes ## 3 0.822 0.178 No ## 4 0.871 0.129 No ## 5 0.923 0.0771 No ## 6 0.547 0.453 No roc_curve_data &lt;- roc_curve( results_cla, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39; ) roc_curve_data ## # A tibble: 2,083 × 3 ## .threshold specificity sensitivity ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -Inf 0 1 ## 2 0.0331 0 1 ## 3 0.0339 0.000646 1 ## 4 0.0339 0.00129 1 ## 5 0.0340 0.00194 1 ## 6 0.0348 0.00259 1 ## 7 0.0349 0.00323 1 ## 8 0.0356 0.00323 0.998 ## 9 0.0357 0.00388 0.998 ## 10 0.0361 0.00452 0.998 ## # … with 2,073 more rows roc_curve_plot &lt;- roc_curve_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;)+ theme_minimal() roc_curve_plot pr_curve_data &lt;- pr_curve( results_cla, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39; ) pr_curve_data ## # A tibble: 2,082 × 3 ## .threshold recall precision ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Inf 0 1 ## 2 0.964 0.00177 1 ## 3 0.946 0.00353 1 ## 4 0.946 0.00530 1 ## 5 0.929 0.00707 1 ## 6 0.921 0.00707 0.8 ## 7 0.918 0.00883 0.833 ## 8 0.918 0.0106 0.857 ## 9 0.915 0.0124 0.875 ## 10 0.908 0.0141 0.889 ## # … with 2,072 more rows pr_curve_plot &lt;- pr_curve_data %&gt;% ggplot(aes(x = recall, y = precision)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;)+ theme_minimal() pr_curve_plot Pueden usar la app de shiny que nos permite jugar con el treshold de clasificación para tomar la mejor decisión. 3.8 Ejercicios Crear un workflow de principio a fin usando SVM con kernel lineal Crear un workflow de principio a fin usando SVM con kernel polinomial Crear un workflow de principio a fin usando SVR con kernel lineal Crear un workflow de principio a fin usando SVR con kernel polinomial Comparar resultados con ejercicio desarrollado en clase "],["bagging-boosting.html", "Capítulo 4 Bagging &amp; Boosting 4.1 Aprendizaje conjunto 4.2 Bagging vs. boosting 4.3 Algoritmo Bagging 4.4 Algoritmo Boosting 4.5 Ejercicios", " Capítulo 4 Bagging &amp; Boosting El bagging o agregación bootstrap, es un método de aprendizaje por conjuntos que se usa comúnmente para reducir la varianza dentro de un conjunto de datos ruidoso. En este método, se selecciona una muestra aleatoria de datos en un conjunto de entrenamiento con reemplazo, lo que significa que los puntos de datos individuales se pueden elegir más de una vez. Después de generar varias muestras de datos, estos modelos se entrenan de forma independiente y, según el tipo de tarea (regresión o clasificación), el promedio o la mayoría de esas predicciones producen una estimación más precisa. Nota: El algoritmo de bosque aleatorio se considera una extensión del método de bagging, utilizando tanto bagging como la aleatoriedad de características para crear un bosque no correlacionado de árboles de decisión. 4.1 Aprendizaje conjunto El aprendizaje conjunto da crédito a la idea de la “sabiduría de las multitudes,” lo que sugiere que la toma de decisiones de un grupo más grande de individuos (modelos) suele ser mejor que la de un individuo. El aprendizaje en conjunto es un grupo (o conjunto) de individuos o modelos, que trabajan colectivamente para lograr una mejor predicción final. Un solo modelo, también conocido como aprendiz básico puede no funcionar bien individualmente debido a una gran variación o un alto sesgo, sin embargo, cuando se agregan individuos débiles, pueden formar un individuo fuerte, ya que su combinación reduce el sesgo o la varianza, lo que produce un mejor rendimiento del modelo. Los métodos de conjunto se ilustran con frecuencia utilizando árboles de decisión, ya que este algoritmo puede ser propenso a sobreajustar (alta varianza y bajo sesgo) y también puede prestarse a desajuste (baja varianza y alto sesgo) cuando es muy pequeño, como un árbol de decisión con un nivel. Nota: Cuando un algoritmo se adapta o no se adapta a su conjunto de entrenamiento, no se puede generalizar bien a nuevos conjuntos de datos, por lo que se utilizan métodos de conjunto para contrarrestar este comportamiento y permitir la generalización del modelo a nuevos conjuntos de datos. 4.2 Bagging vs. boosting Bagging y el boosting (refuerzo o impulso) son dos tipos principales de métodos de aprendizaje por conjuntos. La principal diferencia entre estos métodos de aprendizaje es la forma en que se capacitan. En bagging, los modelos se entrenan en paralelo, pero en el boosting, aprenden secuencialmente. Esto significa que se construyen una serie de modelos y con cada nueva iteración del modelo, se incrementan los pesos de los datos clasificados erróneamente en el modelo anterior. Esta redistribución de pesos ayuda al algoritmo a identificar los parámetros en los que necesita enfocarse para mejorar su desempeño. Un ejemplo de modelo secuencial es: Adaboost y significa “algoritmo de boosting adaptativo,” es uno de los algoritmos de boosting más populares, ya que fue uno de los primeros de su tipo. Otros tipos de algoritmos de booting incluyen XGBoost, GradientBoost y BrownBoost. Otra diferencia en la que difieren bagging y boosting son los escenarios en los que se utilizan. Por ejemplo, los métodos de bagging se utilizan típicamente en modelos débiles que exhiben alta varianza y bajo sesgo, mientras que los métodos de boosting se aprovechan cuando se observa baja varianza y alto sesgo. ¡¡ RECORDAR !! Bagging realiza replicaciones bootstrap y ajusta un árbol a cada muestra de manera independiente, mientras que boosting ajusta un árbol a una versión modificada del conjunto original de datos, la cual se modifica en cada iteración de entrenamiento. 4.2.1 Error Out-Of-Bag Este error es conocido como “OOB.” Se trata de un enfoque distinto a KFCV en donde el error predictivo es calculado a través de los elementos que no fueron seleccionados en la muestra bootstrap. Recordemos que en las muestras bootstrap algunos elementos son seleccionados más de una vez, mientras que otros no aparecen en la muestra. Empíricamente, en cada replicación bootstrap se observan 2/3 partes de la muestra y el resto queda “fuera de la bolsa” (OOB) de entrenamiento. Si B es el número de replicaciones bootstrap, entonces cada observación i recibe cerca de B/3 predicciones, las cuales son usadas para estimar el error predictivo. Para obtener una única predicción en cada observación, las B/3 predicciones son promediadas. 4.3 Algoritmo Bagging Bootstrapping: Bagging aprovecha una técnica de muestreo de bootstrapping para crear muestras diversas. Este método de remuestreo genera diferentes subconjuntos a partir del conjunto de datos de entrenamiento original seleccionando puntos de datos al azar y con reemplazo. Esto significa que cada vez que selecciona un punto del conjunto de entrenamiento, puede seleccionar la misma instancia varias veces. Como resultado, un valor se repite dos veces (o más) en una muestra y algunos no aparecen. Entrenamiento paralelo: estos ejemplos de bootstrap se entrenan de forma independiente y en paralelo entre sí utilizando modelos débiles o básicos. Agregación: Finalmente, dependiendo de la tarea (regresión o clasificación), se toma un promedio o la mayoría de las predicciones para calcular una estimación más precisa. En el caso de la regresión, se toma un promedio de todos los resultados predichos por los clasificadores individuales; esto se conoce como votación suave. Para problemas de clasificación, se acepta la clase con mayor mayoría de votos; esto se conoce como votación en firme o votación por mayoría. 4.3.1 Ventajas y desventajas de bagging Hay una serie de ventajas y desventajas clave que presenta el método de bagging cuando se usa para problemas de clasificación o regresión. Ventajas Facilidad de implementación: las bibliotecas de R como tidymodels facilitan la combinación de las predicciones de los aprendices o estimadores base para mejorar el rendimiento del modelo. Reducción de varianza: bagging puede reducir la varianza dentro de un algoritmo de aprendizaje. Esto es particularmente útil con datos de alta dimensión, donde los valores faltantes pueden conducir a una mayor varianza, lo que los hace más propensos a sobreajustarse y evitar la generalización precisa a nuevos conjuntos de datos. Desventajas Pérdida de interpretabilidad: es difícil obtener información empresarial muy precisa a través del bagging debido al promedio involucrado en las predicciones. Si bien el resultado es más preciso que cualquier punto de datos individual, un conjunto de datos más exacto o completo también podría producir más precisión dentro de un solo modelo de clasificación o regresión. Computacionalmente costoso: bagging se ralentiza y se vuelve más intensivo a medida que aumenta el número de iteraciones. Por lo tanto, no es adecuado para aplicaciones en tiempo real. Los sistemas agrupados o una gran cantidad de núcleos de procesamiento son ideales para crear rápidamente conjuntos en bolsas en conjuntos de prueba grandes. Menos flexible: como técnica, bagging funciona particularmente bien con algoritmos que son menos estables. Uno que sea más estable o esté sujeto a grandes cantidades de sesgo no proporciona tanto beneficio ya que hay menos variación dentro del conjunto de datos del modelo. 4.3.2 Aplicaciónes de Bagging La técnica de bagging se utiliza en una gran cantidad de industrias, proporcionando información sobre el valor del mundo real y perspectivas interesantes. Los casos de uso clave incluyen: TI: bagging también puede mejorar la precisión y exactitud en los sistemas de TI, como los sistemas de detección de intrusiones en la red. Medio ambiente: los métodos de conjunto, como bagging, se han aplicado en el campo de la teledetección (técnica de adquisición de datos de la superficie terrestre desde sensores instalados en plataformas espaciales). Finanzas: bagging también se ha aprovechado con modelos de aprendizaje profundo en la industria financiera, automatizando tareas críticas, incluida la detección de fraudes, evaluaciones de riesgo crediticio y problemas de precios de opciones. 4.3.3 Implementación en R library(tidymodels) library(rsample) data(ames) set.seed(20211212) ames_boot &lt;- bootstraps(ames, times = 500, apparent = TRUE) # Se crean muestras bootstrap # Se entrena un modelo para cada muestra. ames_models &lt;- ames_boot %&gt;% mutate( model = map( splits, ~ lm(Sale_Price ~ 0 + log10(Gr_Liv_Area) + Full_Bath + Year_Built, data = .)), coef_info = map(model, tidy) ) ames_coefs &lt;- ames_models %&gt;% unnest(coef_info) ames_coefs ## # A tibble: 1,503 × 8 ## splits id model term estimate std.error statistic p.value ## &lt;list&gt; &lt;chr&gt; &lt;lis&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &lt;split [2930/1106]&gt; Bootst… &lt;lm&gt; log1… 206873. 9544. 21.7 9.71e-97 ## 2 &lt;split [2930/1106]&gt; Bootst… &lt;lm&gt; Full… 49994. 2434. 20.5 1.14e-87 ## 3 &lt;split [2930/1106]&gt; Bootst… &lt;lm&gt; Year… -278. 14.4 -19.4 9.39e-79 ## 4 &lt;split [2930/1082]&gt; Bootst… &lt;lm&gt; log1… 208068. 9997. 20.8 7.66e-90 ## 5 &lt;split [2930/1082]&gt; Bootst… &lt;lm&gt; Full… 49639. 2541. 19.5 5.35e-80 ## 6 &lt;split [2930/1082]&gt; Bootst… &lt;lm&gt; Year… -281. 15.1 -18.6 2.40e-73 ## 7 &lt;split [2930/1055]&gt; Bootst… &lt;lm&gt; log1… 212558. 9685. 21.9 5.78e-99 ## 8 &lt;split [2930/1055]&gt; Bootst… &lt;lm&gt; Full… 48700. 2454. 19.8 2.32e-82 ## 9 &lt;split [2930/1055]&gt; Bootst… &lt;lm&gt; Year… -287. 14.6 -19.7 3.45e-81 ## 10 &lt;split [2930/1074]&gt; Bootst… &lt;lm&gt; log1… 205522. 9420. 21.8 6.60e-98 ## # … with 1,493 more rows # Evaluación de resultados ames_coefs %&gt;% ggplot(aes(estimate)) + geom_histogram(fill = &quot;light blue&quot;)+ facet_wrap(~term, scales = &quot;free_x&quot;)+ scale_x_continuous(labels = comma)+ theme_minimal() 4.4 Algoritmo Boosting Tradicionalmente, la construcción de una aplicación de aprendizaje automático consistía en tomar un solo estimador, es decir: Un regresor logístico Un árbol de decisión Una máquina de vectores de soporte Una red neuronal artificial Para posteriormente ser entrenado por un conjunto de datos. Luego nacieron los métodos de conjunto, los cuales pueden describirse como técnicas que utilizan un grupo de modelos “débiles” juntos, con el fin de crear uno más fuerte y agregado. El Boosting consiste en la idea de filtrar o ponderar los datos que se utilizan para capacitar a nuestro conjunto de modelos “débiles,” para que cada nuevo modelo pondere o “solo se entrene” con observaciones que han sido mal clasificadas por los anteriores modelos. Al hacer esto, nuestro conjunto de modelos aprende a hacer predicciones precisas sobre todo tipo de datos, no solo sobre las observaciones más comunes o fáciles. Además, si uno de los modelos individuales es muy malo para hacer predicciones sobre algún tipo de observación, no importa, ya que los otros \\(N - 1\\) modelos probablemente lo compensarán. Como se puede ver en la imagen anterior, en boosting el conjunto de datos se pondera (representado por los diferentes tamaños de los datos), de modo que las observaciones que fueron clasificadas incorrectamente por el clasificador \\(n\\) reciben más importancia en el entrenamiento del modelo \\(n + 1\\). En general, los métodos de conjunto reducen el sesgo y la varianza de nuestros modelos de aprendizaje automático. ¡¡ RECORDAR !! Los modelos bootstrap buscan aprender lentamente patrones relevantes a lo largo de muchas iteraciones, de forma que se vaya haciendo un ajuste lento pero preciso. El proceso de entrenamiento depende del algoritmo boosting que estemos usando (Adaboost, LigthGBM, XGBoost, \\(\\dots\\)), pero generalmente sigue este patrón: Todas las muestras de datos comienzan con los mismos pesos. Estas muestras se utilizan para entrenar un modelo individual (digamos un árbol de decisión). Se calcula el error de predicción para cada muestra, aumentando los pesos de aquellas muestras que han tenido un error mayor, para hacerlas más importantes para el entrenamiento del siguiente modelo individual. Dependiendo de qué tan bien le fue a este modelo individual en sus predicciones, se le asigna una importancia/peso. Los datos ponderados se pasan al modelo posterior y se repiten lo pasos 2) y 3). Este paso se repite hasta que se haya alcanzado un cierto número de modelos o hasta que el error esté por debajo de un cierto umbral. En algunos casos, los modelos de boosting se entrenan con un peso fijo específico para cada modelo (llamado tasa de aprendizaje) y en lugar de dar a cada muestra un peso individual, los modelos se entrenan tratando de predecir las diferencias entre las predicciones anteriores en las muestras y los valores reales de la variable objetivo. Esta diferencia es conocida como residuales. La forma de ajustar el modelo sigue los siguientes pasos: Se fija \\(\\hat{f}(x)=0\\) y \\(r_i=y_i\\) para todos los elementos del conjunto de entrenamiento Para \\(b=1,2,...,B\\), repetir: Ajustar un árbol \\(\\hat{f}^b\\) al conjunto de entrenamiento \\((X, r)\\) Actualizar el ajuste \\(\\hat{f}(x)\\) al añadir una nueva versión restringida de un nuevo árbol: \\[\\hat{F}_b(X) \\leftarrow \\hat{F}_{b-1}(X) + \\alpha_b\\hat{h}_b(X, r_{b-1})\\] c) Actualizar los residuos: \\[r_b \\leftarrow r_{b-1} - \\alpha_b\\hat{f}^b(x_i)\\] Resultado del modelo Boosting: \\[\\hat{F}=\\sum_{b=1}^{B}\\alpha_b\\hat{F}_b(x)\\] Para calcular \\(\\alpha_b\\) en cada iteración, se usa la siguiente fórmula: \\[\\underset{\\alpha}{\\operatorname{argmin}}=\\sum_{i=1}^{b}{L(Y_i, \\hat{F}_{i-1}(X_i)+\\alpha \\hat{h}_i(X_i, r_{i-1}))}\\] Donde \\(L(Y, F(X))\\) es una función de pérdida diferenciable. 4.4.1 Predicciones de Boosting La forma en que un modelo de boosting hace predicciones sobre nuevos datos es muy simple. Cuando obtenemos una nueva observación con sus características, se pasa a través de cada uno de los modelos individuales, haciendo que cada modelo haga su propia predicción. Luego, teniendo en cuenta el peso de cada uno de estos modelos, todas estas predicciones se escalan y combinan, y se da una predicción global final. 4.4.2 Modelos Boosting XGBoost Abreviatura de eXtreme Gradient Boosting, como en Gradient Boosting, ajustamos los árboles a los residuos de las predicciones de árboles anteriores, sin embargo, en lugar de usar árboles de decisión de tamaño fijo convencionales, XGBoost usa un tipo diferente de árboles. Estos árboles se construyen calculando puntuaciones de similitud entre las observaciones que terminan en un nodo de salida. Además, XGBoost permite la regularización, reduciendo el posible sobreajuste de nuestros árboles individuales y, por lo tanto, del modelo de conjunto general. Por último, XGBoost está optimizado para superar el límite de los recursos computacionales de los algoritmos de árbol impulsados, lo que lo convierte en un algoritmo rápido y de muy alto rendimiento en términos de tiempo y cálculo. Adaboost Abreviatura de Adaptive Boosting, AdaBoost funciona mediante el proceso descrito anteriormente de entrenar secuencialmente, predecir y actualizar los pesos de las muestras mal clasificadas y de los modelos débiles correspondientes. Se usa principalmente con Decision Tree Stumps: árboles de decisión con solo un nodo raíz y dos nodos de salida, donde solo se evalúa una característica de los datos. Como podemos ver, al tener en cuenta solo una característica de nuestros datos para hacer predicciones, cada pivote es un modelo muy débil. Sin embargo, al combinar muchos de ellos, se puede construir un modelo de conjunto muy robusto y preciso. 4.4.3 Implementación en R 4.4.4 XGBoost para regresión Paso 1: Separación inicial de datos (test, train) library(tidymodels) data(ames) set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) ames_folds &lt;- vfold_cv(ames_train) Contando con datos de entrenamiento, procedemos a realizar el feature engineering para extraer las mejores características que permitirán realizar las estimaciones en el modelo. Paso 2: Pre-procesamiento e ingeniería de variables receta_casas &lt;- recipe(Sale_Price ~ . , data = ames_train) %&gt;% step_unknown(Alley) %&gt;% step_rename(Year_Remod = Year_Remod_Add) %&gt;% step_rename(ThirdSsn_Porch = Three_season_porch) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod, TotalSF = Gr_Liv_Area + Total_Bsmt_SF, AvgRoomSF = Gr_Liv_Area / TotRms_AbvGrd, Pool = if_else(Pool_Area &gt; 0, 1, 0), Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_predictors(), -all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~ Second_Flr_SF:First_Flr_SF) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% step_rm( First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath, Bsmt_Half_Bath, Kitchen_AbvGr, BsmtFin_Type_1_Unf, Total_Bsmt_SF, Kitchen_AbvGr, Pool_Area, Gr_Liv_Area, Sale_Type_Oth, Sale_Type_VWD ) %&gt;% prep() receta_casas ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 73 ## ## Training data contained 2197 data points and no missing data. ## ## Operations: ## ## Unknown factor level assignment for Alley [trained] ## Variable renaming for Year_Remod [trained] ## Variable renaming for ThirdSsn_Porch [trained] ## Ratios from Bedroom_AbvGr, Gr_Liv_Area [trained] ## Variable mutation for ~Year_Sold - Year_Remod, ~Gr_Liv_Area + To... [trained] ## Re-order factor level to ref_level for Exter_Cond [trained] ## Centering and scaling for Lot_Frontage, Lot_Area, Year_Built, Year_Remod,... [trained] ## Dummy variables from MS_SubClass, MS_Zoning, Street, Alley, Lot_Shape, Land_Co... [trained] ## Interactions with Second_Flr_SF:First_Flr_SF [trained] ## Interactions with (Bsmt_Cond_Fair + Bsmt_Cond_Good + Bsmt_Cond_No_Ba... [trained] ## Variables removed First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath... [trained] Recordemos que la función recipe() solo son los pasos a seguir, necesitamos usar la función prep() que nos devuelve una receta actualizada con las estimaciones y la función juice() que nos devuelve la matriz de diseño. Paso 3: Selección de tipo de modelo con hiperparámetros iniciales xgboost_reg_model &lt;- boost_tree( mode = &quot;regression&quot;, trees = 1000, tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), learn_rate = tune() ) %&gt;% set_engine( &quot;xgboost&quot;, importance = &quot;impurity&quot; ) xgboost_reg_model ## Boosted Tree Model Specification (regression) ## ## Main Arguments: ## mtry = tune() ## trees = 1000 ## min_n = tune() ## tree_depth = tune() ## learn_rate = tune() ## loss_reduction = tune() ## sample_size = tune() ## ## Engine-Specific Arguments: ## importance = impurity ## ## Computational engine: xgboost Paso 4: Inicialización de workflow o pipeline xgboost_reg_workflow &lt;- workflow() %&gt;% add_model(xgboost_reg_model) %&gt;% add_recipe(receta_casas) xgboost_reg_workflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: boost_tree() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 11 Recipe Steps ## ## • step_unknown() ## • step_rename() ## • step_rename() ## • step_ratio() ## • step_mutate() ## • step_relevel() ## • step_normalize() ## • step_dummy() ## • step_interact() ## • step_interact() ## • ... ## • and 1 more step. ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Boosted Tree Model Specification (regression) ## ## Main Arguments: ## mtry = tune() ## trees = 1000 ## min_n = tune() ## tree_depth = tune() ## learn_rate = tune() ## loss_reduction = tune() ## sample_size = tune() ## ## Engine-Specific Arguments: ## importance = impurity ## ## Computational engine: xgboost Paso 5: Creación de grid search xgboost_param_grid &lt;- grid_latin_hypercube( tree_depth(range = c(3, 50)), min_n(range = c(2,50)), loss_reduction(range = c(-10, 1.5), trans = log10_trans()), learn_rate(range = c(-6, -0.25), trans = log10_trans()), mtry(range = c(1, 70)), sample_size = sample_prop(), size = 1000 ) xgboost_param_grid ## # A tibble: 1,000 × 6 ## tree_depth min_n loss_reduction learn_rate mtry sample_size ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 8 50 7.99 0.0000189 13 0.910 ## 2 7 47 0.000000222 0.144 46 0.986 ## 3 32 6 0.00000000222 0.00000352 24 0.750 ## 4 29 35 0.00199 0.0000131 16 0.621 ## 5 35 16 0.00000646 0.00000246 59 0.560 ## 6 15 48 1.46 0.000118 51 0.112 ## 7 4 4 0.000000330 0.00000630 26 0.154 ## 8 27 10 0.000426 0.269 17 0.278 ## 9 50 7 0.807 0.000384 17 0.262 ## 10 38 28 0.00970 0.00419 68 0.576 ## # … with 990 more rows Paso 6: Entrenamiento de modelos con hiperparámetros definidos UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) xgb1 &lt;- Sys.time() xgboost_reg_tune_result &lt;- tune_grid( xgboost_reg_workflow, resamples = ames_folds, grid = xgboost_param_grid, metrics = metric_set(rmse, mae, mape, rsq), control = ctrl_grid ) xgb2 &lt;- Sys.time(); xgb2 - xgb1 stopCluster(cluster) xgboost_reg_tune_result %&gt;% saveRDS(&quot;models/xgboost_model_reg.rds&quot;) Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) xgboost_reg_tune_result &lt;- readRDS(&quot;models/xgboost_model_reg.rds&quot;) collect_metrics(xgboost_reg_tune_result) ## # A tibble: 4,000 × 12 ## mtry min_n tree_depth learn_rate loss_reduction sample_size .metric ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 58 36 36 0.0000895 0.000000405 0.965 mae ## 2 58 36 36 0.0000895 0.000000405 0.965 mape ## 3 58 36 36 0.0000895 0.000000405 0.965 rmse ## 4 58 36 36 0.0000895 0.000000405 0.965 rsq ## 5 45 18 27 0.0000425 0.572 0.314 mae ## 6 45 18 27 0.0000425 0.572 0.314 mape ## 7 45 18 27 0.0000425 0.572 0.314 rmse ## 8 45 18 27 0.0000425 0.572 0.314 rsq ## 9 22 12 25 0.0000154 0.000925 0.287 mae ## 10 22 12 25 0.0000154 0.000925 0.287 mape ## # … with 3,990 more rows, and 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, ## # n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt; En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos: xgboost_reg_tune_result %&gt;% autoplot() show_best(xgboost_reg_tune_result, n = 10, metric = &quot;rsq&quot;) %&gt;% select(mtry:sample_size, mean:std_err, -n) ## # A tibble: 10 × 8 ## mtry min_n tree_depth learn_rate loss_reduction sample_size mean std_err ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 27 9 4 0.0272 0.000106 0.625 0.853 0.00553 ## 2 5 24 4 0.0855 8.75 0.960 0.853 0.00678 ## 3 6 12 17 0.0186 0.00000260 0.520 0.851 0.00489 ## 4 4 8 15 0.0232 0.00000000536 0.371 0.851 0.00443 ## 5 7 24 6 0.0800 0.0311 0.953 0.850 0.00654 ## 6 4 5 14 0.00780 0.0457 0.839 0.850 0.00570 ## 7 5 27 47 0.0188 0.00298 0.913 0.848 0.00559 ## 8 5 8 40 0.0190 0.00000862 0.342 0.848 0.00627 ## 9 10 7 11 0.0195 1.01 0.289 0.848 0.00708 ## 10 5 19 44 0.0491 0.0000656 0.561 0.847 0.00658 Paso 8: Selección de modelo a usar best_xgboost_reg_model &lt;- select_best(xgboost_reg_tune_result, metric = &quot;rsq&quot;) best_xgboost_reg_model ## # A tibble: 1 × 7 ## mtry min_n tree_depth learn_rate loss_reduction sample_size .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 27 9 4 0.0272 0.000106 0.625 Preprocessor1_Mo… best_xgboost_reg_1se_model &lt;- xgboost_reg_tune_result %&gt;% select_by_one_std_err(metric = &quot;rsq&quot;, &quot;rsq&quot;) best_xgboost_reg_1se_model ## # A tibble: 1 × 14 ## mtry min_n tree_depth learn_rate loss_reduction sample_size .metric ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4 5 14 0.00780 0.0457 0.839 rsq ## # … with 7 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, ## # std_err &lt;dbl&gt;, .config &lt;chr&gt;, .best &lt;dbl&gt;, .bound &lt;dbl&gt; Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) final_xgboost_reg_model &lt;- xgboost_reg_workflow %&gt;% #finalize_workflow(best_xgboost_model) %&gt;% finalize_workflow(best_xgboost_reg_1se_model) %&gt;% fit(data = ames_train) ## [17:33:15] WARNING: amalgamation/../src/learner.cc:627: ## Parameters: { &quot;importance&quot; } might not be used. ## ## This could be a false alarm, with some parameters getting used by language bindings but ## then being mistakenly passed down to XGBoost core, or some parameter actually being used ## but getting flagged wrongly here. Please open an issue if you find any such cases. final_xgboost_reg_model ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: boost_tree() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 11 Recipe Steps ## ## • step_unknown() ## • step_rename() ## • step_rename() ## • step_ratio() ## • step_mutate() ## • step_relevel() ## • step_normalize() ## • step_dummy() ## • step_interact() ## • step_interact() ## • ... ## • and 1 more step. ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## ##### xgb.Booster ## raw: 9.1 Mb ## call: ## xgboost::xgb.train(params = list(eta = 0.0078034925239406, max_depth = 14L, ## gamma = 0.0457345133884295, colsample_bytree = 1, colsample_bynode = 0.173913043478261, ## min_child_weight = 5L, subsample = 0.83899691531139, objective = &quot;reg:squarederror&quot;), ## data = x$data, nrounds = 1000, watchlist = x$watchlist, verbose = 0, ## importance = &quot;impurity&quot;, nthread = 1) ## params (as set within xgb.train): ## eta = &quot;0.0078034925239406&quot;, max_depth = &quot;14&quot;, gamma = &quot;0.0457345133884295&quot;, colsample_bytree = &quot;1&quot;, colsample_bynode = &quot;0.173913043478261&quot;, min_child_weight = &quot;5&quot;, subsample = &quot;0.83899691531139&quot;, objective = &quot;reg:squarederror&quot;, importance = &quot;impurity&quot;, nthread = &quot;1&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.evaluation.log() ## # of features: 23 ## niter: 1000 ## nfeatures : 23 ## evaluation_log: ## iter training_rmse ## 1 195903.219 ## 2 194485.609 ## --- ## 999 7337.960 ## 1000 7331.632 Como hemos hablado anteriormente, este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo library(vip) final_xgboost_reg_model %&gt;% extract_fit_parsnip() %&gt;% vip::vip(num_features = 25) + ggtitle(&quot;Importancia de las variables&quot;) Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: results &lt;- predict(final_xgboost_reg_model, ames_test) %&gt;% dplyr::bind_cols(truth = ames_test$Sale_Price) %&gt;% dplyr::rename(pred_xgb_reg = .pred, Sale_Price = truth) head(results) ## # A tibble: 6 × 2 ## pred_xgb_reg Sale_Price ## &lt;dbl&gt; &lt;int&gt; ## 1 131668. 105000 ## 2 176134. 185000 ## 3 176002. 180400 ## 4 119015. 141000 ## 5 210505. 210000 ## 6 218120. 216000 multi_metric &lt;- metric_set(rmse, rsq, mae, mape, ccc) multi_metric(results, truth = Sale_Price, estimate = pred_xgb_reg) %&gt;% mutate(.estimate = round(.estimate, 2)) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 30205. ## 2 rsq standard 0.86 ## 3 mae standard 19721. ## 4 mape standard 12.0 ## 5 ccc standard 0.92 results %&gt;% ggplot(aes(x = pred_xgb_reg, y = Sale_Price)) + geom_point() + geom_abline(color = &quot;red&quot;) + xlab(&quot;Prediction&quot;) + ylab(&quot;Observation&quot;) + ggtitle(&quot;Comparisson&quot;) 4.4.5 XGBoost para clasificación Paso 1: Separación inicial de datos (test, train) set.seed(1234) telco_split &lt;- initial_split(telco, prop = .70) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) telco_folds &lt;- vfold_cv(telco_train) telco_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [4437/493]&gt; Fold01 ## 2 &lt;split [4437/493]&gt; Fold02 ## 3 &lt;split [4437/493]&gt; Fold03 ## 4 &lt;split [4437/493]&gt; Fold04 ## 5 &lt;split [4437/493]&gt; Fold05 ## 6 &lt;split [4437/493]&gt; Fold06 ## 7 &lt;split [4437/493]&gt; Fold07 ## 8 &lt;split [4437/493]&gt; Fold08 ## 9 &lt;split [4437/493]&gt; Fold09 ## 10 &lt;split [4437/493]&gt; Fold10 Paso 2: Pre-procesamiento e ingeniería de variables binner &lt;- function(x) { x &lt;- cut(x, breaks = c(0, 12, 24, 36,48,60,72), include.lowest = TRUE) as.numeric(x) } telco_rec &lt;- recipe(Churn ~ ., data = telco_train) %&gt;% update_role(customerID, new_role = &quot;id variable&quot;) %&gt;% step_num2factor( tenure, transform = binner, levels = c(&quot;0-1 year&quot;, &quot;1-2 years&quot;, &quot;2-3 years&quot;, &quot;3-4 years&quot;, &quot;4-5 years&quot;, &quot;5-6 years&quot;)) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_rm(customerID, skip=T) %&gt;% prep() telco_rec ## Recipe ## ## Inputs: ## ## role #variables ## id variable 1 ## outcome 1 ## predictor 19 ## ## Training data contained 4930 data points and 10 incomplete rows. ## ## Operations: ## ## Factor variables from tenure [trained] ## Centering and scaling for SeniorCitizen, MonthlyCharges, TotalCharges [trained] ## Dummy variables from gender, Partner, Dependents, tenure, PhoneService, Multip... [trained] ## Median imputation for SeniorCitizen, MonthlyCharges, TotalCharges, ge... [trained] ## Variables removed customerID [trained] Paso 3: Selección de tipo de modelo con hiperparámetros iniciales xgboost_model &lt;- boost_tree( mode = &quot;classification&quot;, trees = 1000, tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), learn_rate = tune() ) %&gt;% set_engine( &quot;xgboost&quot;, importance = &quot;impurity&quot; ) xgboost_model ## Boosted Tree Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = 1000 ## min_n = tune() ## tree_depth = tune() ## learn_rate = tune() ## loss_reduction = tune() ## sample_size = tune() ## ## Engine-Specific Arguments: ## importance = impurity ## ## Computational engine: xgboost Paso 4: Inicialización de workflow o pipeline xgboost_workflow &lt;- workflow() %&gt;% add_model(xgboost_model) %&gt;% add_recipe(telco_rec) xgboost_workflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: boost_tree() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 5 Recipe Steps ## ## • step_num2factor() ## • step_normalize() ## • step_dummy() ## • step_impute_median() ## • step_rm() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Boosted Tree Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = 1000 ## min_n = tune() ## tree_depth = tune() ## learn_rate = tune() ## loss_reduction = tune() ## sample_size = tune() ## ## Engine-Specific Arguments: ## importance = impurity ## ## Computational engine: xgboost Paso 5: Creación de grid search xgboost_param_grid &lt;- grid_latin_hypercube( tree_depth(range = c(2, 30)), min_n(range = c(2,50)), loss_reduction(range = c(-10, 1.5), trans = log10_trans()), learn_rate(range = c(-6, -0.25), trans = log10_trans()), mtry(range = c(1, 20)), sample_size = sample_prop(), size = 500 ) xgboost_param_grid ## # A tibble: 500 × 6 ## tree_depth min_n loss_reduction learn_rate mtry sample_size ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 4 6 0.00237 0.0000277 19 0.637 ## 2 28 17 0.0189 0.0612 15 0.557 ## 3 13 18 0.000467 0.0436 5 0.977 ## 4 5 22 0.272 0.388 14 0.192 ## 5 10 45 0.000143 0.0383 18 0.480 ## 6 24 37 0.00000000778 0.00000364 1 0.233 ## 7 11 23 0.0000000629 0.00000795 15 0.531 ## 8 9 30 2.07 0.000773 17 0.896 ## 9 11 11 0.000000160 0.227 16 0.160 ## 10 5 35 0.00000000477 0.0000427 13 0.209 ## # … with 490 more rows Paso 6: Entrenamiento de modelos con hiperparámetros definidos library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) xgbt1 &lt;- Sys.time() xgboost_tune_result &lt;- tune_grid( xgboost_workflow, resamples = telco_folds, grid = xgboost_param_grid, metrics = metric_set(roc_auc, pr_auc) ) xgb2 &lt;- Sys.time(); xgb2 - xgbt1 stopCluster(cluster) xgboost_tune_result %&gt;% saveRDS(&quot;models/xgboost_model_classification.rds&quot;) Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) xgboost_tune_result &lt;- readRDS(&quot;models/xgboost_model_classification.rds&quot;) collect_metrics(xgboost_tune_result) ## # A tibble: 1,000 × 12 ## mtry min_n tree_depth learn_rate loss_reduction sample_size .metric ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 17 25 15 0.00000885 5.95e-10 0.461 pr_auc ## 2 17 25 15 0.00000885 5.95e-10 0.461 roc_auc ## 3 9 16 4 0.0757 4.82e- 4 0.349 pr_auc ## 4 9 16 4 0.0757 4.82e- 4 0.349 roc_auc ## 5 2 8 18 0.0000319 9.32e- 8 0.293 pr_auc ## 6 2 8 18 0.0000319 9.32e- 8 0.293 roc_auc ## 7 4 24 25 0.00592 3.02e- 7 0.926 pr_auc ## 8 4 24 25 0.00592 3.02e- 7 0.926 roc_auc ## 9 4 48 28 0.352 6.40e- 5 0.366 pr_auc ## 10 4 48 28 0.352 6.40e- 5 0.366 roc_auc ## # … with 990 more rows, and 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, ## # n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt; En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos: autoplot(xgboost_tune_result) show_best(xgboost_tune_result, n = 10, metric = &quot;roc_auc&quot;) ## # A tibble: 10 × 12 ## mtry min_n tree_depth learn_rate loss_reduction sample_size .metric ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4 18 5 0.00252 0.0000419 0.764 roc_auc ## 2 13 8 16 0.00318 1.92 0.214 roc_auc ## 3 18 15 24 0.00281 0.101 0.459 roc_auc ## 4 17 11 5 0.00508 0.0000000452 0.377 roc_auc ## 5 20 18 7 0.00190 0.00984 0.700 roc_auc ## 6 3 11 21 0.00106 0.0000103 0.423 roc_auc ## 7 7 10 12 0.00291 0.00000000108 0.217 roc_auc ## 8 5 18 6 0.00198 0.000466 0.881 roc_auc ## 9 9 37 14 0.00456 0.000131 0.605 roc_auc ## 10 2 3 7 0.00131 0.00000000122 0.326 roc_auc ## # … with 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, ## # std_err &lt;dbl&gt;, .config &lt;chr&gt; show_best(xgboost_tune_result, n = 10, metric = &quot;pr_auc&quot;) ## # A tibble: 10 × 12 ## mtry min_n tree_depth learn_rate loss_reduction sample_size .metric ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4 18 5 0.00252 4.19e- 5 0.764 pr_auc ## 2 3 11 21 0.00106 1.03e- 5 0.423 pr_auc ## 3 5 18 6 0.00198 4.66e- 4 0.881 pr_auc ## 4 17 11 5 0.00508 4.52e- 8 0.377 pr_auc ## 5 18 15 24 0.00281 1.01e- 1 0.459 pr_auc ## 6 16 4 28 0.00215 9.72e- 1 0.208 pr_auc ## 7 2 3 7 0.00131 1.22e- 9 0.326 pr_auc ## 8 7 10 12 0.00291 1.08e- 9 0.217 pr_auc ## 9 6 31 25 0.00384 3.61e-10 0.807 pr_auc ## 10 18 23 27 0.00334 4.55e- 2 0.801 pr_auc ## # … with 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, ## # std_err &lt;dbl&gt;, .config &lt;chr&gt; Paso 8: Selección de modelo a usar best_xgboost_model &lt;- select_best(xgboost_tune_result, metric = &quot;pr_auc&quot;) best_xgboost_model ## # A tibble: 1 × 7 ## mtry min_n tree_depth learn_rate loss_reduction sample_size .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4 18 5 0.00252 0.0000419 0.764 Preprocessor1_Mo… best_xgboost_model_1se &lt;- xgboost_tune_result %&gt;% select_by_one_std_err(metric = &quot;pr_auc&quot;, &quot;pr_auc&quot;) best_xgboost_model_1se ## # A tibble: 1 × 14 ## mtry min_n tree_depth learn_rate loss_reduction sample_size .metric ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2 8 18 0.0000319 0.0000000932 0.293 pr_auc ## # … with 7 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, ## # std_err &lt;dbl&gt;, .config &lt;chr&gt;, .best &lt;dbl&gt;, .bound &lt;dbl&gt; Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) final_xgboost_model &lt;- xgboost_workflow %&gt;% #finalize_workflow(best_xgboost_model) %&gt;% finalize_workflow(best_xgboost_model_1se) %&gt;% fit(data = telco_test) ## [17:33:23] WARNING: amalgamation/../src/learner.cc:627: ## Parameters: { &quot;importance&quot; } might not be used. ## ## This could be a false alarm, with some parameters getting used by language bindings but ## then being mistakenly passed down to XGBoost core, or some parameter actually being used ## but getting flagged wrongly here. Please open an issue if you find any such cases. final_xgboost_model ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: boost_tree() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 5 Recipe Steps ## ## • step_num2factor() ## • step_normalize() ## • step_dummy() ## • step_impute_median() ## • step_rm() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## ##### xgb.Booster ## raw: 1.2 Mb ## call: ## xgboost::xgb.train(params = list(eta = 3.18565038552185e-05, ## max_depth = 18L, gamma = 9.31534488985793e-08, colsample_bytree = 1, ## colsample_bynode = 0.666666666666667, min_child_weight = 8L, ## subsample = 0.293253452659119, objective = &quot;binary:logistic&quot;), ## data = x$data, nrounds = 1000, watchlist = x$watchlist, verbose = 0, ## importance = &quot;impurity&quot;, nthread = 1) ## params (as set within xgb.train): ## eta = &quot;3.18565038552185e-05&quot;, max_depth = &quot;18&quot;, gamma = &quot;9.31534488985793e-08&quot;, colsample_bytree = &quot;1&quot;, colsample_bynode = &quot;0.666666666666667&quot;, min_child_weight = &quot;8&quot;, subsample = &quot;0.293253452659119&quot;, objective = &quot;binary:logistic&quot;, importance = &quot;impurity&quot;, nthread = &quot;1&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.evaluation.log() ## # of features: 3 ## niter: 1000 ## nfeatures : 3 ## evaluation_log: ## iter training_logloss ## 1 0.6931346 ## 2 0.6931242 ## --- ## 999 0.6811728 ## 1000 0.6811615 Como hemos hablado anteriormente, este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo. library(vip) final_xgboost_model %&gt;% pull_workflow_fit() %&gt;% vip::vip() + ggtitle(&quot;Importancia de las variables&quot;) ## Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3. ## Please use `extract_fit_parsnip()` instead. Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: class_results &lt;- predict(final_xgboost_model, telco_test, type = &quot;prob&quot;) %&gt;% bind_cols(Churn = telco_test$Churn) %&gt;% mutate(Churn = factor(Churn, levels = c(&#39;No&#39;, &#39;Yes&#39;), labels = c(&#39;No&#39;, &#39;Yes&#39;))) head(class_results) ## # A tibble: 6 × 3 ## .pred_No .pred_Yes Churn ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.512 0.488 No ## 2 0.495 0.505 Yes ## 3 0.508 0.492 No ## 4 0.512 0.488 No ## 5 0.510 0.490 No ## 6 0.502 0.498 No roc_auc(class_results, truth = Churn, estimate = .pred_Yes, event_level = &quot;second&quot;) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.833 pr_auc(class_results, truth = Churn, estimate = .pred_Yes, event_level = &quot;second&quot;) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 pr_auc binary 0.650 A continuación, conoceremos el nivel de sensitividad y especificidad para cada punto de corte: roc_curve_data &lt;- roc_curve( class_results, truth = Churn, estimate = .pred_Yes, event_level = &#39;second&#39; ) roc_curve_data ## # A tibble: 1,864 × 3 ## .threshold specificity sensitivity ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -Inf 0 1 ## 2 0.487 0 1 ## 3 0.487 0.000646 1 ## 4 0.487 0.00323 1 ## 5 0.487 0.00388 1 ## 6 0.487 0.00452 1 ## 7 0.487 0.00646 1 ## 8 0.487 0.00776 1 ## 9 0.487 0.00905 1 ## 10 0.487 0.0103 1 ## # … with 1,854 more rows A través de estas métricas es posible crear la curva ROC: roc_curve_plot &lt;- roc_curve_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;)+ theme_minimal() roc_curve_plot De igual manera, podemos calcular la precisión y cobertura para cada punte de corte: pr_curve_data &lt;- pr_curve( class_results, truth = Churn, estimate = .pred_Yes, event_level = &#39;second&#39; ) pr_curve_data ## # A tibble: 1,863 × 3 ## .threshold recall precision ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Inf 0 1 ## 2 0.507 0.00177 1 ## 3 0.507 0.00353 1 ## 4 0.507 0.00707 1 ## 5 0.507 0.0106 1 ## 6 0.507 0.0124 1 ## 7 0.507 0.0124 0.875 ## 8 0.507 0.0141 0.889 ## 9 0.507 0.0159 0.9 ## 10 0.507 0.0177 0.909 ## # … with 1,853 more rows Y graficar su respectiva curva: pr_curve_plot &lt;- pr_curve_data %&gt;% ggplot(aes(x = recall, y = precision)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;)+ theme_minimal() pr_curve_plot 4.5 Ejercicios Ejecutar un modelo propio usando Adaboost. Cada alumno deberá proponer su propia configuración y comparar resultados con XGBoost: adaboost_model &lt;- boost_tree( mode = &quot;classification&quot;, trees = 1000, tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), learn_rate = tune() ) %&gt;% set_engine(&quot;C5.0&quot;, importance = &quot;impurity&quot;) "],["workflowsets-stacking.html", "Capítulo 5 Workflowsets &amp; Stacking 5.1 Múltiples recetas 5.2 Múltiples modelos 5.3 Creación de workflowset 5.4 Ajuste y evaluación de modelos 5.5 Extracción de modelos 5.6 Métodos de carrera 5.7 Stacking 5.8 Ejercicios", " Capítulo 5 Workflowsets &amp; Stacking Es común no sepamos ni remotamente cuál es el mejor modelo que podríamos implementar al iniciar un proyecto con datos que nunca antes hemos visto. Es posible que un profesional de datos deba seleccionar muchas combinaciones de modelos y preprocesadores. También es posible tener poco o ningún conocimiento a priori sobre qué método funcionará mejor con un nuevo conjunto de datos. Una buena estrategia es dedicar un esfuerzo inicial a probar una variedad de enfoques de modelado, determinar qué funciona mejor y luego invertir tiempo adicional ajustando / optimizando un pequeño conjunto de modelos. 5.1 Múltiples recetas Algunos modelos requieren predictores que se han centrado y escalado, por lo que algunos flujos de trabajo de modelos requerirán recetas con estos pasos de preprocesamiento. Para otros modelos, crear interacciones cuadráticas y bidireccionales. Para estos fines, creamos múltiples recetas: library(tidymodels) data(ames) set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) ames_folds &lt;- vfold_cv(ames_train) Receta Original receta_original &lt;- recipe(Sale_Price ~ . , data = ames_train) %&gt;% step_unknown(Alley) %&gt;% step_rename(Year_Remod = Year_Remod_Add) %&gt;% step_rename(ThirdSsn_Porch = Three_season_porch) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod, TotalSF = Gr_Liv_Area + Total_Bsmt_SF, AvgRoomSF = Gr_Liv_Area / TotRms_AbvGrd, Pool = if_else(Pool_Area &gt; 0, 1, 0), Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_predictors(), -all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~ Second_Flr_SF:First_Flr_SF) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% step_rm( First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath, Bsmt_Half_Bath, Kitchen_AbvGr, BsmtFin_Type_1_Unf, Total_Bsmt_SF, Kitchen_AbvGr, Pool_Area, Gr_Liv_Area, Sale_Type_Oth, Sale_Type_VWD ) receta_original_prep &lt;- receta_original %&gt;% prep() Receta KNN: receta_knn &lt;- recipe(Sale_Price ~ . , data = ames_train) %&gt;% step_unknown(Alley) %&gt;% step_unknown(Pool_QC) %&gt;% step_unknown(Misc_Feature) %&gt;% step_unknown(Fence) %&gt;% step_unknown(Garage_Finish) %&gt;% step_unknown(Garage_Cond) %&gt;% step_unknown(Garage_Type) %&gt;% step_unknown(Bsmt_Exposure) %&gt;% step_unknown(Bsmt_Cond) %&gt;% step_unknown(BsmtFin_Type_1) %&gt;% step_unknown(BsmtFin_Type_2) %&gt;% step_unknown(Mas_Vnr_Type) %&gt;% step_unknown(Electrical) %&gt;% step_impute_knn( Mas_Vnr_Area, Lot_Frontage, impute_with= all_predictors(), neighbors = 5) %&gt;% step_rename(Year_Remod = Year_Remod_Add) %&gt;% step_rename(ThirdSsn_Porch = Three_season_porch) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod, TotalSF = Gr_Liv_Area + Total_Bsmt_SF, AvgRoomSF = Gr_Liv_Area / TotRms_AbvGrd, Pool = if_else(Pool_Area &gt; 0, 1, 0), Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_predictors(), -all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~ Second_Flr_SF:First_Flr_SF) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) receta_knn_prep &lt;- receta_knn %&gt;% prep() Receta Grande: receta_grande &lt;- recipe(Sale_Price ~ . , data = ames_train) %&gt;% step_unknown(Alley) %&gt;% step_unknown(Fence) %&gt;% step_unknown(Garage_Type) %&gt;% step_unknown(Garage_Finish) %&gt;% step_unknown(Garage_Cond) %&gt;% step_unknown(Bsmt_Cond) %&gt;% step_unknown(Bsmt_Exposure) %&gt;% step_unknown(BsmtFin_Type_1) %&gt;% step_unknown(BsmtFin_Type_2) %&gt;% step_unknown(Mas_Vnr_Type) %&gt;% step_unknown(Electrical) %&gt;% step_unknown(Heating_QC) %&gt;% step_unknown(Pool_QC) %&gt;% step_impute_knn( Mas_Vnr_Area, Lot_Frontage, impute_with= all_predictors(), neighbors = 5) %&gt;% step_rename(Year_Remod = Year_Remod_Add) %&gt;% step_rename(ThirdSsn_Porch = Three_season_porch) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_ratio(Second_Flr_SF, denom = denom_vars(First_Flr_SF)) %&gt;% step_mutate( TotalBaths = Full_Bath + Bsmt_Full_Bath + 0.5 * (Half_Bath + Bsmt_Half_Bath), Age_House = Year_Sold - Year_Remod, TotalSF = Gr_Liv_Area + Total_Bsmt_SF, Porc_H_over_TotalSF = (TotalSF / Lot_Area) * 100, AvgRoomSF = Gr_Liv_Area / TotRms_AbvGrd, Porch_SF = Enclosed_Porch + ThirdSsn_Porch + Open_Porch_SF, Porch = factor(Porch_SF &gt; 0), Pool = if_else(Pool_Area &gt; 0,1,0), Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;)), Condition_1 = forcats::fct_collapse( Condition_1, Artery_Feedr = c(&quot;Feedr&quot;, &quot;Artery&quot;), Railr = c(&quot;RRAn&quot;, &quot;RRNn&quot;, &quot;RRNe&quot;, &quot;RRAe&quot;), Norm = &quot;Norm&quot;, Pos = c(&quot;PosN&quot;, &quot;PosA&quot;)), Land_Slope = forcats::fct_collapse(Land_Slope, Mod_Sev = c(&quot;Mod&quot;, &quot;Sev&quot;)), Land_Contour = forcats::fct_collapse(Land_Contour, Low_HLS = c(&quot;Low&quot;,&quot;HLS&quot;), Bnk_Lvl = c(&quot;Lvl&quot;,&quot;Bnk&quot;)), Lot_Shape = forcats::fct_collapse(Lot_Shape, IRREG = c(&quot;Slightly_Irregular&quot;, &quot;Moderately_Irregular&quot;, &quot;Irregular&quot;)), Bsmt_Cond = forcats::fct_collapse(Bsmt_Cond, Exc = c(&quot;Good&quot;, &quot;Excellent&quot;)), BsmtFin_Type_1 = forcats::fct_collapse(BsmtFin_Type_1, Rec_BLQ = c(&quot;Rec&quot;, &quot;BLQ&quot;)), BsmtFin_Type_2 = forcats::fct_collapse(BsmtFin_Type_2, Rec_BLQ = c(&quot;Rec&quot;, &quot;BLQ&quot;,&quot;LwQ&quot;)), Neighborhood = forcats::fct_collapse( Neighborhood, NoRidge_GrnHill = c(&quot;Northridge&quot;, &quot;Green_Hills&quot;), Crawfor_Greens = c(&quot;Crawford&quot;, &quot;Greens&quot;), Blueste_Mitchel = c(&quot;Blueste&quot;, &quot;Mitchell&quot;), Blmngtn_CollgCr = c(&quot;Bloomington_Heights&quot;, &quot;College_Creek&quot;), NPkVill_NAmes = c(&quot;Northpark_Villa&quot;, &quot;North_Ames&quot;), Veenker_StoneBr = c(&quot;Veenker&quot;, &quot;Stone_Brook&quot;), BrDale_IDOTRR = c(&quot;Briardale&quot;, &quot;Iowa_DOT_and_Rail_Road&quot;), SWISU_Sawyer = c(&quot;South_and_West_of_Iowa_State_University&quot;, &quot;Sawyer&quot;), ClearCr_Somerst = c(&quot;Clear_Creek&quot;, &quot;Somerset&quot;)), Heating = forcats::fct_collapse( Heating, Grav_Wall = c(&quot;Grav&quot;, &quot;Wall&quot;), GasA_W = c(&quot;GasA&quot;, &quot;GasW&quot;, &quot;OthW&quot;)), MS_Zoning = forcats::fct_collapse( MS_Zoning, I_R_M_H = c(&quot;Residential_Medium_Density&quot;, &quot;I_all&quot;, &quot;Residential_High_Density&quot; )), Bldg_Type = forcats::fct_collapse(Bldg_Type, Du_Tu = c(&quot;Duplex&quot;, &quot;Twnhs&quot;)), Foundation = forcats::fct_collapse(Foundation, Wood_Stone = c(&quot;Wood&quot;, &quot;Stone&quot;)), Functional = forcats::fct_collapse( Functional, Min = c(&quot;Min1&quot;, &quot;Min2&quot;), Maj = c(&quot;Maj1&quot;, &quot;Maj2&quot;, &quot;Mod&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_relevel(Condition_1, ref_level = &quot;Norm&quot;) %&gt;% step_normalize(all_predictors(), -all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~ Second_Flr_SF:Bedroom_AbvGr) %&gt;% step_interact(~ TotalSF:TotRms_AbvGrd) %&gt;% step_interact(~ Age_House:TotRms_AbvGrd) %&gt;% step_interact(~ Second_Flr_SF:First_Flr_SF) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% step_interact(~ matches(&quot;BsmtFin_Type_1&quot;):BsmtFin_SF_1) %&gt;% step_interact(~ matches(&quot;BsmtFin_Type_1&quot;):Total_Bsmt_SF) %&gt;% step_interact(~ matches(&quot;Heating_QC&quot;):TotRms_AbvGrd) %&gt;% step_interact(~ matches(&quot;Heating_QC&quot;):TotalSF) %&gt;% step_interact(~ matches(&quot;Heating_QC&quot;):Second_Flr_SF) %&gt;% step_interact(~ matches(&quot;Neighborhood&quot;):matches(&quot;Condition_1&quot;)) %&gt;% step_rm( First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath, Bsmt_Half_Bath, Kitchen_AbvGr, BsmtFin_Type_1_Unf, Total_Bsmt_SF, Kitchen_AbvGr, Pool_Area, Gr_Liv_Area, Porch_SF, Sale_Type_Oth, Sale_Type_VWD ) receta_grande_prep &lt;- receta_grande %&gt;% prep() 5.2 Múltiples modelos Una vez que tenemos suficientes recetas, podemos experimentar con múltiples modelos para poner a prueba. Usaremos los modelos que hemos aprendido a implementar en todo el curso: library(rules) library(baguette) library(tune) elasticnet_model &lt;- linear_reg( mode = &quot;regression&quot;, penalty = tune(), mixture = tune()) %&gt;% set_engine(&quot;glmnet&quot;) knn_model &lt;- nearest_neighbor( mode = &quot;regression&quot;, neighbors = tune(&quot;K&quot;), dist_power = tune(), weight_func = tune()) %&gt;% set_engine(&quot;kknn&quot;) rforest_model &lt;- rand_forest( mode = &quot;regression&quot;, trees = 1000, mtry = tune(), min_n = tune()) %&gt;% set_engine( &quot;ranger&quot;, importance = &quot;impurity&quot; ) svm_rbf_model &lt;- svm_rbf( cost = tune(), rbf_sigma = tune(), margin = tune()) %&gt;% set_engine(&quot;kernlab&quot;) %&gt;% set_mode(&quot;regression&quot;) xgboost_model &lt;- boost_tree( mode = &quot;regression&quot;, trees = 1000, tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), learn_rate = tune()) %&gt;% set_engine( &quot;xgboost&quot;, importance = &quot;impurity&quot; ) ¿Cómo podemos hacer coincidir estos modelos con las recetas desarrolladas, ajustarlos y luego evaluar su rendimiento de manera eficiente? WORKFLOWSETS ofrece una solución. 5.3 Creación de workflowset Los conjuntos de flujo de trabajo toman listas nombradas de preprocesadores y especificaciones de modelos y las combinan en un objeto que contiene múltiples flujos de trabajo. Como primer ejemplo de conjunto de flujo de trabajo, combinemos las recetas creadas en la sección anterior. workflow_set_models &lt;- workflow_set( preproc = list( receta_original = receta_original_prep, receta_knn = receta_knn_prep, receta_grande = receta_grande_prep ), models = list( elasticnet = elasticnet_model, knn = knn_model, rf = rforest_model, svm_rbf = svm_rbf_model, boost = xgboost_model ) ) 5.4 Ajuste y evaluación de modelos Casi todos estos flujos de trabajo contienen parámetros de ajuste. Para evaluar su rendimiento, podemos utilizar las funciones estándar de ajuste o remuestreo (por ejemplo, tune_grid()). La función workflow_map() aplicará la misma función a todos los flujos de trabajo del conjunto; el valor predeterminado es tune_grid(). A continuación se declaran los parámetros para cada modelo y el grid: elasticnet_params &lt;- elasticnet_model %&gt;% parameters() %&gt;% update( penalty = penalty( range = c(-2, 3), trans = log10_trans()), mixture = dials::mixture(range = c(0, 1)) ) knn_params &lt;- knn_model %&gt;% parameters() %&gt;% update( K = dials::neighbors(c(5, 80)), dist_power = dist_power(range = c(1, 3)), weight_func = weight_func(values = c(&quot;rectangular&quot;, &quot;inv&quot;, &quot;gaussian&quot;, &quot;cos&quot;)) ) rforest_params &lt;- rforest_model %&gt;% parameters() %&gt;% update( mtry = finalize(mtry(range = c(15, 80))), min_n = min_n(range = c(3,15)) ) svm_rbf_params &lt;- svm_rbf_model %&gt;% parameters() %&gt;% update( cost = cost(c(0, 0.5)), rbf_sigma = rbf_sigma(c(-3, 3)), margin = svm_margin(c(-2, 2)) ) xgboost_params &lt;- xgboost_model %&gt;% parameters() %&gt;% update( min_n = min_n(range = c(5,15)), mtry = finalize(mtry(range = c(5, 80))), tree_depth = tree_depth(range = c(3, 50)), loss_reduction = loss_reduction(range = c(-10, 1.5), trans = log10_trans()), learn_rate = learn_rate(range = c(-6, -0.25), trans = log10_trans()), sample_size = sample_prop() ) # Declaración del grid workflow_tunning_set_models &lt;- workflow_set_models %&gt;% option_add(param_info = elasticnet_params, id = &quot;elasticnet&quot;) %&gt;% option_add(param_info = knn_params, id = &quot;knn&quot;) %&gt;% option_add(param_info = rforest_params, id = &quot;rf&quot;) %&gt;% option_add(param_info = svm_rbf_params, id = &quot;svm_rbf&quot;) %&gt;% option_add(param_info = xgboost_params, id = &quot;xgboost&quot;) workflow_tunning_set_models ## # A workflow set/tibble: 15 × 4 ## wflow_id info option result ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 receta_original_elasticnet &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 2 receta_original_knn &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 3 receta_original_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 4 receta_original_svm_rbf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 5 receta_original_boost &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 6 receta_knn_elasticnet &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 7 receta_knn_knn &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 8 receta_knn_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 9 receta_knn_svm_rbf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 10 receta_knn_boost &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 11 receta_grande_elasticnet &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 12 receta_grande_knn &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 13 receta_grande_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 14 receta_grande_svm_rbf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 15 receta_grande_boost &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; Dado que el preprocesador contiene más de una entrada, la función crea todas las combinaciones de preprocesadores y modelos. info: Contiene un tibble con algunos identificadores y el objeto de flujo de trabajo. option: Es un marcador de posición para cualquier argumento que se utilice cuando evaluamos el flujo de trabajo. result: Es un marcador de posición para la salida de las funciones de ajuste o remuestreo. Para este ejemplo, la búsqueda del grid se aplica al flujo de trabajo. library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) grid_ctrl &lt;- control_grid( save_pred = TRUE, save_workflow = TRUE, parallel_over = &quot;everything&quot; ) set.seed(536) tunning_models_result &lt;- workflow_tunning_set_models %&gt;% workflow_map( fn = &quot;tune_grid&quot;, seed = 20220603, resamples = ames_folds, grid = 100, metrics = metric_set(rmse, mae, mape, rsq), control = grid_ctrl, verbose = TRUE ) stopCluster(cluster) tunning_models_result %&gt;% saveRDS(&quot;models/ensemble_model.rds&quot;) tunning_models_result &lt;- readRDS(&quot;models/ensemble_model.rds&quot;) tunning_models_result %&gt;% rank_results(select_best = T) %&gt;% select(-c(.config, n, preprocessor, std_err)) %&gt;% pivot_wider(names_from = .metric, values_from = mean) ## # A tibble: 15 × 7 ## wflow_id model rank mae mape rmse rsq ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 receta_knn_boost boost_tree 1 16614. 10.4 25425. 0.900 ## 2 receta_knn_rf rand_forest 2 17766. 11.1 28489. 0.877 ## 3 receta_knn_svm_rbf svm_rbf 3 17985. 11.3 29103. 0.865 ## 4 receta_knn_knn nearest_neighbor 4 19471. 11.6 30465. 0.861 ## 5 receta_original_boost boost_tree 5 20019. 12.2 30569. 0.856 ## 6 receta_grande_boost boost_tree 6 20019. 12.2 30569. 0.856 ## 7 receta_original_rf rand_forest 7 21058. 12.9 33599. 0.828 ## 8 receta_grande_rf rand_forest 8 21058. 12.9 33599. 0.828 ## 9 receta_grande_knn nearest_neighbor 9 22561. 13.0 35353. 0.817 ## 10 receta_original_knn nearest_neighbor 10 22561. 13.0 35353. 0.817 ## 11 receta_grande_svm_rbf svm_rbf 11 22839. 13.8 35431. 0.807 ## 12 receta_original_svm_rbf svm_rbf 12 22839. 13.8 35431. 0.807 ## 13 receta_knn_elasticnet linear_reg 13 24196. 14.7 39880. 0.756 ## 14 receta_grande_elasticnet linear_reg 14 30129. 18.4 44124. 0.694 ## 15 receta_original_elasticnet linear_reg 15 30129. 18.4 44124. 0.694 autoplot( tunning_models_result, rank_metric = &quot;rsq&quot;, metric = &quot;rsq&quot;, select_best = F) + lims(y = c(0, 1)) + ggtitle(&quot;Model Comparisson&quot;) autoplot( tunning_models_result, rank_metric = &quot;rsq&quot;, metric = &quot;rsq&quot;, select_best = T) + geom_text(aes(y = mean - 0.10 , label = wflow_id), angle = 90, hjust = 1) + lims(y = c(0, 1)) + ggtitle(&quot;Model Comparisson&quot;) 5.5 Extracción de modelos Una vez que hemos realizado una exploración sobre el desempeño de todas las combinaciones de preprocesamientos con modelos, es posible tomar varios caminos hacia adelante. Algunas de las opciones más comunes son: Realizar múltiples iteraciones de modificaciones a las recetas para extraer lo mejor de cada una. Realizar mejoras a los hiperparámetros. Eliminar modelos y/o recetas que no tuvieron buen desempeño. Crear un modelo a partir de la combinación de los modelos más competentes. Para empezar, se realiza una exploración del resultado del desempeño de los mejores modelos y los hiperparámetros usado en cada caso. Resultados de XGBoost tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_knn_boost&quot;) %&gt;% autoplot(metric = &quot;rsq&quot;) tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_knn_boost&quot;) %&gt;% show_best(n = 10, metric = &quot;rsq&quot;) %&gt;% select(-c(.estimator, .metric, .config, n)) ## # A tibble: 10 × 8 ## mtry min_n tree_depth learn_rate loss_reduction sample_size mean std_err ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 4 8 0.0318 0.0000000180 0.601 0.900 0.00487 ## 2 8 7 6 0.0661 0.0692 0.723 0.897 0.00537 ## 3 4 7 7 0.0871 5.50 0.892 0.896 0.00493 ## 4 25 4 11 0.0381 7.61 0.425 0.894 0.00596 ## 5 5 11 5 0.0263 0.0198 0.444 0.893 0.00581 ## 6 21 5 13 0.0120 0.00108 0.415 0.891 0.00724 ## 7 12 7 15 0.00611 3.24 0.656 0.891 0.00730 ## 8 29 3 9 0.0375 0.00000107 0.820 0.891 0.00419 ## 9 12 13 14 0.0158 25.3 0.778 0.891 0.00825 ## 10 15 19 7 0.0161 17.5 0.692 0.891 0.00767 Resultados de Ranfom Forest tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_knn_rf&quot;) %&gt;% autoplot(metric = &quot;rmse&quot;) tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_knn_rf&quot;) %&gt;% show_best(n = 10, metric = &quot;rmse&quot;) %&gt;% select(-c(.estimator, .metric, .config, n)) ## # A tibble: 10 × 4 ## mtry min_n mean std_err ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 7 28489. 1010. ## 2 13 5 28491. 883. ## 3 10 4 28524. 970. ## 4 18 5 28604. 882. ## 5 12 8 28613. 947. ## 6 11 9 28620. 939. ## 7 13 6 28628. 928. ## 8 5 7 28689. 1052. ## 9 16 6 28699. 920. ## 10 16 8 28737. 912. 5.5.1 Selección de modelo Habiendo determinado los hiperparámetros adecuados para la configuración del modelo, procedemos a seleccionar la configuración adecuada para nosotros. Estos pasos son los mismos que corresponden a la selección del modelo con un único workflow. Mejor modelo XGBoost best_xgb_model &lt;- tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_knn_boost&quot;) %&gt;% select_best(metric = &quot;rsq&quot;, &quot;rsq&quot;) best_xgb_model %&gt;% mutate_all(as.character) %&gt;% pivot_longer(everything(), names_to = &quot;metric&quot;, values_to = &quot;value&quot;) ## # A tibble: 7 × 2 ## metric value ## &lt;chr&gt; &lt;chr&gt; ## 1 mtry 2 ## 2 min_n 4 ## 3 tree_depth 8 ## 4 learn_rate 0.0318069757974226 ## 5 loss_reduction 1.8022375282862e-08 ## 6 sample_size 0.60145520768594 ## 7 .config Preprocessor1_Model058 Mejor modelo XGBoost a menos de una desviación estandar best_regularized_xgb_model_1se &lt;- tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_knn_boost&quot;) %&gt;% select_by_one_std_err(metric = &quot;rsq&quot;, &quot;rsq&quot;) best_regularized_xgb_model_1se %&gt;% mutate_all(as.character) %&gt;% pivot_longer(everything(), names_to = &quot;metric&quot;, values_to = &quot;value&quot;) ## # A tibble: 14 × 2 ## metric value ## &lt;chr&gt; &lt;chr&gt; ## 1 mtry 8 ## 2 min_n 7 ## 3 tree_depth 6 ## 4 learn_rate 0.0660688823690034 ## 5 loss_reduction 0.0691664318630444 ## 6 sample_size 0.723484068220947 ## 7 .metric rsq ## 8 .estimator standard ## 9 mean 0.896674936544582 ## 10 n 10 ## 11 std_err 0.00536531804472105 ## 12 .config Preprocessor1_Model035 ## 13 .best 0.900303988301004 ## 14 .bound 0.895438860790247 Mejor modelo XGBoost a menos de un porcentaje fijo best_regularized_xgb_model_pct &lt;- tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_knn_boost&quot;) %&gt;% select_by_pct_loss(metric = &quot;rsq&quot;, &quot;rsq&quot;, limit = 10) best_regularized_xgb_model_pct %&gt;% mutate_all(as.character) %&gt;% pivot_longer(everything(), names_to = &quot;metric&quot;, values_to = &quot;value&quot;) ## # A tibble: 14 × 2 ## metric value ## &lt;chr&gt; &lt;chr&gt; ## 1 mtry 13 ## 2 min_n 36 ## 3 tree_depth 2 ## 4 learn_rate 0.00313559984945049 ## 5 loss_reduction 0.190594567355415 ## 6 sample_size 0.36900916616735 ## 7 .metric rsq ## 8 .estimator standard ## 9 mean 0.824569900806526 ## 10 n 10 ## 11 std_err 0.00788602755362551 ## 12 .config Preprocessor1_Model030 ## 13 .best 0.900303988301004 ## 14 .loss 8.41205731381887 Ajuste del modelo seleccionado final_regularized_xgb_model &lt;- tunning_models_result %&gt;% extract_workflow(&quot;receta_knn_boost&quot;) %&gt;% finalize_workflow(best_regularized_xgb_model_1se) %&gt;% parsnip::fit(data = ames_train) ## [17:33:47] WARNING: amalgamation/../src/learner.cc:627: ## Parameters: { &quot;importance&quot; } might not be used. ## ## This could be a false alarm, with some parameters getting used by language bindings but ## then being mistakenly passed down to XGBoost core, or some parameter actually being used ## but getting flagged wrongly here. Please open an issue if you find any such cases. final_regularized_xgb_model ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: boost_tree() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 23 Recipe Steps ## ## • step_unknown() ## • step_unknown() ## • step_unknown() ## • step_unknown() ## • step_unknown() ## • step_unknown() ## • step_unknown() ## • step_unknown() ## • step_unknown() ## • step_unknown() ## • ... ## • and 13 more steps. ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## ##### xgb.Booster ## raw: 2.6 Mb ## call: ## xgboost::xgb.train(params = list(eta = 0.0660688823690034, max_depth = 6L, ## gamma = 0.0691664318630444, colsample_bytree = 1, colsample_bynode = 0.258064516129032, ## min_child_weight = 7L, subsample = 0.723484068220947, objective = &quot;reg:squarederror&quot;), ## data = x$data, nrounds = 1000, watchlist = x$watchlist, verbose = 0, ## importance = &quot;impurity&quot;, nthread = 1) ## params (as set within xgb.train): ## eta = &quot;0.0660688823690034&quot;, max_depth = &quot;6&quot;, gamma = &quot;0.0691664318630444&quot;, colsample_bytree = &quot;1&quot;, colsample_bynode = &quot;0.258064516129032&quot;, min_child_weight = &quot;7&quot;, subsample = &quot;0.723484068220947&quot;, objective = &quot;reg:squarederror&quot;, importance = &quot;impurity&quot;, nthread = &quot;1&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.evaluation.log() ## # of features: 31 ## niter: 1000 ## nfeatures : 31 ## evaluation_log: ## iter training_rmse ## 1 184947.832 ## 2 173406.092 ## --- ## 999 1720.214 ## 1000 1717.597 Como hemos hablado anteriormente, este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo library(vip) final_regularized_xgb_model %&gt;% extract_fit_parsnip() %&gt;% vip::vip(num_features = 25) + ggtitle(&quot;Importancia de las variables&quot;) Por último… Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: results &lt;- predict(final_regularized_xgb_model, ames_test) %&gt;% dplyr::bind_cols(truth = ames_test$Sale_Price) %&gt;% dplyr::rename(pred_xgb_reg = .pred, Sale_Price = truth) head(results) ## # A tibble: 6 × 2 ## pred_xgb_reg Sale_Price ## &lt;dbl&gt; &lt;int&gt; ## 1 131972. 105000 ## 2 175629. 185000 ## 3 176203. 180400 ## 4 106614. 141000 ## 5 238267. 210000 ## 6 213937. 216000 multi_metric &lt;- metric_set(rmse, rsq, mae, mape, ccc) multi_metric(results, truth = Sale_Price, estimate = pred_xgb_reg) %&gt;% mutate(.estimate = round(.estimate, 2)) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 26851. ## 2 rsq standard 0.89 ## 3 mae standard 17078. ## 4 mape standard 10.5 ## 5 ccc standard 0.94 results %&gt;% ggplot(aes(x = pred_xgb_reg, y = Sale_Price)) + geom_point() + geom_abline(color = &quot;red&quot;) + xlab(&quot;Prediction&quot;) + ylab(&quot;Observation&quot;) + ggtitle(&quot;Comparisson&quot;) 5.6 Métodos de carrera Un problema con la búsqueda del grid es que todos los modelos deben ajustarse en todos los remuestreos antes de que se puedan evaluar los parámetros de ajuste. En machine learning, el conjunto de técnicas que son llamadas métodos de carreras evalúa todos los modelos en un subconjunto inicial de remuestreo. En función de sus métricas de rendimiento actuales, algunos conjuntos de parámetros no se consideran en remuestreos posteriores. Dado un workflow, podemos usar la función workflow_map() para un enfoque de carreras. El paquete finetune contiene funciones para el método de carreras. Calcula un conjunto de métricas de rendimiento (por ejemplo, precisión o RMSE) para un conjunto predefinido de parámetros de ajuste que corresponden a un modelo o receta a través de una o más muestras de los datos. 5.6.1 Optimización ANOVA Realiza un modelo de análisis de varianza (ANOVA) para probar la significación estadística de las diferentes configuraciones del modelo. Después de evaluar un número inicial de remuestreos, el proceso elimina las combinaciones de parámetros de ajuste que probablemente no sean los mejores resultados usando medidas repetidas de un modelo ANOVA. La función implementada es tune_race_anova library(finetune) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) race_ctrl &lt;- control_race( save_pred = TRUE, parallel_over = &quot;everything&quot;, save_workflow = TRUE ) tunning_race_anova_results &lt;- workflow_tunning_set_models %&gt;% workflow_map( &quot;tune_race_anova&quot;, seed = 1503, resamples = ames_folds, grid = 100, metrics = metric_set(rmse, mae, mape, rsq), control = race_ctrl, verbose = TRUE ) stopCluster(cluster) tunning_race_anova_results %&gt;% saveRDS(&quot;models/race_anova_results.rds&quot;) Las mismas funciones útiles están disponibles para que este objeto interrogue los resultados y, de hecho, el método básico autoplot() produce tendencias similares: tunning_race_anova_results %&gt;% rank_results(select_best = T) %&gt;% select(-c(.config, n, preprocessor, std_err)) %&gt;% pivot_wider(names_from = .metric, values_from = mean) ## # A tibble: 15 × 7 ## wflow_id model rank mae mape rmse rsq ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 receta_knn_boost boost_tree 1 16275. 10.3 25609. 0.898 ## 2 receta_knn_rf rand_forest 2 17676. 11.1 28416. 0.877 ## 3 receta_knn_svm_rbf svm_rbf 3 18390. 11.6 29772. 0.859 ## 4 receta_knn_knn nearest_neighbor 4 19383. 11.5 30727. 0.859 ## 5 receta_original_boost boost_tree 5 20342. 12.4 30962. 0.851 ## 6 receta_grande_boost boost_tree 6 20342. 12.4 30962. 0.851 ## 7 receta_original_rf rand_forest 7 21016. 12.9 33521. 0.830 ## 8 receta_grande_rf rand_forest 8 21016. 12.9 33521. 0.830 ## 9 receta_grande_svm_rbf svm_rbf 9 22677. 13.7 34676. 0.812 ## 10 receta_original_svm_rbf svm_rbf 10 22677. 13.7 34676. 0.812 ## 11 receta_grande_knn nearest_neighbor 11 22584. 13.0 35472. 0.815 ## 12 receta_original_knn nearest_neighbor 12 22584. 13.0 35472. 0.815 ## 13 receta_knn_elasticnet linear_reg 13 24197. 14.7 39883. 0.756 ## 14 receta_grande_elasticnet linear_reg 14 30130. 18.4 44123. 0.694 ## 15 receta_original_elasticnet linear_reg 15 30130. 18.4 44123. 0.694 autoplot( tunning_race_anova_results, rank_metric = &quot;rsq&quot;, metric = &quot;rsq&quot;, select_best = TRUE) + geom_text(aes(y = mean - 0.10 , label = wflow_id), angle = 90, hjust = 1) + lims(y = c(0, 1)) 5.6.2 Optimización Logística Después de evaluar un número inicial de remuestreos, el proceso elimina las combinaciones de parámetros de ajuste que probablemente no sean los mejores resultados usando un modelo estadístico. Para cada combinación por pares de parámetros de ajuste, se calculan las estadísticas de ganancia/pérdida y se usa un modelo de regresión logística para medir la probabilidad de que cada combinación gane en general. Esto se logra mediante la función tune_race_win_loss UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) race_ctrl &lt;- control_race( save_pred = TRUE, parallel_over = &quot;everything&quot;, save_workflow = TRUE ) tunning_race_win_loss_results &lt;- workflow_tunning_set_models %&gt;% workflow_map( &quot;tune_race_win_loss&quot;, seed = 1503, resamples = ames_folds, grid = 100, metrics = metric_set(rmse, mae, mape, rsq), control = race_ctrl, verbose = TRUE ) stopCluster(cluster) tunning_race_win_loss_results %&gt;% saveRDS(&quot;models/race_win_loss_results.rds&quot;) Las mismas funciones útiles están disponibles para que este objeto interrogue los resultados y, de hecho, el método básico autoplot() produce tendencias similares: tunning_race_win_loss_results %&gt;% rank_results(select_best = T) %&gt;% select(-c(.config, n, preprocessor, std_err)) %&gt;% pivot_wider(names_from = .metric, values_from = mean) ## # A tibble: 15 × 7 ## wflow_id model rank mae mape rmse rsq ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 receta_knn_boost boost_tree 1 16216. 10.2 25608. 0.898 ## 2 receta_knn_rf rand_forest 2 17637. 11.1 28341. 0.878 ## 3 receta_knn_svm_rbf svm_rbf 3 18390. 11.6 29772. 0.859 ## 4 receta_knn_knn nearest_neighbor 4 19383. 11.5 30727. 0.859 ## 5 receta_original_boost boost_tree 5 20583. 12.5 31149. 0.850 ## 6 receta_grande_boost boost_tree 6 20583. 12.5 31149. 0.850 ## 7 receta_original_rf rand_forest 7 21012. 12.9 33503. 0.830 ## 8 receta_grande_rf rand_forest 8 21012. 12.9 33503. 0.830 ## 9 receta_grande_svm_rbf svm_rbf 9 22677. 13.7 34676. 0.812 ## 10 receta_original_svm_rbf svm_rbf 10 22677. 13.7 34676. 0.812 ## 11 receta_grande_knn nearest_neighbor 11 22584. 13.0 35472. 0.815 ## 12 receta_original_knn nearest_neighbor 12 22584. 13.0 35472. 0.815 ## 13 receta_knn_elasticnet linear_reg 13 24197. 14.7 39883. 0.756 ## 14 receta_grande_elasticnet linear_reg 14 30130. 18.4 44123. 0.694 ## 15 receta_original_elasticnet linear_reg 15 30130. 18.4 44123. 0.694 autoplot( tunning_race_win_loss_results, rank_metric = &quot;rsq&quot;, metric = &quot;rsq&quot;, select_best = TRUE) + geom_text(aes(y = mean - 0.10 , label = wflow_id), angle = 90, hjust = 1) + lims(y = c(0, 1)) 5.6.3 Optimización Bayesiana La función tune_bayes usa modelos para crear nuevas combinaciones de hiperparámetros que sean candidatos de tunning basado en resultados previos. El método utiliza métodos MCMC de simulación bayesiana para determinar los hiperparámetros óptimos. A partir de un buen candidato se realiza una exploración aleatoria que permita encontrar nuevos óptimos o sub-óptimos a encontrar. Es posible configurar el número de iteraciones antes regresar al último óptimo o sub-óptimo para continuar la exploración por otro lado. La forma de implementarlo es muy similar a las técnicas anteriores. Esta vez se utiliza la función tune_bayes y control_bayes. UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) race_ctrl &lt;- control_bayes( verbose = TRUE, save_pred = TRUE, parallel_over = &quot;everything&quot;, save_workflow = TRUE ) set.seed(20220603) tunning_race_bayes_results &lt;- workflow_tunning_set_models %&gt;% workflow_map( &quot;tune_bayes&quot;, seed = 1503, resamples = ames_folds, iter = 100, metrics = metric_set(rmse, mae, mape, rsq), control = race_ctrl, verbose = TRUE ) stopCluster(cluster) tunning_race_bayes_results %&gt;% saveRDS(&quot;models/race_bayes_results.rds&quot;) tunning_race_bayes_results &lt;- readRDS(&quot;models/race_bayes_results.rds&quot;) Las mismas funciones útiles están disponibles para que este objeto interrogue los resultados y, de hecho, el método básico autoplot() produce tendencias similares: tunning_race_bayes_results %&gt;% filter(stringr::str_detect(wflow_id, pattern = &quot;(rf)|(boost)&quot;, negate = T) ) %&gt;% rank_results(select_best = T) %&gt;% select(-c(.config, n, preprocessor, std_err)) %&gt;% pivot_wider(names_from = .metric, values_from = mean) ## # A tibble: 9 × 7 ## wflow_id model rank mae mape rmse rsq ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 receta_knn_svm_rbf svm_rbf 1 17912. 11.4 28954. 0.867 ## 2 receta_knn_knn nearest_neighbor 2 19465. 11.6 30463. 0.861 ## 3 receta_original_svm_rbf svm_rbf 3 22651. 13.7 34736. 0.811 ## 4 receta_grande_svm_rbf svm_rbf 4 22651. 13.7 34736. 0.811 ## 5 receta_original_knn nearest_neighbor 5 22512. 13.0 35283. 0.818 ## 6 receta_grande_knn nearest_neighbor 6 22512. 13.0 35283. 0.818 ## 7 receta_knn_elasticnet linear_reg 7 24197. 14.7 39880. 0.756 ## 8 receta_original_elasticnet linear_reg 8 30129. 18.4 44122. 0.694 ## 9 receta_grande_elasticnet linear_reg 9 30129. 18.4 44122. 0.694 tunning_race_bayes_results %&gt;% filter(stringr::str_detect(wflow_id, pattern = &quot;(rf)|(boost)&quot;, negate = T) ) %&gt;% autoplot( rank_metric = &quot;rsq&quot;, metric = &quot;rsq&quot;, select_best = TRUE) + geom_text(aes(y = mean - 0.10 , label = wflow_id), angle = 90, hjust = 1) + lims(y = c(0, 1)) 5.7 Stacking El ensamblaje de modelos es un proceso en el que se utilizan varios modelos base para predecir un resultado. La motivación para usar modelos de conjunto es reducir el error de generalización de la predicción. Siempre que los modelos base sean diversos e independientes, el error de predicción disminuye cuando se utiliza el enfoque de conjunto. Aunque el modelo de conjunto tiene varios modelos base dentro del modelo, actúa y funciona como un solo modelo. Un conjunto de modelos, donde las predicciones de varios modelos individuales se agregan para hacer una predicción, puede producir un modelo final de alto rendimiento. Los métodos más populares para crear modelos de conjuntos son: Bagging Bosques aleatorios Boosting Cada uno de estos métodos combina las predicciones de múltiples versiones del mismo tipo de modelo. Uno de los primeros métodos para crear conjuntos es el apilamiento de modelos (stacking). Stacking combina las predicciones de múltiples modelos de cualquier tipo. Por ejemplo, una regresión logística, un árbol de clasificación y una máquina de vectores de soporte se pueden incluir en un conjunto de apilamiento, así como diferentes configuraciones de un mismo modelo. El proceso de construcción de un conjunto apilado es: Reunir el conjunto de entrenamiento de predicciones (producidas mediante remuestreo). Crear un modelo para combinar estas predicciones. Para cada modelo del conjunto, ajustar el modelo en el conjunto de entrenamiento original. 5.7.1 Elección de modelos Para cada observación en el conjunto de entrenamiento, el apilamiento (stacking) requiere una predicción fuera de la muestra de algún tipo. Para comenzar a ensamblar con el paquete stacks, se crea una pila de datos vacía usando la función stacks() y luego se agrega el flujo de trabajo para ajustar una amplia variedad de modelos a estos datos. library(tidymodels) library(stacks) tidymodels_prefer() concrete_stack &lt;- stacks() %&gt;% add_candidates(tunning_models_result) concrete_stack ## # A data stack with 15 model definitions and 996 candidate members: ## # receta_original_elasticnet: 100 model configurations ## # receta_original_knn: 100 model configurations ## # receta_original_rf: 97 model configurations ## # receta_original_svm_rbf: 100 model configurations ## # receta_original_boost: 100 model configurations ## # receta_knn_elasticnet: 100 model configurations ## # receta_knn_knn: 100 model configurations ## # receta_knn_rf: 99 model configurations ## # receta_knn_svm_rbf: 100 model configurations ## # receta_knn_boost: 100 model configurations ## # receta_grande_elasticnet: 0 model configurations ## # receta_grande_knn: 0 model configurations ## # receta_grande_rf: 0 model configurations ## # receta_grande_svm_rbf: 0 model configurations ## # receta_grande_boost: 0 model configurations ## # Outcome: Sale_Price (integer) La regularización mediante la penalización de lazo tiene varias ventajas: El uso de la penalización de lazo puede eliminar modelos (y, a veces, tipos de modelos completos) del conjunto. La correlación entre los candidatos del conjunto tiende a ser muy alta y la regularización ayuda a mitigar este problema. Dado que nuestro resultado es numérico, se utiliza la regresión lineal para el metamodelo. set.seed(20220612) assembly &lt;- concrete_stack %&gt;% blend_predictions(metric = metric_set(rmse, mae, mape, rsq)) assembly %&gt;% saveRDS(&quot;models/stack_predictions.rds&quot;) assembly &lt;- readRDS(&quot;models/stack_predictions.rds&quot;) assembly ## ── A stacked ensemble model ───────────────────────────────────── ## ## Out of 996 possible candidate members, the ensemble retained 19. ## Penalty: 0.1. ## Mixture: 1. ## ## The 10 highest weighted members are: ## # A tibble: 10 × 3 ## member type weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 receta_knn_svm_rbf_1_007 svm_rbf 0.504 ## 2 receta_knn_boost_1_058 boost_tree 0.316 ## 3 receta_knn_boost_1_086 boost_tree 0.0962 ## 4 receta_knn_boost_1_041 boost_tree 0.0925 ## 5 receta_knn_boost_1_040 boost_tree 0.0907 ## 6 receta_knn_boost_1_035 boost_tree 0.0843 ## 7 receta_knn_boost_1_025 boost_tree 0.0739 ## 8 receta_original_knn_1_003 nearest_neighbor 0.0600 ## 9 receta_knn_svm_rbf_1_100 svm_rbf 0.0483 ## 10 receta_original_boost_1_002 boost_tree 0.0432 ## ## Members have not yet been fitted with `fit_members()`. autoplot(assembly, &quot;weights&quot;) + theme_minimal() Esto evalúa el modelo de meta aprendizaje sobre un grid predefinido de valores de penalización de lazo y utiliza un método de remuestreo interno para determinar el mejor valor. El método autoplot() nos ayuda a comprender si el método de penalización predeterminado fue suficiente: autoplot(assembly) El panel de en medio muestra el número promedio de modelos del conjunto retenidos por el modelo de meta aprendizaje. Es posible que el rango predeterminado no nos haya servido bien aquí. Para evaluar el modelo de meta aprendizaje con penalizaciones mayores. set.seed(20220612) assembly_v2 &lt;- concrete_stack %&gt;% blend_predictions( metric = metric_set(rmse, mae, mape, rsq), penalty = 10^seq(-5, 5, length = 50) ) assembly_v2 %&gt;% saveRDS(&quot;models/stack_predictions_v2.rds&quot;) assembly_v2 &lt;- readRDS(&quot;models/stack_predictions_v2.rds&quot;) autoplot(assembly_v2) ## Warning: Removed 1 rows containing missing values (geom_point). El valor de penalización asociado a las curvas fue de 1456.34. La impresión del objeto muestra los detalles del modelo de meta aprendizaje: assembly_v2 ## ── A stacked ensemble model ───────────────────────────────────── ## ## Out of 996 possible candidate members, the ensemble retained 16. ## Penalty: 1456.34847750124. ## Mixture: 1. ## ## The 10 highest weighted members are: ## # A tibble: 10 × 3 ## member type weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 receta_knn_boost_1_058 boost_tree 0.338 ## 2 receta_knn_boost_1_086 boost_tree 0.102 ## 3 receta_knn_boost_1_040 boost_tree 0.0981 ## 4 receta_knn_boost_1_035 boost_tree 0.0938 ## 5 receta_knn_boost_1_041 boost_tree 0.0829 ## 6 receta_knn_boost_1_025 boost_tree 0.0665 ## 7 receta_original_knn_1_003 nearest_neighbor 0.0516 ## 8 receta_original_boost_1_002 boost_tree 0.0427 ## 9 receta_knn_knn_1_004 nearest_neighbor 0.0388 ## 10 receta_knn_boost_1_047 boost_tree 0.0320 ## ## Members have not yet been fitted with `fit_members()`. El modelo de meta aprendizaje contenía cuatro coeficientes de combinación. El método autoplot() se puede usar nuevamente para mostrar las contribuciones de cada tipo de modelo: autoplot(assembly_v2, &quot;weights&quot;) + theme_minimal() El modelo de bosques aleatorios tiene la mayor contribución al conjunto. Para este conjunto, el resultado se predice con la ecuación: \\[\\begin{aligned} \\text{Predicción ensamblada} = -3130.71 &amp;+ 0.0516 * \\text{receta_original_knn_1_003} \\\\ &amp;+ 0.0427 * \\text{receta_original_boost_1_002} \\\\ &amp;+ 0.0015 * \\text{receta_original_boost_1_075} \\\\ &amp;+ 0.0274 * \\text{receta_knn_knn_1_003} \\\\ &amp;+ 0.0020 * \\text{receta_knn_knn_1_053} \\\\ &amp;+ 0.0387 * \\text{receta_knn_knn_1_004} \\\\ &amp;+ 0.3380 * \\text{receta_knn_boost_1_058} \\\\ &amp;+ 0.0192 * \\text{receta_knn_boost_1_028} \\\\ &amp;+ 0.0980 * \\text{receta_knn_boost_1_040} \\\\ &amp;+ 0.0264 * \\text{receta_knn_boost_1_013} \\\\ &amp;+ 0.0938 * \\text{receta_knn_boost_1_035} \\\\ &amp;+ 0.0319 * \\text{receta_knn_boost_1_047} \\\\ &amp;+ 0.0664 * \\text{receta_knn_boost_1_025} \\\\ &amp;+ 0.1015 * \\text{receta_knn_boost_1_086} \\\\ &amp;+ 0.0006 * \\text{receta_knn_boost_1_045} \\\\ &amp;+ 0.0829 * \\text{receta_knn_boost_1_041} \\\\ \\end{aligned}\\] Ahora sabemos cómo se pueden combinar sus predicciones en una predicción final para el conjunto. Sin embargo, estos ajustes de modelos individuales aún no se han creado. 5.7.2 Ajuste final Para poder usar el modelo de stacking, se requieren los ajustes de todos los modelos candidatos. Estos utilizan todo el conjunto de entrenamiento con los predictores originales. fit_assembly &lt;- fit_members(assembly_v2) fit_assembly %&gt;% saveRDS(&quot;models/stack_fit_members.rds&quot;) fit_assembly &lt;- readRDS(&quot;models/stack_fit_members.rds&quot;) fit_assembly ## ── A stacked ensemble model ───────────────────────────────────── ## ## Out of 996 possible candidate members, the ensemble retained 16. ## Penalty: 1456.34847750124. ## Mixture: 1. ## ## The 10 highest weighted members are: ## # A tibble: 10 × 3 ## member type weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 receta_knn_boost_1_058 boost_tree 0.338 ## 2 receta_knn_boost_1_086 boost_tree 0.102 ## 3 receta_knn_boost_1_040 boost_tree 0.0981 ## 4 receta_knn_boost_1_035 boost_tree 0.0938 ## 5 receta_knn_boost_1_041 boost_tree 0.0829 ## 6 receta_knn_boost_1_025 boost_tree 0.0665 ## 7 receta_original_knn_1_003 nearest_neighbor 0.0516 ## 8 receta_original_boost_1_002 boost_tree 0.0427 ## 9 receta_knn_knn_1_004 nearest_neighbor 0.0388 ## 10 receta_knn_boost_1_047 boost_tree 0.0320 Esto actualiza el objeto de apilamiento con los objetos de flujo de trabajo ajustados para cada miembro. En este punto, el modelo de stacking se puede utilizar para la predicción. regression_metrics &lt;- metric_set(rmse, mae, mape, rsq, ccc) fit_assembly_pred_test &lt;- predict(fit_assembly, ames_test) %&gt;% bind_cols(ames_test) fit_assembly_pred_test %&gt;% regression_metrics(Sale_Price, .pred) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 25774. ## 2 mae standard 16377. ## 3 mape standard 10.2 ## 4 rsq standard 0.898 ## 5 ccc standard 0.944 5.7.3 Comparación de métricas Para ver la efectividad del ensamblaje, realizamos una comparación con el mejor modelo entrenado anteriormente (XGBoost). Es importante que la comparación se realice utilizando los mismos datos de prueba. multi_metric(results, truth = Sale_Price, estimate = pred_xgb_reg) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 26851. ## 2 rsq standard 0.889 ## 3 mae standard 17078. ## 4 mape standard 10.5 ## 5 ccc standard 0.939 Este capítulo demuestra cómo combinar diferentes modelos en un conjunto para un mejor desempeño predictivo. El proceso de creación del conjunto puede eliminar automáticamente los modelos candidatos para encontrar un pequeño subconjunto que mejore el rendimiento. 5.8 Ejercicios Para obtener un mejor aprendizaje de este capítulo, cada alumno deberá: Replicar este proceso eligiendo un número de iteraciones mayor a 500 con cualquiera de las metodologías revisadas en este capítulo. Realizar el proceso análogo con los datos de respuesta categórica y mandar un reporte con sus resultados "],["sesgo-e-inequidad.html", "Capítulo 6 Sesgo e Inequidad 6.1 Propósito Vs Error 6.2 Métricas", " Capítulo 6 Sesgo e Inequidad Machine Learning por naturaleza es discriminante, pues justo lo que hacemos es discriminar datos a través del uso de la estadística. Sin embargo, esta discriminación puede ser un problema cuando brinda ventajas sistemáticas a grupos privilegiados y desventajas sistemáticas a grupos no privilegiados. Por ejemplo: Privilegiar la atención médica a pacientes blancos sobre pacientes afroamericanos. Un sesgo en el conjunto de entrenamiento ya sea por prejuicio o por un sobre/sub muestreo lleva a tener modelos sesgados. Un mal entendido común al hacer modelos de machine learning consiste en evitar utilizar características que pueden generar una inquedidad por ejemplo: sexo, edad, etnia, etc. Sin embargo, no ocuparlos nos lleva a tener puntos ciegos en nuestros modelos para cuantificar si efectivamente tenemos un sesgo o inequidad en algunos grupos. Deberemos de ocupar estas características en los modelos, justo porque queremos evitar estos sesgos. Para ello, identificaremos y cuantificaremos estos sesgos e inequedades en diferentes grupos para después mitigarlos y cuantificar la consecuencia en nuestras métricas de desempelo off line. 6.1 Propósito Vs Error El siguiente árbol de decisión está desarrollado pensando desde el punto de vista del tomador de decisiones -operativas- al que ayudamos desarrollando un modelo de machine learning para identificar en qué métricas deberíamos de concentrarnos para cuantificar el sesgo y la inequidad (bias y fairness). Métricas FP/GS: False Positive over Group Size. Es el riesgo de ser incorrectamente clasificado como positivo, dado el grupo de pertenencia FDR: False Discovery Rate. Es similar al error tipo 1 en pruebas de hipótesis estadísticas. FPR: False Positive Rate. Recall: Cobertura del modelo respecto al total de positivos. FN/GS: False Negative over Group Size. Es el riesgo de ser incorrectamente clasificado como negativo, dado el grupo de pertenencia FOR: False Omission Rate. Similar al error tipo 2 en pruebas de hipótesis estadísticas. FNR: False Negative Rate Tipo de modelo | Aplicación Modelo Punitivo: Corresponde a modelos en donde al menos una de las acciones asociadas a nuestro modelo de predicción está relacionada con un “castigo.” Por ejemplo: Algoritmos donde se predice la probabilidad de reincidencia en algún delito y que es tomada como variable para decidir si dan libertad provisional o no. Modelo Asistivo: Corresponde a modelos en donde la acción asociada al modelo son del estilo de preventivo. Por ejemplo: Priorización de inspecciones a realizar: médicas, a hogares, a estaciones de generación de energía, etc. RECORDATORIO PREDICTED REAL TP 1 1 FP 1 0 TN 0 0 FN 0 1 \\[(1-\\tau) \\leq \\text{Medida de disparidad}_\\text{grupo i} \\leq \\frac{1}{(1 - \\tau)}, \\] donde \\(\\tau\\) es el fairness threshold definido por nosotros. En los siguientes ejemplos utilizaremos \\(\\tau=20%\\) por lo que cualquier métrica de paridad que se encuentre entre \\(0.8\\) y \\(1.25\\) va a ser tratado como justo (sin sesgo). 6.2 Métricas El paquete fairness implementa \\(11\\) métricas de equidad. Muchos de estos son mutuamente excluyentes: los resultados para un problema de clasificación, a menudo no pueden ser justos en términos de todas las métricas. Dependiendo del contexto, es importante seleccionar una métrica adecuada para evaluar la equidad. A continuación, se describen las funciones utilizadas para calcular las métricas implementadas. Cada función tiene un conjunto similar de argumentos: data: data.frame que contiene los datos de entrada y las predicciones del modelo grupo: nombre de la columna que indica el grupo base (variable de factor) base: nivel base del grupo base para el cálculo de métricas de equidad resultado: nombre de la columna que indica la variable de resultado binaria result_base: nivel base de la variable de resultado (es decir, clase negativa) para el cálculo de métricas de equidad También necesitamos proporcionar predicciones de modelos, estas predicciones se pueden agregar al data.frame original o se pueden proporcionar como un vector. Cuando se trabaja con predicciones probabilísticas, algunas métricas requieren un valor de corte para convertir probabilidades en predicciones de clase proporcionadas como límite. 6.2.1 Equal Parity or Demographic or Statistical Parity Cuando nos interesa que cada grupo de la variable “protegida” tenga la misma proporción de etiquetas positivas predichas (TP). Por ejemplo: En un modelo que predice si darte o no un crédito, nos gustaría que sin importar el género de la persona tuvieran la misma oportunidad. La paridad demográfica se calcula en base a la comparación del número absoluto de todos los individuos clasificados positivamente en todos los subgrupos de datos. En el resiltado del vector con nombres, al grupo de referencia se le asignará 1, mientras que a todos los demás grupos se les asignarán valores según si su proporción de observaciones pronosticadas positivamente es menor o mayor en comparación con el grupo de referencia. Las proporciones más bajas se reflejarán en números inferiores a 1 en el vector con resultado de nombre. Fórmula: \\(TP + FP\\) Se utiliza esta métrica cuando: Queremos cambiar el estado actual para “mejorarlo.” Por ejemplo: Ver más personas de grupos desfavorecidos con mayor oportunidad de tener un préstamo. Conocemos que ha habido una ventaja histórica que afecta los datos con los que construiremos el modelo. Al querer eliminar las desventajas podríamos poner en más desventaja al grupo que históricamente ha tenido desventaja, ya que no está preparado (literalmente) para recibir esa ventaja. Por ejemplo, si damos créditos a grupos a los que antes de hacer fairness no lo hacíamos, sin ninguna educación financiera o apoyo de educación financiera de nuestra parte, muy probablemente esas personas caerán en default aumentando el sesgo que ya teníamos inicialmente. library(tidymodels) library(fairness) library(magrittr) data(compas) compas %&lt;&gt;% mutate(Two_yr_Recidivism_01 = if_else(Two_yr_Recidivism == &#39;yes&#39;, 1, 0)) glimpse(compas) ## Rows: 6,172 ## Columns: 10 ## $ Two_yr_Recidivism &lt;fct&gt; no, yes, no, no, no, no, yes, yes, yes, no, no, n… ## $ Number_of_Priors &lt;dbl&gt; -0.6843578, 2.2668817, -0.6843578, -0.6843578, -0… ## $ Age_Above_FourtyFive &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, n… ## $ Age_Below_TwentyFive &lt;fct&gt; no, no, no, no, no, no, no, no, no, yes, no, no, … ## $ Female &lt;fct&gt; Male, Male, Female, Male, Male, Male, Male, Male,… ## $ Misdemeanor &lt;fct&gt; yes, no, yes, no, yes, yes, no, no, no, no, no, y… ## $ ethnicity &lt;fct&gt; Other, Caucasian, Caucasian, African_American, Hi… ## $ probability &lt;dbl&gt; 0.3151557, 0.8854616, 0.2552680, 0.4173908, 0.320… ## $ predicted &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1… ## $ Two_yr_Recidivism_01 &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1… dem_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic ## Positively classified 1070 2599.000000 7.000000000 214.0 ## Demographic Parity 1 2.428972 0.006542056 0.2 ## Group size 2103 3175.000000 31.000000000 509.0 ## Native_American Other ## Positively classified 7.000000000 152.0000000 ## Demographic Parity 0.006542056 0.1420561 ## Group size 11.000000000 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 6.2.2 Proportional Parity o Impact Parity o Minimizing Disparate Impact Cuando nos interesa que cada grupo de la variable “protegida” tenga el mismo impacto. La paridad proporcional se logra si la proporción de predicciones positivas en los subgrupos es cercana entre sí. Similar a la paridad demográfica, esta medida tampoco depende de las etiquetas verdaderas. Fórmula: \\(\\frac{TP + FP}{TP + FP + TN + FN}\\) prop_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic ## Proportion 0.508797 0.8185827 0.2258065 0.4204322 ## Proportional Parity 1.000000 1.6088592 0.4438046 0.8263261 ## Group size 2103.000000 3175.0000000 31.0000000 509.0000000 ## Native_American Other ## Proportion 0.6363636 0.4431487 ## Proportional Parity 1.2507222 0.8709735 ## Group size 11.0000000 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 6.2.3 Equalized odds Las probabilidades igualadas se logran si las sensibilidades en los subgrupos están cerca unas de otras. Las sensibilidades específicas del grupo indican el número de verdaderos positivos dividido por el número total de positivos en ese grupo. Fórmula: \\(\\frac{TP}{TP + FN}\\) equal_odds( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic ## Sensitivity 0.676399 0.910295 0.2500000 0.5978836 ## Equalized odds 1.000000 1.345796 0.3696043 0.8839214 ## Group size 2103.000000 3175.000000 31.0000000 509.0000000 ## Native_American Other ## Sensitivity 0.6000000 0.6532258 ## Equalized odds 0.8870504 0.9657403 ## Group size 11.0000000 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 6.2.4 Predictive rate parity La paridad de tasa predictiva se logra si las precisiones (o valores predictivos positivos) en los subgrupos están cerca unas de otras. La precisión representa el número de verdaderos positivos dividido por el número total de ejemplos predichos positivos dentro de un grupo. Fórmula: \\(\\frac{TP}{TP + FP}\\) pred_rate_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic ## Precision 0.5196262 0.5817622 0.2857143 0.5280374 ## Predictive Rate Parity 1.0000000 1.1195784 0.5498458 1.0161871 ## Group size 2103.0000000 3175.0000000 31.0000000 509.0000000 ## Native_American Other ## Precision 0.4285714 0.5328947 ## Predictive Rate Parity 0.8247688 1.0255348 ## Group size 11.0000000 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 6.2.5 Accuracy parity La paridad de precisión se logra si las precisiones (todos los ejemplos clasificados con precisión divididos por el número total de ejemplos) en los subgrupos están cerca entre sí. Fórmula: \\(\\frac{TP + TN}{TP + FP + TN + FN}\\) acc_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic ## Accuracy 0.6291013 0.6107087 0.6451613 0.6522593 ## Accuracy Parity 1.0000000 0.9707637 1.0255285 1.0368113 ## Group size 2103.0000000 3175.0000000 31.0000000 509.0000000 ## Native_American Other ## Accuracy 0.4545455 0.6676385 ## Accuracy Parity 0.7225314 1.0612575 ## Group size 11.0000000 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 6.2.6 False Negative Parity o Equal Oppportunity La paridad de tasas de falsos negativos se logra si las tasas de falsos negativos (la relación entre el número de falsos negativos y el número total de positivos) en los subgrupos están cerca entre sí. Fórmula: \\(\\frac{FN}{TP + FN}\\) Se utiliza esta métrica cuando: El modelo necesita ser muy bueno en detectar la etiqueta positiva. No hay -mucho- costo en introducir falsos negativos al sistema -tanto al usuario como a la empresa-. Por ejemplo: Generar FPs en tarjeta de crédito. La definición de la variable target no es subjetiva. Por ejemplo: Fraude o No Fraude no es algo subjetivo, buen empleado o no, puede ser muy subjetivo. Para poder cumplir con tener el mismo porcentaje de TPR en todos los grupos de la variable protegida, incurriremos en agregar más falsos positivos, lo que puede afectar más a ese grupo a largo plazo. fnr_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic Native_American ## FNR 0.323601 0.0897050 0.750000 0.4021164 0.40000 ## FNR Parity 1.000000 0.2772087 2.317669 1.2426304 1.23609 ## Group size 2103.000000 3175.0000000 31.000000 509.0000000 11.00000 ## Other ## FNR 0.3467742 ## FNR Parity 1.0716105 ## Group size 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 6.2.7 False Positive Parity Cuando queremos que todos los grupos de la variable protegida tengan la misma tasa de falsos positivos. Es decir, nos equivocamos en las mismas proporciones para etiquetas positivas que eran negativas. Fórmula: \\(\\frac{FP}{TN + FP}\\) fpr_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic Native_American ## FPR 0.401249 0.7179657 0.2173913 0.3156250 0.6666667 ## FPR Parity 1.000000 1.7893269 0.5417865 0.7866063 1.6614786 ## Group size 2103.000000 3175.0000000 31.0000000 509.0000000 11.0000000 ## Other ## FPR 0.3242009 ## FPR Parity 0.8079793 ## Group size 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 6.2.8 Negative predictive value parity La paridad de valor predictivo negativo se logra si los valores predictivos negativos en los subgrupos están cerca unos de otros. El valor predictivo negativo se calcula como una relación entre el número de negativos verdaderos y el número total de negativos previstos. Esta función puede considerarse la “inversa” de la paridad de tasa predictiva (predictive rate parity). Fórmula: \\(\\frac{TN}{TN + FN}\\) npv_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic Native_American ## NPV 0.7424976 0.7413194 0.750000 0.7423729 0.5000000 ## NPV Parity 1.0000000 0.9984133 1.010104 0.9998321 0.6734029 ## Group size 2103.0000000 3175.0000000 31.000000 509.0000000 11.0000000 ## Other ## NPV 0.7748691 ## NPV Parity 1.0435982 ## Group size 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 6.2.9 Specificity parity La paridad de especificidad se logra si las especificidades (la relación entre el número de verdaderos negativos y el número total de negativos) en los subgrupos están próximas entre sí. Esta función puede considerarse la “inversa” de las probabilidades igualadas (equalized odds). Fórmula: \\(\\frac{TN}{TN + FP}\\) spec_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic ## Specificity 0.598751 0.2820343 0.7826087 0.684375 ## Specificity Parity 1.000000 0.4710378 1.3070688 1.143004 ## Group size 2103.000000 3175.0000000 31.0000000 509.000000 ## Native_American Other ## Specificity 0.3333333 0.6757991 ## Specificity Parity 0.5567145 1.1286814 ## Group size 11.0000000 343.0000000 ## ## $Metric_plot ## ## $Probability_plot "],["interpretabilidad-de-modelos.html", "Capítulo 7 Interpretabilidad de modelos 7.1 LIME 7.2 DALEXtra", " Capítulo 7 Interpretabilidad de modelos Interpretabilidad: En Machine Learning, nos referimos a interpretabilidad al grado en el que un humano puede entender la causa de una decisión o clasificación, la cual nos permite identificar y evitar tener sesgo, injusticia, inequidad en los modelos que generamos. Poder interpretar nuestros modelos nos brinda más confianza en que lo que estamos haciendo es correcto (además de las métricas de desempeño). Por otro lado, para la gente que lo ocupa, permite transparentar y tener más confianza al modelo. Ejemplos Etiquetado: Google photos (2015) etiqueta incorrectamente personas afroamericanas como gorillas. Facial Recognition (IBM, Microsoft, Megvii): Reconocimiento para hombres blancos 99%, mujeres afroamericanas 35%. Facebook automatic translation: Arresto de un palestino por traducción incorrecta de “buenos días” en hebreo a “atácalos.” Interpretabilidad en ML General Data Protection Regulation (GDPR): Desde mayo de 2018 existe el right to explanation. Por ejemplo: Algoritmos de predicción de riesgo en créditos hipotecarios. Se puede tener interpretabilidad de modelos de aprendizaje supervisado. Se tiene la creencia equivocada de que en Europa no se puede ocupar Deep Learning debido a la falta de interpretabilidad y el GDPR. Esto no es verdad. Es verdad que preferimos ocupar modelos más simples porque nos permiten entender -y explicar- de manera más sencilla por qué se están tomando las decisiones. Te recomiendo leer el artículo Why should I trust you? (2016) base de mucho de lo desarrollado para interpretabilidad. 7.1 LIME Acrónimo de Local Interpretable Model-Agnostic Explanation. El objetivo es tener explicacion que un humano pueda entender sobre cualquier modelo supervisado a través de un modelo local más simple. La explicación se genera para cada predicción realizada. Esta basado en la suposición de que un modelo complejo es lineal en una escala local. En el contexto de LIME: Local: Se refiere a que un modelo simple es lo suficientemente bueno/igual de bueno localmente que uno complejo globalmente. Interpretable: Se refiere a que la explicación debe ser entendida por un ser humano. Model-agnostic: Se refiere a tratar a cualquier modelo de clasificación, complejo o no, como una caja negra a la que metemos observaciones y obtenemos predicciones, no nos interesa cómo genera estas predicciones, lo que nos interesa es generarlas. 7.1.1 Proceso El objetivo de LIME es entender por qué el modelo de machine learning hace cierta predicción, para ello LIME prueba qué pasa con las predicciones si le brindamos al modelo las mismas observaciones pero con variaciones. Genera un nuevo set de datos que consiste de una muestra permutada con sus predicciones originales. Con este nuevo set LIME entrena un modelo interpretable que puede ser: regresión lineal como LASSO regresión logística árboles de decisión Naive Bayes K-NN. etc. Este modelo interpretable es ponderado por la proximidad de las observaciones en la muestra a la instancia de interés (la predicción que queremos explicar). El modelo generado debe ser una buena aproximación al modelo de caja negra localmente, pero no necesariamente en forma global. A esto se le llama local fidelity. Para entrenar el modelo interpertable, seguimos los siguientes pasos: Seleccionamos la observación de la que queremos una explicación. Seleccionamos una vecindad para que LIME ajuste el modelo. Seleccionamos el número de features más importantes con los que queremos realizar la explicación (se recomienda que sea menor a \\(10\\)). LIME genera perturbaciones en una muestra de datos del conjunto de datos Texto: “Agrega” o “quita” palabras del texto original aleatoriamente. La columna de probabilidad corresponde a la probabilidad de que el enunciado sea Spam o no. La columna peso corresponde a la proximidad del enunciado con variación al enunciado original y está calculado como 1 menos la proporción de palabras que fueron eliminadas. Por ejemplo: Si una de 7 palabras fueron removidas, el peso correspondería a: \\(1- 1/7=0.86\\). Al pasar dos enunciados al modelo de interpretabilidad se identifica que para los casos donde está la palabra channel! el enunciado será clasificado como spam que es el feature con mayor peso para la etiqueta spam. Imágenes: “Apaga” y “prende” pixeles de la imágen original. Datos tabulares: De cada feature genera nuevas muestras tomadas de una distribución normal con la \\(\\mu\\) y \\(\\sigma\\) del feature. Las observaciones más “cercanas” tendrán más peso que el resto. LIME ocupa un kernel de suavizamiento exponencial. La implementación de LIME ocupa como kernel \\(0.75\\) veces la raíz cuadrada del número de features que tenga nuestro dataset. ¿Eso es bueno o malo? Hasta el momento no hay una justificación matemática o estadística que justifique el definir ese número. Obtiene las predicciones del modelo de caja negra para las observaciones generadas en el paso \\(3\\). Calcula las distancias entre la predicción original y las obtenidas con los datos modificados. Datos tabulares: Distancia euclidiana por default. Datos numéricos: Se obtiene la media y desviación estándar y se discretiza a sus cuartiles. Datos categóricos: Se calcula la frecuencia de cada valor. Texto: Distancia coseno. Imágenes: Distancia euclidiana. Convierte la distancia a un score de similitud. Datos tabulares: Utiliza el kernel de suavizamiento exponencial. Texto: Distancia coseno. Imágenes: Genera un modelo simple con los datos modificados Selecciona las \\(m\\) mejores características (depende del modelo utilizado para crear el modelo de interpretabilidad) como explicación para la predicción del modelo de caja negra. 7.1.2 Características principales Vecindad: kernel con suavizamiento exponencial. Peso de influencia Similitud: Con respecto a las observaciones originales. Selección de variables: Las variables que explican la predicción generada. Ventajas: Fácil de implementar. Se puede ocupar en datos tabulares, imágenes y texto. Existen paquetes de implementación para R (lime) y Python (lime). Las explicaciones son cortas (pocos features), por lo que son más fáciles de entender por un humano no entrenado en machine learning. La métrica de fidelity nos permite identificar qué tan confiable es el modelo de interpretabilidad en explicar las predicciones del modelo de caja negra en las vecindad del punto de interés. Se pueden ocupar otros features en el modelo de interpretabilidad que los ocupados para entrenar el de caja negra. Desventajas: La selección correcta de vecindad es el problema más grave de LIME. Para minimizar este problema deberemos probar con diferentes vecindades y ver cuál(es) son las de mayor sentido. Variación de las observaciones originales: ¿Qué tal que la distribución no es normal? Al muestrear de una distribución gausiana podemos ignorar correlaciones entre features. Dependiendo del tamaño de la vecindad tendremos resultados muy diferentes. Este es el mayor problema de LIME. 7.1.3 Implementación con R library(pdp) library(vip) library(randomForest) library(lime) set.seed(123) model_rf &lt;- randomForest( x = dplyr::select(telco_train, -Churn), y = telco_train$Churn, ntree = 100 ) explainer_caret &lt;- lime::lime(dplyr::select(telco_train, -Churn), model_rf) Veamos cómo llegaron a ser la predicciones, usando Lime. model_type.randomForest &lt;- function(x,...){ return(&quot;classification&quot;) # for classification problem } predict_model.randomForest &lt;- function(x, newdata, type = &quot;prob&quot;) { # return prediction value predict(x, newdata, type = type) %&gt;% as.data.frame() } set.seed(123) new_data &lt;- telco_train %&gt;% dplyr::select(-Churn) %&gt;% .[2,] new_data %&gt;% t() ## [,1] ## SeniorCitizen -0.4417148 ## MonthlyCharges 0.6636772 ## TotalCharges 0.4721422 ## gender_Male 1.0000000 ## Partner_Yes 1.0000000 ## Dependents_Yes 1.0000000 ## tenure_X1.2.years 0.0000000 ## tenure_X2.3.years 0.0000000 ## tenure_X3.4.years 1.0000000 ## tenure_X4.5.years 0.0000000 ## tenure_X5.6.years 0.0000000 ## PhoneService_Yes 1.0000000 ## MultipleLines_Yes 0.0000000 ## InternetService_Fiber.optic 1.0000000 ## InternetService_No 0.0000000 ## OnlineSecurity_Yes 0.0000000 ## OnlineBackup_Yes 0.0000000 ## DeviceProtection_Yes 1.0000000 ## TechSupport_Yes 0.0000000 ## StreamingTV_Yes 1.0000000 ## StreamingMovies_Yes 0.0000000 ## Contract_One.year 1.0000000 ## Contract_Two.year 0.0000000 ## PaperlessBilling_Yes 1.0000000 ## PaymentMethod_Credit.card..automatic. 0.0000000 ## PaymentMethod_Electronic.check 1.0000000 ## PaymentMethod_Mailed.check 0.0000000 explanation &lt;- lime::explain( x = new_data, explainer = explainer_caret, feature_select = &quot;auto&quot;, # Method of feature selection for lime n_features = 10, # Number of features to explain the model n_labels = 1 ) Para obtener una representación más intuitiva, podemos usar plot_features() proporcionado para obtener una descripción visual de las explicaciones. plot_features(explanation, ncol = 1) Otro ejemplo set.seed(123) new_data2 &lt;- telco_train %&gt;% dplyr::select(-Churn) %&gt;% .[c(2, 4),] new_data2 %&gt;% t() ## [,1] [,2] ## SeniorCitizen -0.4417148 2.2634452 ## MonthlyCharges 0.6636772 0.6487142 ## TotalCharges 0.4721422 -0.8504924 ## gender_Male 1.0000000 0.0000000 ## Partner_Yes 1.0000000 0.0000000 ## Dependents_Yes 1.0000000 0.0000000 ## tenure_X1.2.years 0.0000000 0.0000000 ## tenure_X2.3.years 0.0000000 0.0000000 ## tenure_X3.4.years 1.0000000 0.0000000 ## tenure_X4.5.years 0.0000000 0.0000000 ## tenure_X5.6.years 0.0000000 0.0000000 ## PhoneService_Yes 1.0000000 1.0000000 ## MultipleLines_Yes 0.0000000 1.0000000 ## InternetService_Fiber.optic 1.0000000 1.0000000 ## InternetService_No 0.0000000 0.0000000 ## OnlineSecurity_Yes 0.0000000 0.0000000 ## OnlineBackup_Yes 0.0000000 0.0000000 ## DeviceProtection_Yes 1.0000000 0.0000000 ## TechSupport_Yes 0.0000000 0.0000000 ## StreamingTV_Yes 1.0000000 0.0000000 ## StreamingMovies_Yes 0.0000000 1.0000000 ## Contract_One.year 1.0000000 0.0000000 ## Contract_Two.year 0.0000000 0.0000000 ## PaperlessBilling_Yes 1.0000000 1.0000000 ## PaymentMethod_Credit.card..automatic. 0.0000000 0.0000000 ## PaymentMethod_Electronic.check 1.0000000 1.0000000 ## PaymentMethod_Mailed.check 0.0000000 0.0000000 explanation2 &lt;- lime::explain( x = new_data2, explainer = explainer_caret, feature_select = &quot;auto&quot;, # Method of feature selection for lime n_features = 10, # Number of features to explain the model n_labels = 1 ) plot_features(explanation2, ncol = 2) 7.2 DALEXtra El marco tidymodels no contiene software para explicaciones de modelos. En cambio, los modelos entrenados y evaluados con tidymodels se pueden explicar con paquetes complementarios como lime, vip y DALEX: DALEX: Contiene funciones que usamos cuando queremos usar métodos explicativos independientes del modelo, por lo que se pueden aplicar a cualquier algoritmo. Construyamos explicaciones independientes del modelo de regresión de XGBoost para descubrir por qué hacen las predicciones que hacen. Podemos usar el paquete adicional DALEXtra para implementar DALEX, que brinda soporte para tidymodels. # Se declara el modelo de clasificación xgboost_reg_model &lt;- boost_tree( mode = &quot;regression&quot;, trees = 1000, tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), learn_rate = tune() ) %&gt;% set_engine(&quot;xgboost&quot;) # Se declara el flujo de trabajo xgboost_workflow &lt;- workflow() %&gt;% add_model(xgboost_reg_model) %&gt;% add_recipe(receta_casas_prep) xgboost_tune_result &lt;- readRDS(&quot;models/xgboost_model_reg.rds&quot;) best_xgboost_model_1se &lt;- select_by_one_std_err( xgboost_tune_result, metric = &quot;rsq&quot;, &quot;rsq&quot;) # Selección del mejor modelo final_xgboost_model &lt;- xgboost_workflow %&gt;% finalize_workflow(best_xgboost_model_1se) %&gt;% fit(data = ames_train) library(DALEXtra) explainer_xgb &lt;- explain_tidymodels( final_xgboost_model, data = ames_train, y = ames_train$Sale_Price , label = &quot;XGBoost&quot;, verbose = TRUE ) ## Preparation of a new explainer is initiated ## -&gt; model label : XGBoost ## -&gt; data : 2197 rows 74 cols ## -&gt; data : tibble converted into a data.frame ## -&gt; target variable : 2197 values ## -&gt; predict function : yhat.workflow will be used ( default ) ## -&gt; predicted values : No value for predict function target column. ( default ) ## -&gt; model_info : package tidymodels , ver. 0.2.0 , task regression ( default ) ## -&gt; predicted values : numerical, min = 36927.61 , mean = 180440.3 , max = 704172.8 ## -&gt; residual function : difference between y and yhat ( default ) ## -&gt; residuals : numerical, min = -49510.72 , mean = 117.1645 , max = 50827.19 ## A new explainer has been created! Las explicaciones del modelo local proporcionan información sobre una predicción para una sola observación. Por ejemplo, consideremos una casa dúplex antigua en el vecindario de North Ames. duplex &lt;- ames_train[537,] duplex %&gt;% glimpse() ## Rows: 1 ## Columns: 74 ## $ MS_SubClass &lt;fct&gt; Duplex_All_Styles_and_Ages ## $ MS_Zoning &lt;fct&gt; Residential_Low_Density ## $ Lot_Frontage &lt;dbl&gt; 60 ## $ Lot_Area &lt;int&gt; 7200 ## $ Street &lt;fct&gt; Pave ## $ Alley &lt;fct&gt; No_Alley_Access ## $ Lot_Shape &lt;fct&gt; Regular ## $ Land_Contour &lt;fct&gt; Lvl ## $ Utilities &lt;fct&gt; AllPub ## $ Lot_Config &lt;fct&gt; Inside ## $ Land_Slope &lt;fct&gt; Gtl ## $ Neighborhood &lt;fct&gt; North_Ames ## $ Condition_1 &lt;fct&gt; Norm ## $ Condition_2 &lt;fct&gt; Norm ## $ Bldg_Type &lt;fct&gt; Duplex ## $ House_Style &lt;fct&gt; One_Story ## $ Overall_Cond &lt;fct&gt; Average ## $ Year_Built &lt;int&gt; 1949 ## $ Year_Remod_Add &lt;int&gt; 1950 ## $ Roof_Style &lt;fct&gt; Gable ## $ Roof_Matl &lt;fct&gt; CompShg ## $ Exterior_1st &lt;fct&gt; BrkFace ## $ Exterior_2nd &lt;fct&gt; Stone ## $ Mas_Vnr_Type &lt;fct&gt; None ## $ Mas_Vnr_Area &lt;dbl&gt; 0 ## $ Exter_Cond &lt;fct&gt; Typical ## $ Foundation &lt;fct&gt; Slab ## $ Bsmt_Cond &lt;fct&gt; No_Basement ## $ Bsmt_Exposure &lt;fct&gt; No_Basement ## $ BsmtFin_Type_1 &lt;fct&gt; No_Basement ## $ BsmtFin_SF_1 &lt;dbl&gt; 5 ## $ BsmtFin_Type_2 &lt;fct&gt; No_Basement ## $ BsmtFin_SF_2 &lt;dbl&gt; 0 ## $ Bsmt_Unf_SF &lt;dbl&gt; 0 ## $ Total_Bsmt_SF &lt;dbl&gt; 0 ## $ Heating &lt;fct&gt; Wall ## $ Heating_QC &lt;fct&gt; Fair ## $ Central_Air &lt;fct&gt; N ## $ Electrical &lt;fct&gt; FuseF ## $ First_Flr_SF &lt;int&gt; 1040 ## $ Second_Flr_SF &lt;int&gt; 0 ## $ Gr_Liv_Area &lt;int&gt; 1040 ## $ Bsmt_Full_Bath &lt;dbl&gt; 0 ## $ Bsmt_Half_Bath &lt;dbl&gt; 0 ## $ Full_Bath &lt;int&gt; 2 ## $ Half_Bath &lt;int&gt; 0 ## $ Bedroom_AbvGr &lt;int&gt; 2 ## $ Kitchen_AbvGr &lt;int&gt; 2 ## $ TotRms_AbvGrd &lt;int&gt; 6 ## $ Functional &lt;fct&gt; Typ ## $ Fireplaces &lt;int&gt; 0 ## $ Garage_Type &lt;fct&gt; Detchd ## $ Garage_Finish &lt;fct&gt; Unf ## $ Garage_Cars &lt;dbl&gt; 2 ## $ Garage_Area &lt;dbl&gt; 420 ## $ Garage_Cond &lt;fct&gt; Typical ## $ Paved_Drive &lt;fct&gt; Paved ## $ Wood_Deck_SF &lt;int&gt; 0 ## $ Open_Porch_SF &lt;int&gt; 0 ## $ Enclosed_Porch &lt;int&gt; 0 ## $ Three_season_porch &lt;int&gt; 0 ## $ Screen_Porch &lt;int&gt; 0 ## $ Pool_Area &lt;int&gt; 0 ## $ Pool_QC &lt;fct&gt; No_Pool ## $ Fence &lt;fct&gt; No_Fence ## $ Misc_Feature &lt;fct&gt; None ## $ Misc_Val &lt;int&gt; 0 ## $ Mo_Sold &lt;int&gt; 6 ## $ Year_Sold &lt;int&gt; 2009 ## $ Sale_Type &lt;fct&gt; WD ## $ Sale_Condition &lt;fct&gt; Normal ## $ Sale_Price &lt;int&gt; 90000 ## $ Longitude &lt;dbl&gt; -93.6089 ## $ Latitude &lt;dbl&gt; 42.03584 Existen múltiples enfoques para comprender por qué un modelo predice un precio determinado para esta casa dúplex. Una se denomina explicación de “desglose” y calcula cómo las contribuciones atribuidas a características individuales cambian la predicción del modelo medio para una observación en particular, como nuestro dúplex. Para el modelo lide bosques aleatorios, las variables Total_Bsmt_SF, Gr_Liv_Area y BsmtFin_Type_1 son las que más contribuyen a que el precio baje desde la intercepción. xgb_breakdown &lt;- predict_parts(explainer = explainer_xgb, new_observation = duplex) xgb_breakdown %&gt;% saveRDS(&quot;models/xgb_breakdown.rds&quot;) xgb_breakdown &lt;- readRDS(&#39;models/xgb_breakdown.rds&#39;) ¡¡ TEORÍA !! La idea detrás de Shapley Additive Explanations (Lundberg y Lee 2017), es que las contribuciones promedio de las características se calculan bajo diferentes combinaciones o “coaliciones” de ordenamientos de características. Calculemos las atribuciones SHAP para nuestra dúplex, usando B = 20 ordenaciones aleatorias. set.seed(1801) shap_duplex &lt;- predict_parts( explainer = explainer_xgb, new_observation = duplex, type = &quot;shap&quot;, B = 20 ) shap_duplex %&gt;% saveRDS(&#39;models/shap_duplex.rds&#39;) Los diagramas de caja de la figura siguiente muestran la distribución de las contribuciones en todos los ordenamientos que probamos, y las barras muestran la atribución promedio para cada variable. shap_duplex &lt;- readRDS(&#39;models/shap_duplex.rds&#39;) shap_duplex2 &lt;- shap_duplex %&gt;% as_tibble() %&gt;% dplyr::filter(contribution !=0) %&gt;% dplyr::arrange(desc(abs(contribution))) shap_duplex2 %&gt;% group_by(variable) %&gt;% mutate(mean_val = mean(contribution)) %&gt;% ungroup() %&gt;% mutate(variable = fct_reorder(variable, abs(mean_val))) %&gt;% ggplot(aes(contribution, variable, fill = mean_val &gt; 0)) + geom_col(data = ~distinct(., variable, mean_val), aes(mean_val, variable), alpha = 0.5) + geom_boxplot(width = 0.5) + scale_fill_viridis_d() + theme(legend.position = &quot;none&quot;) + labs(y = NULL) ¿Qué pasa con una observación diferente en nuestro conjunto de datos? Veamos una casa unifamiliar más grande y nueva en el vecindario de Gilbert. big_house &lt;- ames_train[1671,] big_house %&gt;% glimpse() ## Rows: 1 ## Columns: 74 ## $ MS_SubClass &lt;fct&gt; Two_Story_1946_and_Newer ## $ MS_Zoning &lt;fct&gt; Residential_Low_Density ## $ Lot_Frontage &lt;dbl&gt; 0 ## $ Lot_Area &lt;int&gt; 8068 ## $ Street &lt;fct&gt; Pave ## $ Alley &lt;fct&gt; No_Alley_Access ## $ Lot_Shape &lt;fct&gt; Slightly_Irregular ## $ Land_Contour &lt;fct&gt; Lvl ## $ Utilities &lt;fct&gt; AllPub ## $ Lot_Config &lt;fct&gt; Inside ## $ Land_Slope &lt;fct&gt; Gtl ## $ Neighborhood &lt;fct&gt; Gilbert ## $ Condition_1 &lt;fct&gt; Norm ## $ Condition_2 &lt;fct&gt; Norm ## $ Bldg_Type &lt;fct&gt; OneFam ## $ House_Style &lt;fct&gt; Two_Story ## $ Overall_Cond &lt;fct&gt; Average ## $ Year_Built &lt;int&gt; 2002 ## $ Year_Remod_Add &lt;int&gt; 2002 ## $ Roof_Style &lt;fct&gt; Gable ## $ Roof_Matl &lt;fct&gt; CompShg ## $ Exterior_1st &lt;fct&gt; VinylSd ## $ Exterior_2nd &lt;fct&gt; VinylSd ## $ Mas_Vnr_Type &lt;fct&gt; None ## $ Mas_Vnr_Area &lt;dbl&gt; 0 ## $ Exter_Cond &lt;fct&gt; Typical ## $ Foundation &lt;fct&gt; PConc ## $ Bsmt_Cond &lt;fct&gt; Typical ## $ Bsmt_Exposure &lt;fct&gt; No ## $ BsmtFin_Type_1 &lt;fct&gt; Unf ## $ BsmtFin_SF_1 &lt;dbl&gt; 7 ## $ BsmtFin_Type_2 &lt;fct&gt; Unf ## $ BsmtFin_SF_2 &lt;dbl&gt; 0 ## $ Bsmt_Unf_SF &lt;dbl&gt; 1010 ## $ Total_Bsmt_SF &lt;dbl&gt; 1010 ## $ Heating &lt;fct&gt; GasA ## $ Heating_QC &lt;fct&gt; Excellent ## $ Central_Air &lt;fct&gt; Y ## $ Electrical &lt;fct&gt; SBrkr ## $ First_Flr_SF &lt;int&gt; 1010 ## $ Second_Flr_SF &lt;int&gt; 1257 ## $ Gr_Liv_Area &lt;int&gt; 2267 ## $ Bsmt_Full_Bath &lt;dbl&gt; 0 ## $ Bsmt_Half_Bath &lt;dbl&gt; 0 ## $ Full_Bath &lt;int&gt; 2 ## $ Half_Bath &lt;int&gt; 1 ## $ Bedroom_AbvGr &lt;int&gt; 4 ## $ Kitchen_AbvGr &lt;int&gt; 1 ## $ TotRms_AbvGrd &lt;int&gt; 8 ## $ Functional &lt;fct&gt; Typ ## $ Fireplaces &lt;int&gt; 1 ## $ Garage_Type &lt;fct&gt; BuiltIn ## $ Garage_Finish &lt;fct&gt; RFn ## $ Garage_Cars &lt;dbl&gt; 2 ## $ Garage_Area &lt;dbl&gt; 390 ## $ Garage_Cond &lt;fct&gt; Typical ## $ Paved_Drive &lt;fct&gt; Paved ## $ Wood_Deck_SF &lt;int&gt; 120 ## $ Open_Porch_SF &lt;int&gt; 46 ## $ Enclosed_Porch &lt;int&gt; 0 ## $ Three_season_porch &lt;int&gt; 0 ## $ Screen_Porch &lt;int&gt; 0 ## $ Pool_Area &lt;int&gt; 0 ## $ Pool_QC &lt;fct&gt; No_Pool ## $ Fence &lt;fct&gt; No_Fence ## $ Misc_Feature &lt;fct&gt; None ## $ Misc_Val &lt;int&gt; 0 ## $ Mo_Sold &lt;int&gt; 12 ## $ Year_Sold &lt;int&gt; 2009 ## $ Sale_Type &lt;fct&gt; ConLI ## $ Sale_Condition &lt;fct&gt; Normal ## $ Sale_Price &lt;int&gt; 200000 ## $ Longitude &lt;dbl&gt; -93.64331 ## $ Latitude &lt;dbl&gt; 42.05938 Calculamos las atribuciones promedio SHAP de la misma manera. set.seed(1802) shap_house &lt;- predict_parts( explainer = explainer_xgb, new_observation = big_house, type = &quot;shap&quot;, B = 20 ) shap_house %&gt;% saveRDS(&#39;models/shap_house.rds&#39;) shap_house &lt;- readRDS(&#39;models/shap_house.rds&#39;) shap_house2 &lt;- shap_house %&gt;% as_tibble() %&gt;% dplyr::filter(contribution !=0) %&gt;% dplyr::arrange(desc(abs(contribution))) shap_house2 %&gt;% group_by(variable) %&gt;% mutate(mean_val = mean(contribution)) %&gt;% ungroup() %&gt;% mutate(variable = fct_reorder(variable, abs(mean_val))) %&gt;% ggplot(aes(contribution, variable, fill = mean_val &gt; 0)) + geom_col(data = ~distinct(., variable, mean_val), aes(mean_val, variable), alpha = 0.5) + geom_boxplot(width = 0.5) + scale_fill_viridis_d() + theme(legend.position = &quot;none&quot;) + labs(y = NULL) A diferencia del dúplex, el área habitable, año de construcción y área del segundo piso de esta casa contribuyen a que su precio sea más alto. Los paquetes como DALEX y su paquete de soporte DALEXtra y lime se pueden integrar en un análisis de tidymodels para proporcionar este tipo de explicativos de modelos. Las explicaciones del modelo son solo una parte de la comprensión de si su modelo es apropiado y efectivo, junto con las estimaciones del rendimiento del modelo. 7.2.1 Otros métodos Partial Dependence Plot (PDP). Efecto marginal de una o dos variables en la predicción. Accumulated Local Effects (ALE). Cómo las variables influyen en promedio a la predicción. 7.2.2 Consejos Trata cada modelo que desarrollas como una afectación de vida o muerte a un humano directamente. Identifica si estas agregando sesgo, inequidad, injusticia con interpretabilidad. Siempre que tengas un modelo de aprendizaje supervisado genera interpretabilidad en tu proceso de desarrollo. Siempre que tengas un modelo de aprendizaje supervisado genera interpretabilidad para el usuario final. "],["ab-testing.html", "Capítulo 8 A/B testing 8.1 Elementos en riesgo 8.2 Costo de retención", " Capítulo 8 A/B testing Los modelos de machine learning en la gran mayoría de las ocasiones tienen un propósito comercial. Como científicos de datos, es nuestra responsabilidad ayudar al negocio a tomar mejores decisiones que beneficien a la empresa en donde se ha desarrollado el modelo predictivo. Particularmente, una de las aplicaciones de mayor impacto en los últimos años es la reducción del CHURN El churn rate o tasa de cancelación, es el porcentaje de clientes o suscriptores que dejan de utilizar los servicios que ofrece una empresa. Es bastante conocido en el mundo del marketing que es mucho más caro adquirir nuevos clientes que retener a aquellos con los que ya cuenta la empresa. Es por esta razón que adicional a los esfuerzos de captar nuevos clientes se realizan esfuerzos por retener a los clientes con alta probabilidad de abandonar la empresa. A partir de los modelos de machine learning que han sido estudiados en el curso para detectar la posible inclusión a una categoría (cancelación de clientes) es que se realizará en este capítulo el estudio de técnicas para cuantificar el impacto de los modelos predictivos y de las estrategias de retención. Existen múltiples estrategias que surgen en las áreas de marketing para retener a los clientes. Hay una inversión muy grande de dinero que busca encontrar las mejores ideas que contribuyan a la retención. Sin embargo, poner todas estas ideas y estrategias a trabajar puede ser bastante costoso y riesgoso. ¿Qué pasa si la empresa gasta grandes cantidades de dinero en campañas publicitarias y estas no ayudan a alcanzar la meta? ¿Qué sucede si se le invierte mucho tiempo en refinar la estrategia y esta nunca atrae a los clientes esperados? Este capítulo está destinado a discutir métodos de evaluación de esas ideas, de modo que antes de comprometernos completamente con una campaña e implementarla con todos los clientes, podamos hacer pequeñas pruebas con significancia estadística para determinar cuál de ellas tiene el mejor impacto sobre los objetivos planteados. 8.1 Elementos en riesgo El primer paso es detectar a los posibles canceladores de servicios para la empresa. Esta tarea ha sido ampliamente estudiada en el curso mediante múltiples modelos predictivos, entre los que se encuentran: Regresión bernoulli (logística) Regresión Ridge Regresión Lasso KNN Árbol de decisión Bagging Bosque aleatorio Support Vector Machine Boosting Stacking Recordemos que anteriormente ya se ha realizado la partición de los datos y se cuenta con varias configuraciones de cada modelo. library(tidyverse) library(tidymodels) library(readr) library(patchwork) telco &lt;- read_csv(&quot;data/Churn.csv&quot;) set.seed(1234) telco_split &lt;- initial_split(telco, prop = .70) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) svm_tune_class_result &lt;- readRDS(&quot;models/svm_model_class.rds&quot;) A partir de los resultados de los modelos, se realizó una priorización para determinar cuál de ellos eran los más valiosos en términos de predicción. show_best(svm_tune_class_result, n = 10, metric = &quot;roc_auc&quot;) ## # A tibble: 10 × 9 ## cost rbf_sigma margin .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.23 0.00686 -1.20 roc_auc binary 0.808 7 0.00593 Preprocessor1_… ## 2 1.02 0.0303 0.230 roc_auc binary 0.805 9 0.00529 Preprocessor1_… ## 3 1.05 0.00823 0.904 roc_auc binary 0.805 9 0.00624 Preprocessor1_… ## 4 1.14 0.0215 1.11 roc_auc binary 0.804 10 0.00505 Preprocessor1_… ## 5 1.14 0.0154 -1.91 roc_auc binary 0.803 8 0.00625 Preprocessor1_… ## 6 1.19 0.0306 -1.45 roc_auc binary 0.803 10 0.00517 Preprocessor1_… ## 7 1.06 0.0350 -1.54 roc_auc binary 0.803 10 0.00517 Preprocessor1_… ## 8 1.22 0.0360 0.552 roc_auc binary 0.803 10 0.00517 Preprocessor1_… ## 9 1.36 0.0368 0.596 roc_auc binary 0.803 10 0.00515 Preprocessor1_… ## 10 1.11 0.00676 -1.42 roc_auc binary 0.802 9 0.00670 Preprocessor1_… Y mediante una elección de métrica y método, se seleccionó al más valioso para re-entrenarse usando todos los datos disponibles de entrenamiento. svm_classification_best_model &lt;- select_best(svm_tune_class_result, metric = &quot;roc_auc&quot;) svm_classification_best_model ## # A tibble: 1 × 4 ## cost rbf_sigma margin .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.23 0.00686 -1.20 Preprocessor1_Model043 set.seed(1352) svm_classification_final_model &lt;- svm_class_workflow %&gt;% finalize_workflow(svm_classification_best_model) %&gt;% parsnip::fit(data = telco_train) Mediante este modelo final, se realizaron las predicciones de cancelación de servicios a clientes de telecomunicaciones. Para estos clientes se cuenta con la respuesta correcta debido a que se trata de los datos de testing. class_results &lt;- predict(svm_classification_final_model, telco_test, type = &quot;prob&quot;) %&gt;% bind_cols( Churn = telco_test$Churn, customerID = telco_test$customerID) %&gt;% relocate(customerID, .before = .pred_No) %&gt;% mutate(Churn = factor(Churn, levels = c(&#39;No&#39;, &#39;Yes&#39;), labels = c(&#39;No&#39;, &#39;Yes&#39;))) head(class_results, 10) ## # A tibble: 10 × 4 ## customerID .pred_No .pred_Yes Churn ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5575-GNVDE 0.854 0.146 No ## 2 9305-CDSKC 0.247 0.753 Yes ## 3 6713-OKOMC 0.822 0.178 No ## 4 7469-LKBCI 0.871 0.129 No ## 5 9959-WOFKT 0.923 0.0771 No ## 6 4183-MYFRB 0.547 0.453 No ## 7 1680-VDCWW 0.855 0.145 No ## 8 6322-HRPFA 0.920 0.0799 No ## 9 8665-UTDHZ 0.783 0.217 Yes ## 10 5248-YGIJN 0.950 0.0500 No Hasta este punto, únicamente se han calculado probabilidades y no se han tomado decisiones sobre la determinación de elementos a quienes se realizará alguna intervención de retención. roc_auc_value &lt;- roc_auc( class_results, truth = Churn, estimate = .pred_Yes, event_level = &quot;second&quot; ) pr_auc_value &lt;- pr_auc( class_results, truth = Churn, estimate = .pred_Yes, event_level = &quot;second&quot; ) roc_curve_data &lt;- roc_curve( class_results, truth = Churn, estimate = .pred_Yes, event_level = &#39;second&#39; ) roc_curve_plot &lt;- roc_curve_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(paste0(&quot;ROC Curve &quot;, &quot;(&quot;, round(roc_auc_value$.estimate, 2),&quot;)&quot;)) + theme_minimal() pr_curve_data &lt;- pr_curve( class_results, truth = Churn, estimate = .pred_Yes, event_level = &#39;second&#39; ) pr_curve_plot &lt;- pr_curve_data %&gt;% ggplot(aes(x = recall, y = precision)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + coord_equal() + ggtitle(paste0(&quot;Precision vs Recall &quot;, &quot;(&quot;, round(pr_auc_value$.estimate, 2),&quot;)&quot;)) + theme_minimal() roc_curve_plot + pr_curve_plot 8.2 Costo de retención Supongamos que existe un presupuesto designado a retener a los clientes de una compañía de telecomunicaciones que son áltamente probables de cancelar su servicio en los siguientes 3 meses. El presupuesto asignado es de $50,000.00 dólares y el equipo el marketing aún no se decide qué ofrecer: 1 mes gratis de servicio 1 celular gratis con valor de $100 dólares (para la empresa) ¿Cuántos clientes podrían ser intervenidos con cada uno de los posibles métodos? Considere los siguientes escenarios: Todas las retenciones son mediante meses gratis de servicio. Todas las retenciones son mediante el celular gratis. Las retenciones se realizan con 50% celular y 50% meses gratis. Costo de promoción de servicio El primer escenario considera que se obsequie un mes gratis de servicio al renovar un año completo. Para cada cliente se tiene en los datos el pago mensual que realizan, por lo que es posible priorizar de alguna manera a los clientes con mayor necesidad de retención promo_1 &lt;- class_results %&gt;% mutate( MonthlyCharges = telco_test$MonthlyCharges, Expected_Loss = .pred_Yes * MonthlyCharges) %&gt;% arrange(desc(Expected_Loss)) %&gt;% mutate(Budget = cumsum(MonthlyCharges)) %&gt;% filter(Budget &lt;= 50000) promo_1 %&gt;% select(-Churn, -.pred_No) ## # A tibble: 587 × 5 ## customerID .pred_Yes MonthlyCharges Expected_Loss Budget ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1400-MMYXY 0.964 106. 102. 106. ## 2 5419-CONWX 0.946 99.8 94.4 206. ## 3 2754-SDJRD 0.921 100. 92.2 306. ## 4 3902-FOIGH 0.905 101. 91.7 407. ## 5 6210-KBBPI 0.918 99.4 91.3 507. ## 6 4716-HHKQH 0.852 107. 91.2 614. ## 7 8510-AWCXC 0.895 101. 90.2 714. ## 8 2012-NWRPA 0.903 99.6 89.9 814. ## 9 9851-KIELU 0.816 110. 89.9 924. ## 10 9300-AGZNL 0.946 94 89.0 1018. ## # … with 577 more rows Sin acabarse el presupuesto de $50,000 dólares, el número de clientes a quienes es posible ofrecer la promoción es de 587/2113, lo que representa cerca del 28% de los clientes. promo_1 %&gt;% summarise( min_prob = min(.pred_Yes), mean_monthly_charge = mean(MonthlyCharges), sum_monthly_charge = sum(MonthlyCharges), sum_yearly_charge = sum(MonthlyCharges)*11 ) ## # A tibble: 1 × 4 ## min_prob mean_monthly_charge sum_monthly_charge sum_yearly_charge ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.217 85.1 49960. 549557. Al calcular el valor mínimo de probabilidad de cancelar el servicio, se observa que el umbral se encuentra cercano a 0.20, por lo que este es un buen candidato a usar en futuros meses si se usa este método y se cuenta con tal presupuesto. El gasto mensual promedio de los clientes es de $85 dólares. El costo total para la empresa es de $49,959.75 y el beneficio de invertir esta cantidad es de $549,557 dólares en un año. Este beneficio es el máximo a obtener si todos los clientes aceptaran renovar su suscripción. Será necesario realizar un experimento para comparar el beneficio neto cuando se implementa esta táctiva de marketing u alguna otra estrategia. Costo de promoción de producto El segundo método es más sencillo de calcular, pues supone un costo constante para cualquier cliente. Al regalar un producto en donde la empresa paga $100 dólares por celular, se logra ofrecer \\(50000/100 = 500\\) productos. promo_2 &lt;- class_results %&gt;% mutate( MonthlyCharges = telco_test$MonthlyCharges, Profit = 100, Expected_Loss = .pred_Yes * MonthlyCharges) %&gt;% arrange(desc(Expected_Loss)) %&gt;% mutate(Budget = cumsum(Profit)) %&gt;% filter(Budget &lt;= 50000) promo_2 %&gt;% select(-Churn, -.pred_No) ## # A tibble: 500 × 6 ## customerID .pred_Yes MonthlyCharges Profit Expected_Loss Budget ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1400-MMYXY 0.964 106. 100 102. 100 ## 2 5419-CONWX 0.946 99.8 100 94.4 200 ## 3 2754-SDJRD 0.921 100. 100 92.2 300 ## 4 3902-FOIGH 0.905 101. 100 91.7 400 ## 5 6210-KBBPI 0.918 99.4 100 91.3 500 ## 6 4716-HHKQH 0.852 107. 100 91.2 600 ## 7 8510-AWCXC 0.895 101. 100 90.2 700 ## 8 2012-NWRPA 0.903 99.6 100 89.9 800 ## 9 9851-KIELU 0.816 110. 100 89.9 900 ## 10 9300-AGZNL 0.946 94 100 89.0 1000 ## # … with 490 more rows promo_2 %&gt;% summarise( min_prob = min(.pred_Yes), mean_monthly_charge = mean(MonthlyCharges), sum_monthly_charge = sum(MonthlyCharges), sum_yearly_charge = sum(MonthlyCharges)*11 ) ## # A tibble: 1 × 4 ## min_prob mean_monthly_charge sum_monthly_charge sum_yearly_charge ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.283 86.1 43050. 473545. 8.2.1 Costo de promoción híbrida "],["references.html", "References", " References https://towardsdatascience.com/svm-and-kernel-svm-fed02bef1200 https://towardsdatascience.com/support-vector-machine-explained-8bfef2f17e71 https://medium.com/swlh/the-support-vector-machine-basic-concept-a5106bd3cc5f https://towardsdatascience.com/unlocking-the-true-power-of-support-vector-regression-847fd123a4a0#:~:text=Support%20Vector%20Regression%20is%20a,the%20maximum%20number%20of%20points. https://www.mygreatlearning.com/blog/introduction-to-support-vector-machine/ https://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
