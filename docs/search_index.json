[["index.html", "AMAT- Ciencia de Datos y Machine Learning 2 Capítulo 1 BIENVENIDA 1.1 Objetivo 1.2 Instructores 1.3 Ciencia de Datos en R 1.4 Estructura del curso actual 1.5 Duración y evaluación del curso 1.6 Recursos y dinámica de clase", " AMAT- Ciencia de Datos y Machine Learning 2 Karina Lizette Gamboa Puente Oscar Arturo Bringas López Capítulo 1 BIENVENIDA 1.1 Objetivo Desarrollar conocimiento y habilidades para implementar modelos complejos de Machine Learning a través de un flujo de trabajo limpio, ordenado y sistematizado a mediante las librerías en R más novedosas que han sido desarrolladas hasta el momento. Al finalizar este curso, el participante será capáz de combinar distintas clases de modelos para dar una solución compleja y precisa a problemas predictivos. Aprenderá a cuantificar los problemas éticos asociados al sesgo o inequidad producidos por modelos de machine learning, así como su interpretación en el mundo productivo. Finalmente, se estudiará la manera de desarrollar un diseño de experimento para implementarse en el ámbito empresarial de modo que el participante pueda tomar mejores decisiones para contribuir en su ambiente laboral. Se asume que el alumno tiene conocimientos generales de estadística, bases matemáticas y de programación básica en R y que cuenta con los conocimientos teóricos básicos de machine learning y prácticos con tidymodels. 1.2 Instructores ACT. ARTURO BRINGAS LinkedIn: arturo-bringas Email: act.arturo.b@ciencias.unam.mx Actuario, egresado de la Facultad de Ciencias y Maestría en Ciencia de Datos, ITAM. Experiencia en modelos predictivos y de clasificación de machine learning aplicado a seguros, deportes y movilidad internacional. Es jefe de departamento en Investigación Aplicada y Opinión de la UNAM, donde realiza estudios estadísticos de impacto social. Es consultor para empresas y organizaciones como GNP, El Universal, UNAM, Sinnia, la Organización de las Naciones Unidas Contra la Droga y el Delito (UNODC), entre otros. Actualmente es profesor de ciencia de datos y machine learning en AMAT y se desempeña como consultor independiente en diferentes proyectos contribuyendo a empresas en temas de machine learning, estadística, series de tiempo, visualización de datos y análisis geoespacial. ACT. KARINA LIZETTE GAMBOA LinkedIn: KaLizzyGam Email: lizzygamboa@ciencias.unam.mx Actuaria, egresada de la Facultad de Ciencias, UNAM, candidata a Maestra en Ciencia de Datos por el ITAM. Experiencia en áreas de analítica predictiva e inteligencia del negocio. Lead y Senior Data Scientist en consultoría en diferentes sectores como tecnología, asegurador, financiero y bancario. Experta en entendimiento de negocio para la correcta implementación de algoritmos de inteligencia y explotación de datos. Actualmente se desarrolla como Arquitecta de Soluciones Analíticas en Merama, startup mexicana clasificada como uno de los nuevos unicornios de Latinoamérica. Senior Data Science en CLOSTER y como profesora del diplomado de Metodología de la Investigación Social por la UNAM así como instructora de cursos de Ciencia de Datos en AMAT. Empresas anteriores: GNP, Activer Banco y Casa de Bolsa, PlayCity Casinos, RakenDataGroup Consulting, entre otros. 1.3 Ciencia de Datos en R 1.4 Estructura del curso actual 1.4.1 Alcances del curso Al finalizar el módulo, el participante sabrá plantear un proyecto de ciencia de datos, desde sus requerimientos hasta sus implementación comercial. Sabrá crear flujos de trabajo limpios y ordenados para crear poderosos modelos de Machine Learning. Podrá comparar múltiples modelos y seleccionar el que más aportación realice a su negocio considerando la ética alrededor del sesgo e inequidad producida por modelos. Profundizará su conocimiento en la interpretación de modelos complejos y aprenderá a cuantificar el beneficio comercial de la implementación de modelos. Requisitos: Computadora con al menos 4Gb Ram. Instalación de R con al menos versión 4.1.0 Instalación de Rstudio con al menos versión 1.4 Data Science &amp; Machine Learning (Aprendizaje Supervisado I) Temario: 1.- Machine Learning (10 HRS) Regresión polinomial Regresión con CPA Imputación SVM Boosting 2. Flujos de trabajo y ensamblajes (8 HRS) Pipelines Workflowsets Comparación de modelos Stacking 3. Sesgo e inequidad de modelos (4 HRS) Cuantificación de sesgo Cuantificación de inequidad 4. Interpretación de modelos (4 HRS) LIME DALExtra 5. Aplicación a negocios (6 HRS) Diseño de experimentos en campañas de retención Valuación de implementación de modelos 1.5 Duración y evaluación del curso El programa tiene una duración de 32 hrs. Las clases serán impartidas los días domingo, de 9:00 am a 1:00 pm Serán asignados ejercicios que el participante deberá resolver entre una semana y otra. Al final del curso se solicitará un proyecto final, el cual deberá ser entregado para ser acreedor a la constancia de participación. 1.6 Recursos y dinámica de clase En esta clase estaremos usando: R da click aquí si aún no lo descargas RStudio da click aquí también Zoom Clases Pulgar arriba: Voy bien, estoy entendiendo! Pulgar abajo: Eso no quedó muy claro Mano arriba: Quiero participar/preguntar ó Ya estoy listo para iniciar Grupo de WhatsApp El chismecito está aquí Google Drive Notas de clase Revisame si quieres aprender Finalmente, se dejarán ejercicios que serán clave para el éxito del aprendizaje de los capítulos, por lo que se trabajará en equipo para lograr adquirir el mayor aprendizaje. En este grupo se trabajará en equipos de 3 y 4 personas para enriquecer el aprendizaje y participación de todos. "],["repaso.html", "Capítulo 2 Repaso 2.1 Machine Learning 2.2 Tipos de aprendizaje 2.3 Errores: Sesgo vs varianza 2.4 Partición de datos 2.5 Recetas y tratamiento de datos", " Capítulo 2 Repaso En los cursos anteriores, hemos hablando a cerca del proceso completo de Ciencia de Datos, para poder empezar con la segunda parte del curso de Analisis Supervisado, valele la pena hacer un breve repaso de lo que hemos estudiado hasta el momento. 2.1 Machine Learning Machine Learning o –aprendizaje automático– es una rama de la inteligencia artificial que permite que las máquinas aprendan de los patrones existentes en los datos. Se usan métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. (Está enfocado en la programación de máquinas para aprender de los patrones existentes en datos principalmente estructurados y anticiparse al futuro) 2.2 Tipos de aprendizaje Platicamos en el módulo pasado que al hablar de Machine Learning, existen distintos tipos de aprendizaje, siendo los más comúnes: Aprendizaje supervisado Aprendizaje no supervisado Otreos ejemplos de especialidades son Aprendizaje profundo Aprendizaje por refuerzo La diferencia entre el análisis supervisado y el no supervisado es la etiqueta, es decir, en el análisis supervisado tenemos una etiqueta “correcta” y el objetivo de los algoritmos es predecir esta etiqueta. 2.2.1 Aprendizaje supervisado Conocemos la respuesta correcta de antemano. Esta respuesta correcta fue “etiquetada” por un humano (la mayoría de las veces, en algunas circunstancias puede ser generada por otro algoritmo). Debido a que conocemos la respuesta correcta, existen muchas métricas de desempeño del modelo para verificar que nuestro algoritmo está haciendo las cosas “bien.” 2.2.1.1 Tipos de aprendizaje supervisado (Regresión vs clasificación) Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Los algoritmos de clasificación se usan cuando el resultado deseado es una etiqueta discreta, es decir, clasifican un elemento dentro de diversas clases. En un problema de regresión, la variable target o variable a predecir es un valor numérico. 2.2.2 Aprendizaje no supervisado Aquí no tenemos la respuesta correcta de antemano ¿cómo podemos saber que el algoritmo está bien o mal? Estadísticamente podemos verificar que el algoritmo está bien Siempre tenemos que verificar con el cliente si los resultados que estamos obteniendo tienen sentido de negocio. Por ejemplo, número de grupos y características 2.3 Errores: Sesgo vs varianza En el mundo de Machine Learning cuando desarrollamos un modelo nos esforzamos para hacer que sea lo más preciso, ajustando los parámetros, pero la realidad es que no se puede construir un modelo 100% preciso ya que nunca pueden estar libres de errores. Comprender cómo las diferentes fuentes de error generan sesgo y varianza nos ayudará a mejorar el proceso de ajuste de datos, lo que resulta en modelos más precisos, adicionalmente también evitará el error de sobreajuste y falta de ajuste. Error por sesgo: Es la diferencia entre la predicción esperada de nuestro modelo y los valores verdaderos. Aunque al final nuestro objetivo es siempre construir modelos que puedan predecir datos muy cercanos a los valores verdaderos, no siempre es tan fácil porque algunos algoritmos son simplemente demasiado rígidos para aprender señales complejas del conjunto de datos. Imagina ajustar una regresión lineal a un conjunto de datos que tiene un patrón no lineal, no importa cuántas observaciones más recopiles, una regresión lineal no podrá modelar las curvas en esos datos. Esto se conoce como underfitting. Error por varianza: Se refiere a la cantidad que la estimación de la función objetivo cambiará si se utiliza diferentes datos de entrenamiento. La función objetivo se estima a partir de los datos de entrenamiento mediante un algoritmo de Machine Learning, por lo que deberíamos esperar que el algoritmo tenga alguna variación. Idealmente no debería cambiar demasiado de un conjunto de datos de entrenamiento a otro. Los algoritmos de Machine Learning que tienen una gran varianza están fuertemente influenciados por los detalles de los datos de entrenamiento, esto significa que los detalles de la capacitación influyen en el número y los tipos de parámetros utilizados para caracterizar la función de mapeo. Error irreducible: El error irreducible no se puede reducir, independientemente de qué algoritmo se usa. También se le conoce como ruido y, por lo general, proviene por factores como variables desconocidas que influyen en el mapeo de las variables de entrada a la variable de salida, un conjunto de características incompleto o un problema mal enmarcado. Acá es importante comprender que no importa cuán bueno hagamos nuestro modelo, nuestros datos tendrán cierta cantidad de ruido o un error irreductible que no se puede eliminar. 2.4 Partición de datos Cuando hay una gran cantidad de datos disponibles, una estrategia inteligente es asignar subconjuntos específicos de datos para diferentes tareas, en lugar de asignar la mayor cantidad posible solo a la estimación de los parámetros del modelo. Si el conjunto inicial de datos no es lo suficientemente grande, habrá cierta superposición de cómo y cuándo se asignan nuestros datos, y es importante contar con una metodología sólida para la partición de datos. 2.4.1 Métodos comunes para particionar datos El enfoque principal para la validación del modelo es dividir el conjunto de datos existente en dos conjuntos distintos: Entrenamiento: Este conjunto suele contener la mayoría de los datos, los cuales sirven para la construcción de modelos donde se pueden ajustar diferentes modelos, se investigan estrategias de ingeniería de características, etc. La mayor parte del proceso de modelado se utiliza este conjunto. Prueba: La otra parte de las observaciones se coloca en este conjunto. Estos datos se mantienen en reserva hasta que se elijan uno o dos modelos como los de mejor rendimiento. El conjunto de prueba se utiliza como árbitro final para determinar la eficiencia del modelo, por lo que es fundamental mirar el conjunto de prueba una sola vez. Supongamos que asignamos el \\(80\\%\\) de los datos al conjunto de entrenamiento y el \\(20\\%\\) restante a las pruebas. El método más común es utilizar un muestreo aleatorio simple. El paquete rsample tiene herramientas para realizar divisiones de datos como esta; la función initial_split() fue creada para este propósito. library(tidymodels) data(ames) tidymodels_prefer() # Fijar un número aleatorio con para que los resultados puedan ser reproducibles set.seed(123) # Partición 80/20 de los datos ames_split &lt;- initial_split(ames, prop = 0.80) ames_split ## &lt;Analysis/Assess/Total&gt; ## &lt;2344/586/2930&gt; La información impresa denota la cantidad de datos en el conjunto de entrenamiento \\((n = 2,344)\\), la cantidad en el conjunto de prueba \\((n = 586)\\) y el tamaño del grupo original de muestras \\((n = 2,930)\\). El objeto ames_split es un objeto rsplit y solo contiene la información de partición; para obtener los conjuntos de datos resultantes, aplicamos dos funciones más: ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) dim(ames_train) ## [1] 2344 74 No hay un porcentaje de división óptimo para el conjunto de entrenamiento y prueba. Los porcentajes de división más comunes comunes son: Entrenamiento: \\(80\\%\\), Prueba: \\(20\\%\\) Entrenamiento: \\(67\\%\\), Prueba: \\(33\\%\\) Entrenamiento: \\(50\\%\\), Prueba: \\(50\\%\\) 2.4.2 Conjunto de validación El conjunto de validación se definió originalmente cuando los investigadores se dieron cuenta de que medir el rendimiento del conjunto de entrenamiento conducía a resultados que eran demasiado optimistas. Esto llevó a modelos que se sobreajustaban, lo que significa que se desempeñaron muy bien en el conjunto de entrenamiento pero mal en el conjunto de prueba. Para combatir este problema, se retuvo un pequeño conjunto de datos de validación y se utilizó para medir el rendimiento del modelo mientras este está siendo entrenado. Una vez que la tasa de error del conjunto de validación comenzara a aumentar, la capacitación se detendría. En otras palabras, el conjunto de validación es un medio para tener una idea aproximada de qué tan bien se desempeñó el modelo antes del conjunto de prueba. Por otra parte esta primera particíón de datos evolucionó hasta la manera en que usualmente se hacen con más de una validación: 2.5 Recetas y tratamiento de datos La ingenería de datos y procesamiento de datos es parte vital del desarrollo de un buen modelo. En este curso analizaremos distintos métodos de machine learning que permitirán predecir una respuesta numérica o categórica. Usaremos el lenguaje de programación R para dicho procesamiento. Hay varios pasos que se deben de seguir para crear un modelo útil: Recopilación de datos. Limpieza de datos. Creación de nuevas variables. Estimación de parámetros. Selección y ajuste del modelo. Evaluación del rendimiento. Al comienzo de un proyecto, generalmente hay un conjunto finito de datos disponibles para todas estas tareas. OJO: A medida que los datos se reutilizan para múltiples tareas, aumentan los riesgos de agregar sesgos o grandes efectos de errores metodológicos. 2.5.1 Pre-procesamiento de datos Como punto de partida para nuestro flujo de trabajo de aprendizaje automático, necesitaremos datos de entrada. En la mayoría de los casos, estos datos se cargarán y almacenarán en forma de data frames o tibbles en R. Incluirán una o varias variables predictoras y, en caso de aprendizaje supervisado, también incluirán un resultado conocido. Sin embargo, no todos los modelos pueden lidiar con diferentes problemas de datos y, a menudo, necesitamos transformar los datos para obtener el mejor rendimiento posible del modelo. Este proceso se denomina pre-procesamiento y puede incluir una amplia gama de pasos, como: Dicotomización de variables Near Zero Value (nzv) o Varianza Cero Imputaciones Des-correlacionar Normalizar Transformar Creación de nuevas variables Interacciones En la tabla, \\(\\checkmark\\) indica que el método es obligatorio para el modelo y \\(\\times\\) indica que no lo es. El símbolo \\(\\circ\\) significa que la técnica puede ayudar al modelo, pero no es obligatorio. 2.5.2 Recetas Una receta es una serie de pasos o instrucciones para el procesamiento de datos. A diferencia del método de fórmula dentro de una función de modelado, la receta define los pasos sin ejecutarlos inmediatamente; es sólo una especificación de lo que se debe hacer. La estructura de una receta sigue los siguientes pasos: Inicialización recipe() Transformación step_[...]() Preparación prep() Aplicación bake(), juice() La siguiente sección explica la estructura y flujo de transformaciones: receta &lt;- recipe(response ~ X1 + X2 + X3 + ... + Xn, data = dataset ) %&gt;% step_1(...) %&gt;% step_2(...) %&gt;% step_3(...) %&gt;% ... step_4(...) %&gt;% prep() receta_aplicacion &lt;- bake(receta, new_data = new_dataset) juice(receta_aplicacion) 2.5.2.1 Transformaciones generales En cuanto a las transformaciones posibles, existe una gran cantidad de funciones que soportan este proceso. En esta sección se muestran algunas de las transformación más comunes y aquí La guía completa de las familia de funciones step : step_select(): Selecciona un subconjunto de variables específicas en el conjunto de datos. step_mutate(): Crea una nueva variable o modifica una existente usando dplyr::mutate(). step_mutate_at(): Lee una especificación de un paso de receta que modificará las variables seleccionadas usando una función común a través de dplyr::mutate_at(). step_filter(): Crea una especificación de un paso de receta que eliminará filas usando dplyr::filter(). step_arrange(): Ordena el conjunto de datos de acuerdo con una o más variables. step_rm(): Crea una especificación de un paso de receta que eliminará las variables según su nombre, tipo o función. step_nzv(): Realiza una selección de variables eliminando todas aquellas cuya varianza se encuentre cercana a cero. step_naomit(): Elimina todos los renglones que tengan alguna variable con valores perdidos. step_normalize(): Centra y escala las variables numéricas especificadas, generando una transformación a una distribución normal estándar. step_range(): Transforma el rango de un conjunto de variables numéricas al especificado. step_interact(): Crea un nuevo conjunto de variables basadas en la interacción entre dos variables. step_ratio(): Crea una nueva variable a partir del cociente entre dos variables. all_predictors(): Selecciona a todos los predictores del conjunto de entrenamineto para aplicarles alguna de las funciones mencionadas. all_numeric_predictors(): Selecciona a todos los predictores numéricos del conjunto de entrenamineto para aplicarles alguna de las funciones mencionadas. all_nominal_predictors(): Selecciona a todos los predictores nominales del conjunto de entrenamineto para aplicarles alguna de las funciones mencionadas. "],["feature-engineering.html", "Capítulo 3 Feature Engineering 3.1 Regresión polinomial 3.2 Análisis de Componentes Principales 3.3 Imputación KNN 3.4 Ejercicio", " Capítulo 3 Feature Engineering 3.1 Regresión polinomial Es común encontrar en la literatura este tema junto con la regresión múltiple, sin embargo, el alcance de esta transformación va a más allá de la regresión lineal, por lo que estudiaremos este tema como parte de la ingeniería de variables y no como un modelo lineal exclusivamente. El objetivo de la transformación polinomial dentro de la ingeniería de variables es crear nuevas variables que puedan explicar la relación entre la variable de respuesta y explicativa a través de un polinomio de grado k. En el siguiente gráfico se representa mediante una línea roja a la regresión lineal y mediante una curva azul al polinomio que relaciona a las variables independiente y dependiente. Es evidente que existe un mejor ajuste cuando se considera un polinomio de grado k en vez de la componente lineal. Este método ofrece mayor flexibilidad para el ajuste de un modelo. Será importante mediar posteriormente entre el sesgo y la varianza de forma que podamos tener un mejor ajuste sin caer en el sobreajuste. La fórmula que expresa la relación entre variable de respuesta y explicativas es ahora: \\[Y_i \\sim X_1 + X_1^2\\] Es importante mencionar que cuando se ajusta un modelo polinomial de segundo orden, se deben mantener ambas variables en el model (la original y la cuadrática). Cuando se tenga un modelo polinomial de grado k, se deberán conservar los k elementos que componen el polinomio: \\[Y\\sim X_1+X_1^2 + ... + X_1^k\\] Estas transformaciones son posibles realizarlas a múltiples variables que conforman el conjunto de datos que sirve de insumo para el modelo. A través de los pasos secuenciales en las recetas podemos integrar esta tarea a través de la función step_poly( ). Veamos un ejemplo: library(tidymodels) n = 100 set.seed(12345) original_data &lt;- tibble(x = seq(-10, 10, lengt=n)) %&gt;% mutate(y = x^3 + rnorm(n, mean = 0, sd = 100)) poly_recipe &lt;- recipe(y ~ x, data = original_data) %&gt;% step_poly(x, degree = 3) %&gt;% prep() juice(poly_recipe) ## # A tibble: 100 × 4 ## y x_poly_1 x_poly_2 x_poly_3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -941. -0.171 0.217 -0.249 ## 2 -870. -0.168 0.204 -0.219 ## 3 -895. -0.165 0.191 -0.190 ## 4 -874. -0.161 0.178 -0.163 ## 5 -716. -0.158 0.166 -0.137 ## 6 -908. -0.154 0.154 -0.113 ## 7 -616. -0.151 0.142 -0.0904 ## 8 -661. -0.147 0.131 -0.0690 ## 9 -618. -0.144 0.119 -0.0489 ## 10 -640. -0.140 0.108 -0.0302 ## # … with 90 more rows juice(poly_recipe) %&gt;% lm(y ~ x_poly_1 + x_poly_2 + x_poly_3, data = .) ## ## Call: ## lm(formula = y ~ x_poly_1 + x_poly_2 + x_poly_3, data = .) ## ## Coefficients: ## (Intercept) x_poly_1 x_poly_2 x_poly_3 ## 24.52 3643.95 -34.12 1541.84 juice(poly_recipe) %&gt;% mutate( x = seq(-10, 10, lengt=n), y_est = 24.52 + 3643.95 * x_poly_1 -34.12 * x_poly_2 + 1541.84 * x_poly_3) %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_smooth(aes(y = y_est)) + ggtitle(&quot;Regresión polinomial&quot;) En el ejemplo anterior puede mostrarse que es a través de la ponderación de los términos polinomiales que se logra estimar a la variable de respuesta Y. Este mismo procedimiento puede usarse en la receta de la ingeniería de datos para realizar la predicción de una variable de respuesta mediante cualquier otro algoritmo predictivo. 3.2 Análisis de Componentes Principales El análisis PCA (por sus siglas en inglés) es una técnica de reducción de dimensión útil tanto para el proceso de análisis exploratorio, el inferencial y predictivo. Es una técnica ampliamente usada en muchos estudios, pues permite sintetizar la información relevante y desechar aquello que no aporta tanto. Es particularmente útil en el caso de conjuntos de datos “amplios” en donde las variables están correlacionadas entre sí y donde se tienen muchas variables para cada observación. En los conjuntos de datos donde hay muchas variables presentes, no es fácil trazar los datos en su formato original, lo que dificulta tener una idea de las tendencias presentes en ellos. PCA permite ver la estructura general de los datos, identificando qué observaciones son similares entre sí y cuáles son diferentes. Esto puede permitirnos identificar grupos de muestras que son similares y determinar qué variables hacen a un grupo diferente de otro. La idea detrás de esta técnica es la siguiente: Se desean crear nuevas variables llamadas Componentes Principales, las cuales son creadas como combinación lineal (transformación lineal) de las variables originales, por lo que cada una de las variables nuevas contiene parcialmente información de todas las variables originales. \\[Z_1 = a_{11}X_1 +a_{12}X_2 + ... + a_{1p}X_p\\] \\[Z_2 = a_{21}X_1 +a_{22}X_2 + ... + a_{2p}X_p\\] \\[...\\] \\[Z_p = a_{p1}X_1 +a_{p2}X_2 + ... + a_{pp}X_p\\] Se desea que la primer componente principal capture la mayor varianza posible de todo el conjunto de datos. \\[\\forall i \\in 2,...,p \\quad Var(Z_1)&gt;Var(Z_i)\\] La segunda componente principal deberá SER INDEPENDIENTE de la primera y deberá abarcar la mayor varianza posible del restante. Esta condición se debe cumplir para toda componente i, de tal forma que las nuevas componentes creadas son independientes entre sí y acumulan la mayor proporción de varianza en las primeras de ellas, dejando la mínima proporción de varianza a las últimas componentes. \\[Z_1 \\perp\\!\\!\\!\\perp Z_2 \\quad \\&amp; \\quad Var(Z_1)&gt;Var(Z_2)&gt;Var(Z_i)\\] El punto anterior permite desechar unas cuantas componentes (las últimas) sin perder mucha varianza. ¡¡ RECORDAR !! A través de CPA se logra retener la mayor cantidad de varianza útil pero usando menos componentes que el número de variables originales. Para que este proceso sea efectivo, debe existir ALTA correlación entre las variables originales. Cuando muchas variables se correlacionan entre sí, todas contribuirán fuertemente al mismo componente principal. Cada componente principal suma un cierto porcentaje de la variación total en el conjunto de datos. Cuando sus variables iniciales estén fuertemente correlacionadas entre sí y podrá aproximar la mayor parte de la complejidad de su conjunto de datos con solo unos pocos componentes principales. Agregar componentes adicionales hace que la estimación del conjunto de datos total sea más precisa, pero también más difícil de manejar. 3.2.1 Eigenvalores y eigenvectores Los vectores propios y los valores propios vienen en pares: cada vector propio tiene un valor propio correspondiente. Los vectores propios son la ponderación que permite crear la combinación lineal de las variables para conformar cada componente principal, mientras que el valor propio es la varianza asociada a cada componente principal. Desde un punto de vista geométrico, el eigenvector es la dirección del vector determinado por la componente principal y el eigenvalor es la magnitud de dicho vector. El valor propio de una componente es la varianza de este. La suma acumulada de los primeros \\(j\\) eigenvalores representa la varianza acumulada de las primeras \\(j\\) componentes principales El número de valores propios y vectores propios que existe es igual al número de dimensiones que tiene el conjunto de datos. 3.2.2 Implementación en R library(sf) library(magrittr) library(tidymodels) indice_marg &lt;- st_read(&#39;data/IMEF_2010.dbf&#39;, quiet = TRUE) glimpse(indice_marg) ## Rows: 32 ## Columns: 16 ## $ CVE_ENT &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;1… ## $ AÑO &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20… ## $ POB_TOT &lt;int&gt; 1184996, 3155070, 637026, 822441, 2748391, 650555, 4796580, 34… ## $ ANALF &lt;dbl&gt; 3.274040, 2.600783, 3.234464, 8.370643, 2.645050, 5.157943, 17… ## $ SPRIM &lt;dbl&gt; 14.754823, 12.987567, 14.273833, 22.541207, 12.168029, 18.4761… ## $ OVSDE &lt;dbl&gt; 1.0649743, 0.4322072, 0.9436751, 6.4196750, 1.0916308, 0.68577… ## $ OVSEE &lt;dbl&gt; 0.62347891, 0.94517891, 2.84464884, 2.59080046, 0.53707721, 0.… ## $ OVSAE &lt;dbl&gt; 0.9854257, 3.5616214, 7.0865085, 9.7378176, 1.3908497, 1.17060… ## $ VHAC &lt;dbl&gt; 30.33066, 29.05839, 31.73806, 45.96720, 30.26891, 31.32052, 53… ## $ OVPT &lt;dbl&gt; 1.761813, 3.398537, 5.814081, 4.500699, 1.423701, 4.691477, 15… ## $ PL_5000 &lt;dbl&gt; 25.1626166, 10.3491523, 15.6188287, 30.8755279, 12.1486353, 14… ## $ PO2SM &lt;dbl&gt; 33.64880, 21.86970, 23.29986, 45.51076, 30.04270, 32.04402, 69… ## $ IM &lt;dbl&gt; -0.91086057, -1.14014880, -0.68128749, 0.43357139, -1.14000448… ## $ GM &lt;chr&gt; &quot;Bajo&quot;, &quot;Muy bajo&quot;, &quot;Bajo&quot;, &quot;Alto&quot;, &quot;Muy bajo&quot;, &quot;Bajo&quot;, &quot;Muy a… ## $ LUGAR &lt;int&gt; 28, 30, 23, 10, 29, 26, 2, 21, 32, 15, 14, 1, 6, 27, 22, 8, 19… ## $ NOM_ENT &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Baja California&quot;, &quot;Baja California Sur&quot;, &quot;C… indice_marg %&gt;% dplyr::count(GM, sort = TRUE) ## GM n ## 1 Medio 9 ## 2 Alto 8 ## 3 Bajo 8 ## 4 Muy bajo 4 ## 5 Muy alto 3 pca_recipe &lt;- recipe(IM ~ ., data = indice_marg) %&gt;% update_role(NOM_ENT, GM, new_role = &quot;id&quot;) %&gt;% step_normalize(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% step_pca(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM, num_comp=9, res=&quot;res&quot;) %&gt;% step_rm(LUGAR, AÑO, POB_TOT) %&gt;% prep() juice(pca_recipe) ## # A tibble: 32 × 13 ## CVE_ENT GM NOM_ENT IM PC1 PC2 PC3 PC4 PC5 PC6 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Bajo Aguasc… -0.911 -2.34 -0.227 0.372 0.492 0.264 0.0764 ## 2 02 Muy b… Baja C… -1.14 -2.93 0.595 -0.0597 -0.492 0.291 -0.0508 ## 3 03 Bajo Baja C… -0.681 -1.75 1.37 -0.683 -0.400 -0.304 0.160 ## 4 04 Alto Campec… 0.434 1.12 -0.819 -0.151 -0.271 -0.929 0.178 ## 5 05 Muy b… Coahui… -1.14 -2.93 -0.144 0.157 -0.133 0.0419 -0.0786 ## 6 06 Bajo Colima -0.779 -2.00 0.0316 0.552 -0.136 0.320 -0.729 ## 7 07 Muy a… Chiapas 2.32 5.96 0.132 1.36 -0.0122 -0.673 -0.471 ## 8 08 Bajo Chihua… -0.520 -1.34 1.05 -1.27 0.633 -0.646 -0.387 ## 9 09 Muy b… Distri… -1.48 -3.81 0.110 0.159 -0.453 0.205 -0.355 ## 10 10 Medio Durango 0.0525 0.135 0.675 -1.50 0.929 -0.448 0.146 ## # … with 22 more rows, and 3 more variables: PC7 &lt;dbl&gt;, PC8 &lt;dbl&gt;, PC9 &lt;dbl&gt; Veamos los pasos de esta receta: Primero, debemos decirle a la receta qué datos se usan para predecir la variable de respuesta. Se actualiza el rol de las variables nombre de entidad y grado de marginación con la función NOM_ENT, ya que es una variable que queremos mantener por conveniencia como identificador de filas, pero no son un predictor ni variable de respuesta. Necesitamos centrar y escalar los predictores numéricos, porque estamos a punto de implementar PCA. Finalmente, usamos step_pca() para realizar el análisis de componentes principales. La función prep() es la que realiza toda la preparación de la receta. Una vez que hayamos hecho eso, podremos explorar los resultados del PCA. Comencemos por ver cómo resultó el PCA. Podemos ordenar los resultados mediante la función tidy(), incluido el paso de PCA, que es el segundo paso. Luego hagamos una visualización para ver cómo se ven los componentes. A continuación se muestran la desviación estándar, porcentaje de varianza y porcentaje de varianza acumulada que aporta cada componente principal. summary(pca_recipe$steps[[2]]$res) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.572 0.82085 0.7920 0.64640 0.52101 0.44069 0.31797 ## Proportion of Variance 0.735 0.07487 0.0697 0.04643 0.03016 0.02158 0.01123 ## Cumulative Proportion 0.735 0.80990 0.8796 0.92603 0.95619 0.97777 0.98900 ## PC8 PC9 ## Standard deviation 0.25660 0.18201 ## Proportion of Variance 0.00732 0.00368 ## Cumulative Proportion 0.99632 1.00000 Podemos observar que en la primera componente principal, las \\(9\\) variables que utilizó el Consejo Nacional de Población para obtener el Índice de Marginación 2010 aportan de manera positiva en el primer componente principal. library(tidytext) tidied_pca &lt;- tidy(pca_recipe, 2) tidied_pca %&gt;% filter(component %in% paste0(&quot;PC&quot;, 1:4)) %&gt;% group_by(component) %&gt;% top_n(9, abs(value)) %&gt;% ungroup() %&gt;% mutate(terms = reorder_within(terms, abs(value), component)) %&gt;% ggplot(aes(abs(value), terms, fill = value &gt; 0)) + geom_col() + facet_wrap(~component, scales = &quot;free_y&quot;) + scale_y_reordered() + labs( x = &quot;Absolute value of contribution&quot;, y = NULL, fill = &quot;Positive?&quot; )+ theme_minimal() Notamos que las \\(9\\) variables aportan entre el \\(25\\%\\) y el \\(35\\%\\) a la primera componente principal. 3.2.3 Representación gráfica library(ggrepel) juice(pca_recipe) %&gt;% mutate(GM = factor(GM, levels = c(&quot;Muy alto&quot;, &quot;Alto&quot;, &quot;Medio&quot;, &quot;Bajo&quot;, &quot;Muy bajo&quot;)), ordered = T) %&gt;% ggplot(aes(PC1, PC2, label = NOM_ENT)) + geom_point(aes(color = GM), alpha = 0.7, size = 2) + geom_text_repel() + ggtitle(&quot;Grado de marginación de entidades&quot;) Finalmente, podemos observar como (de izquierda a derecha) los estados con grado de marginación Muy bajo, Bajo, Medio, Alto y Muy Alto respectivamente. juice(pca_recipe) %&gt;% ggplot(aes(x = IM, y = PC1)) + geom_smooth(method = &quot;lm&quot;) + geom_point(size = 2) + ggtitle(&quot;Comparación: Índice Marginación Vs PCA CP1&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.2.4 ¿Cuántas componentes retener? Existe en la literatura basta información sobre el número de componentes a retener en un análisis de PCA. El siguiente gráfico lleva por nombre gráfico de codo y muestra el porcentaje de varianza explicado por cada componente principal. library(factoextra) library(FactoMineR) res.pca &lt;- indice_marg %&gt;% select(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% as.data.frame() %&gt;% set_rownames(indice_marg$NOM_ENT) %&gt;% PCA(graph=FALSE) fviz_eig(res.pca, addlabels=TRUE, ylim=c(0, 100)) El grafico anterior muestra que hay una diferencia muy grande entre la varianza retenida por la 1er componente principal y el resto de las variables. Dependiendo del objetivo del analisis podra elegirse el numero adecuado de componentes a retener, no obstante, la literatura sugiere retener 1 o 2 componentes principales. Regresando al tema de feature engineering, es posible realizar el proceso de componentes principales y elegir una de las dos opciones siguientes: Especificar el número de componentes a retener Indicar el porcentaje de varianza a alcanzar La segunda opción elegirá tantas componentes como sean necesarias hasta alcanzar el hiperparámetro mínimo indicado. A continuación se ejemplifica: Caso 1: pca_recipe &lt;- recipe(IM ~ ., data = indice_marg) %&gt;% update_role(NOM_ENT, GM, new_role = &quot;id&quot;) %&gt;% step_normalize(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% step_pca(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM,num_comp=2) %&gt;% step_rm(LUGAR, AÑO, POB_TOT) %&gt;% prep() juice(pca_recipe) ## # A tibble: 32 × 6 ## CVE_ENT GM NOM_ENT IM PC1 PC2 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Bajo Aguascalientes -0.911 -2.34 -0.227 ## 2 02 Muy bajo Baja California -1.14 -2.93 0.595 ## 3 03 Bajo Baja California Sur -0.681 -1.75 1.37 ## 4 04 Alto Campeche 0.434 1.12 -0.819 ## 5 05 Muy bajo Coahuila de Zaragoza -1.14 -2.93 -0.144 ## 6 06 Bajo Colima -0.779 -2.00 0.0316 ## 7 07 Muy alto Chiapas 2.32 5.96 0.132 ## 8 08 Bajo Chihuahua -0.520 -1.34 1.05 ## 9 09 Muy bajo Distrito Federal -1.48 -3.81 0.110 ## 10 10 Medio Durango 0.0525 0.135 0.675 ## # … with 22 more rows Caso 2: pca_recipe &lt;- recipe(IM ~ ., data = indice_marg) %&gt;% update_role(NOM_ENT, GM, new_role = &quot;id&quot;) %&gt;% step_normalize(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% step_pca(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM,threshold=0.90) %&gt;% step_rm(LUGAR, AÑO, POB_TOT) %&gt;% prep() juice(pca_recipe) ## # A tibble: 32 × 8 ## CVE_ENT GM NOM_ENT IM PC1 PC2 PC3 PC4 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Bajo Aguascalientes -0.911 -2.34 -0.227 0.372 0.492 ## 2 02 Muy bajo Baja California -1.14 -2.93 0.595 -0.0597 -0.492 ## 3 03 Bajo Baja California Sur -0.681 -1.75 1.37 -0.683 -0.400 ## 4 04 Alto Campeche 0.434 1.12 -0.819 -0.151 -0.271 ## 5 05 Muy bajo Coahuila de Zaragoza -1.14 -2.93 -0.144 0.157 -0.133 ## 6 06 Bajo Colima -0.779 -2.00 0.0316 0.552 -0.136 ## 7 07 Muy alto Chiapas 2.32 5.96 0.132 1.36 -0.0122 ## 8 08 Bajo Chihuahua -0.520 -1.34 1.05 -1.27 0.633 ## 9 09 Muy bajo Distrito Federal -1.48 -3.81 0.110 0.159 -0.453 ## 10 10 Medio Durango 0.0525 0.135 0.675 -1.50 0.929 ## # … with 22 more rows Así es como usaremos el análisis de componentes principales para mejorar la estructura de variables que sirven de input para cualquiera de los modelos posteriores. Continuaremos con un paso más de pre-procesamiento antes de comenzar a aprender nuevos modelos. 3.3 Imputación KNN Antes de aprender el uso de la función de imputación, recordaremos brevemente como funciona el algoritmo de K-Nearest-Neighbor (KNN) KNN es un algoritmo de aprendizaje supervisado que podemos usar tanto para regresión como clasificación. Es un algoritmo fácil de interpretar y que permite ser flexible en el balance entre sesgo y varianza (dependiendo de los hiper-parámetros seleccionados). El algoritmo de K vecinos más cercanos realiza comparaciones entre un nuevo elemento y las observaciones anteriores que ya cuentan con etiqueta. La esencia de este algoritmo está en etiquetar a un nuevo elemento de manera similar a como están etiquetados aquellos K elementos que más se le parecen. Veremos este proceso para cada uno de los posibles casos: 3.3.1 Ventajas y limitaciones del Clasificador KNN Ventajas: KNN no hace ninguna suposición subyacente sobre los datos. Con la adición de más puntos de datos, el clasificador evoluciona constantemente y es capaz de adaptarse rápidamente a los cambios en el conjunto de datos de entrada. Le da al usuario la flexibilidad de elegir la métrica de medida de distancia. Desventajas: KNN es muy sensible a los valores atípicos. No funciona con datos faltantes. A medida que crece el conjunto de datos, la clasificación se vuelve más lenta. Existe la llamada maldición de la dimensionalidad. Este algoritmo es altamente usado para imputación de datos faltantes, ¿tiene lógica, cierto?, con recipes podemos aplicar un paso con la función: step_impute_knn antes llamada step_knnimpute(), podemos observar la documentación de la función en el siguiente enlace. 3.3.2 Implementación en R Veámos cómo implementar la imputación por KNN para datos faltantes en R: step_impute_knn( recipe, ..., ### Variables a imputar neighbors = 5, impute_with = imp_vars(all_predictors()), id = rand_id(&quot;impute_knn&quot;) ) Acerca de los parámetros: neighbors: Número de vecinos impute_with: Una llamada a imp_vars para especificar qué variables se usan para imputar las variables. Si una columna se incluye en ambas listas para ser imputada y para ser un predictor de imputación, se eliminará de esta última y no se usará para imputarse a sí misma. id: Una cadena de caracteres que es exclusiva de este paso para identificarlo. La función utiliza el conjunto de entrenamiento para imputar cualquier otro conjunto de datos. La única función de distancia disponible es la distancia de Gower, que se puede utilizar para combinaciones de datos nominales y numéricos. Acerca de Gower El coeficiente de similitud de Gower propuesto en 1971 permite la manipulación simultánea de variables cuantitativas y cualitativas en una base de datos, mediante la aplicación de este coeficiente se logra hallar la similitud entre individuos a los cuales se les han medido una serie de características en común. Una similaridad alta, es decir cercana a 1, indicara gran homogeneidad entre los individuos; por el contrario, una similaridad cercana a cero indica que los individuos son diferentes Vamos a utilizar los datos de biomass de la libreria modeldata que contiene un conjunto de datos donde diferentes combustibles de biomasa se caracterizan por la cantidad de ciertas moléculas (carbono, hidrógeno, oxígeno, nitrógeno y azufre) y el poder calorífico superior correspondiente (HHV). En esta base hemos retirado valores aleatoriamente sobre dos variables para realizar el ejercicio. library(recipes) library(modeldata) library(DataExplorer) data(biomass) biomass_te_whole &lt;- as_tibble(biomass) # induce some missing data at random set.seed(19735) carb_missing &lt;- sample(1:nrow(biomass_te_whole), 75) nitro_missing &lt;- sample(1:nrow(biomass_te_whole), 75) biomass_te_whole$carbon[carb_missing] &lt;- NA biomass_te_whole$nitrogen[nitro_missing] &lt;- NA biomass_te_whole[&quot;carb_imputed&quot;] &lt;- &quot;No&quot; biomass_te_whole$carb_imputed[carb_missing] &lt;- &quot;Yes&quot; biomass_te_whole[&quot;nitro_imputed&quot;] &lt;- &quot;No&quot; biomass_te_whole$nitro_imputed[nitro_missing] &lt;- &quot;Yes&quot; biomass_tr &lt;- biomass_te_whole %&gt;% filter( dataset == &#39;Training&#39;) biomass_te &lt;- biomass_te_whole %&gt;% filter( dataset == &quot;Testing&quot;) biomass_te_whole[nitro_missing,] ## # A tibble: 75 × 10 ## sample dataset carbon hydrogen oxygen nitrogen sulfur HHV carb_imputed ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Red Alder … Traini… 49.6 6.06 43.8 NA 0.07 19.3 No ## 2 Cereals, H… Traini… 44.8 5 42.5 NA 0.13 18.4 No ## 3 Sewage Slu… Traini… 28.3 4.07 17.5 NA 1.25 12.1 No ## 4 Palm Fibre Testing 47.5 6.01 36.4 NA 0.3 19.2 No ## 5 Rice Straw Traini… 35.7 4.62 39.1 NA 0 14.8 No ## 6 Prune Pits Traini… 49.3 6.59 41.8 NA 0.07 20.1 No ## 7 Ryegrass S… Traini… 46.7 5.8 41.9 NA 0.2 18.5 No ## 8 Peanut Hul… Traini… 45.8 5.46 39.6 NA 0.12 18.6 No ## 9 Cereals, H… Traini… NA 4.9 43.1 NA 0.12 18.2 Yes ## 10 Erco Char Traini… 65.9 2.57 12.8 NA 0.1 24.2 No ## # … with 65 more rows, and 1 more variable: nitro_imputed &lt;chr&gt; biomass_tr %&gt;% DataExplorer::plot_missing( title = &quot;Train&quot; ) biomass_te %&gt;% DataExplorer::plot_missing( title = &quot;Test&quot; ) En la primera opción vamos a aplicar el paso de immputación indicando qué columnas quieren ser imputadas y con cuáles variables queremos que se haga el proceso. recipe_esp &lt;- recipe( HHV ~ carbon + hydrogen + oxygen + nitrogen + sulfur, data = biomass_tr) %&gt;% step_impute_knn( carbon, nitrogen, impute_with= imp_vars(hydrogen, oxygen), neighbors = 3) %&gt;% prep() imputed_esp_train &lt;- bake(recipe_esp, biomass_tr) imputed_esp_test &lt;- bake(recipe_esp, biomass_te) ## Probando la receta con un test imputed_esp_train %&gt;% DataExplorer::plot_missing( title = &quot;Imputacion Train&quot;) imputed_esp_test %&gt;% DataExplorer::plot_missing(title = &quot;Imputacion Test&quot;) Sin embargo, siempre podemos pedirle al modelo que haga la imputación de todas las variables que tengan nulos, con toda la información de las demás variables disponibles (no nulas). recipe &lt;- recipe( HHV ~ carbon + hydrogen + oxygen + nitrogen + sulfur, data = biomass_tr) %&gt;% step_impute_knn(all_predictors(), neighbors = 3) %&gt;% prep() imputed &lt;- bake(recipe, biomass_te) %&gt;% bind_cols(biomass_te %&gt;% select(carb_imputed, nitro_imputed)) # prueba con test imputed %&gt;% DataExplorer::plot_missing() 3.4 Ejercicio Cada equipo estará a cargo de desarrollar una receta de feature engineering utilizando los pasos vistos en el curso pasado y el actual. Tendrán 15 días para probar distintas estrategias que mejoren las predicciones del precio de ventas. Se deberá entregar y explicar el código creado por el equipo. Este código servirá para los ejercicios de optimización de modelos en los siguientes capítulos. "],["support-vector-machine-svm-svr.html", "Capítulo 4 Support Vector Machine (SVM / SVR) 4.1 Maximum Margin Classifier 4.2 Support Vector Classifiers 4.3 Support Vector Machine 4.4 El truco del Kernel 4.5 Support Vector Regression 4.6 Ventajas y desventajas 4.7 Ajuste del modelo con R", " Capítulo 4 Support Vector Machine (SVM / SVR) Es común encontrar en la literatura el nombre de SVM para referirse tanto al caso de regresión como al de clasificación, no obstante, SVR se refiere particularmente a Suport Vector Regression. Support vector machine, llamado SVM, es un algoritmo de aprendizaje supervisado que se puede utilizar para problemas de clasificación y regresión. Se utiliza para conjuntos de datos más pequeños, ya que tarda demasiado en procesarse. El principal objetivo de esta técnica es encontrar el Hiperplano de Separación Óptima, también conocido como Boundary Decision, el cual será el margen de clasificación más grande que podamos ajustar para separar a las clases involucradas, limitando las veces que una observación viola dicho margen. Para entender este algoritmo es necesario entender 3 conceptos principales: Maximum margin classifiers Support vector classifiers Support vector machines Estudiemos cada uno de estos principios. 4.1 Maximum Margin Classifier A menudo se generalizan con máquinas de vectores de soporte, pero SVM tiene muchos más parámetros en comparación. El clasificador de margen máximo considera un hiperplano con ancho de separación máxima para clasificar los datos. Sin embargo, se pueden dibujar infinitos hiperplanos en un conjunto de datos por lo que es importante elegir el hiperplano ideal para la clasificación. En un espacio n-dimensional, un hiperplano es un subespacio de la dimensión n-1. Es decir, si los datos tienen un espacio bidimensional, entonces el hiperplano puede ser una línea recta que divide el espacio de datos en dos mitades y pasa por la siguiente ecuacion: \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2=0\\] Las observaciones que caen en el hiperplano sigue la ecuación anterior. Las observaciones que caen en la región por encima o por debajo del hiperplano sigue las siguientes ecuaciones: \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2&gt;0\\] \\[\\beta_0 + \\beta_1X_1 + \\beta_2X_2&lt;0\\] El clasificador de margen máximo a menudo falla en la situación de casos no separables en los que no puede asignar un hiperplano diferente para clasificar datos no separables. Para tales casos, un clasificador de vectores de soporte viene al rescate. Del diagrama anterior, podemos suponer infinitos hiperplanos (izquierda). El clasificador de margen máximo viene con un solo hiperplano que divide los datos como en la gráfica de la derecha. Los datos que tocan los hiperplanos positivo y negativo se denominan vectores de soporte. 4.2 Support Vector Classifiers Los vectores de soporte son las observaciones que están más cerca del hiperplano e influyen en la posición y orientación del hiperplano. Este tipo de clasificador puede considerarse como una versión extendida del clasificador de margen máximo. Cuando tratamos con datos de la vida real, encontramos que la mayoría de las observaciones están en clases superpuestas. Es por eso que se implementan clasificadores de vectores de soporte. Usando estos vectores de soporte, maximizamos el margen del clasificador. Eliminar los vectores de soporte cambiará la posición del hiperplano. Estos son los puntos que nos ayudan a construir nuestro SVM. Consideremos un parámetro de ajuste C. Entendamos con el siguiente diagrama. Podemos ver en el gráfico de la izquierda que los valores más altos de C generaron más errores que se consideran una violación o infracción. El diagrama de la derecha muestra un valor más bajo de C y no brinda suficientes posibilidades de infracción al reducir el ancho del margen. Puede considerarse al parámetro C como el monto de regularización, tal que: Si C es bajo, el margen será más amplio y tendremos un mayor número de violaciones al margen, pero el modelo generalizará mejor Si C es alto, nuestro margen será menos amplio y tendrá menos violaciones. Sin embargo, no generalizará bien. Este modelo es sensible a cambios en la escala de datos de entrada, por lo que será importante estandarizar las variables antes de usar este modelo. 4.3 Support Vector Machine El enfoque de la máquina de vectores de soporte se considera durante una decisión no lineal y los datos no son separables por un clasificador de vectores de soporte, independientemente de la función de costo. Cuando es casi imposible separar clases de manera no lineal, aplicamos el truco llamado truco del kernel el cual ayuda a manejar la separación de los datos. En el gráfico anterior, los datos que eran inseparables en una dimensión se separaron una vez que se transformaron a un espacio de dos dimensiones después de aplicar una transformación mediante kernel polinomial de segundo grado. Ahora veamos cómo manejar los datos bidimensionales linealmente inseparables. En datos bidimensionales, el núcleo polinomial de segundo grado se aplica utilizando un plano lineal después de transformarlo a dimensiones superiores. 4.4 El truco del Kernel Las funciones Kernel son métodos con los que se utilizan clasificadores lineales como SVM para clasificar puntos de datos separables no linealmente. Esto se hace representando los puntos de datos en un espacio de mayor dimensión que su original. Por ejemplo, los datos 1D se pueden representar como datos 2D en el espacio, los datos 2D se pueden representar como datos 3D, etcétera. El truco del kernel ofrece una forma de calcular las relaciones entre los puntos de datos utilizando funciones del kernel y representar los datos de una manera más eficiente con menos cómputo. Los modelos que utilizan esta técnica se denominan “modelos kernelizados”. Hay varias funciones que utiliza SVM para realizar esta tarea. Algunos de los más comunes son: El núcleo lineal: Se utiliza para datos lineales. Esto simplemente representa los puntos de datos usando una relación lineal. \\[K(x, y)=(x^T \\cdot y)\\] \\[f(x)=w^T \\cdot x + b\\] Esta formulación se presenta como solución al problema de optimización sobre w: \\[min_{w\\in R^d} \\frac{1}{2}\\parallel w \\parallel ^2+ C\\sum_{i}^{N}{max(0, 1-y_i f(x_i))}\\] \\[s.a. \\quad y_i f(x_i) \\geq 1 - max(0, 1-y_i f(x_i))\\] En donde \\(1-y_i f(x_i)\\) es la distancia de \\(x_i\\) al correspondiente margen de la clase si \\(x_i\\) se encuentra en el lado equivocado del margen y cero en caso contrario. De esta forma, los puntos que se encuentran lejos del margen del lado equivocado obtendrán una mayor penalización. Dar click en la siguiente liga para mayor entendimiento del problema de optimización. Función de núcleo polinomial: Transforma los puntos de datos mediante el uso del producto escalar y la transformación de los datos en una “dimensión n,” n podría ser cualquier valor de 2, 3, etcétera, es decir, la transformación será un producto al cuadrado o superior. Por lo tanto, representar datos en un espacio de mayor dimensión utilizando los nuevos puntos transformados. \\[K(x, y)=(c+ x^T \\cdot y)^p\\] Cuando se emplea \\(p=1\\) y \\(c=0\\), el resultado es el mismo que el de un kernel lineal. Si \\(p&gt;1\\), se generan límites de decisión no lineales, aumentando la no linealidad a medida que aumenta p. No suele ser recomendable emplear valores de p mayores 5 por problemas de overfitting. La función de base radial (RBF): Esta función se comporta como un “modelo de vecino más cercano ponderado.” Transforma los datos representándolos en dimensiones infinitas, La función Radial puede ser de Gauss o de Laplace. Esto depende de un hiperparámetro conocido como gamma \\(\\gamma\\). Cuanto menor sea el valor del hiperparámetro, menor será el sesgo y mayor la varianza. Mientras que un valor más alto de hiperparámetro da un sesgo más alto y menor varianza. Este es el núcleo más utilizado. \\[K(x, y)=exp(-\\gamma \\parallel x - y\\parallel^2)=exp(-\\frac{\\parallel x-y \\parallel ^2}{2\\sigma²})\\] \\[f(x)=w^T \\cdot \\phi(x) + b\\] Se realiza un mapeo de x a \\(\\phi(x)\\) en donde los datos son separables Es recomendable probar el kernel RBF. Este kernel tiene dos ventajas: que solo tiene dos hiperparámetros que optimizar (\\(\\gamma\\) y la penalización \\(C\\) común a todos los SVM) y que su flexibilidad puede ir desde un clasificador lineal a uno muy complejo. La función sigmoide: También conocida como función tangente hiperbólica (Tanh), encuentra más aplicación en redes neuronales como función de activación. Esta función mapea los valores de entrada al intervalo [-1, 1]. \\[K(x, y)= tanh(\\kappa x\\cdot y-\\delta)\\] ¿Por qué se llama un “truco del kernel?” SVM vuelve a representar los puntos de datos no lineales utilizando cualquiera de las funciones del kernel de una manera que parece que los datos se han transformado, luego encuentra el hiperplano de separación óptimo, sin embargo, en realidad, los puntos de datos siguen siendo los mismos, en realidad no se han transformado. Es por eso que se llama un ‘truco del kernel.’ 4.5 Support Vector Regression El problema de la regresión es encontrar una función que aproxime la relación de un dominio de datos de entrada a números reales con base en una muestra de entrenamiento. Veamos cómo funciona SVR en realidad. Consideremos las dos líneas rojas como el límite de decisión y la línea verde como el hiperplano. Nuestro objetivo, cuando avanzamos con SVR, es básicamente considerar los puntos que están dentro de la línea límite de decisión. Nuestra línea de mejor ajuste es el hiperplano que tiene un número máximo de puntos. Lo primero que entenderemos será el límite de decisión. Consideremos estas líneas como si estuvieran a cualquier distancia, digamos ‘a,’ del hiperplano. Entonces, estas son las líneas que dibujamos a la distancia ‘+a’ y ‘-a’ del hiperplano. Esta ‘a’ en el texto se conoce básicamente como épsilon y representa el margen. Suponiendo que la ecuación del hiperplano es la siguiente: \\[Y_i = W^TX + b\\] Entonces estas ecuaciones se transforman en la siguiente forma: \\(W^TX + b = +a\\) \\(W^TX + b = -a\\) Por lo tanto, cualquier hiperplano que satisfaga nuestra SVR debería satisfacer: \\(-a &lt; Y- WX+b &lt; +a\\) Nuestro objetivo principal aquí es decidir un límite de decisión a una distancia ‘a’ del hiperplano original, de modo que los puntos de datos más cercanos al hiperplano o los vectores de soporte estén dentro de esa línea límite. Vamos a tomar solo aquellos puntos que están dentro del límite de decisión y tienen la menor tasa de error, o están dentro del margen de tolerancia. Esto nos da un mejor modelo de ajuste. 4.6 Ventajas y desventajas Ventajas Es un modelo que ajusta bien con pocos datos Son flexibles en datos no estructurados, estructurados y semiestructurados. La función Kernel alivia las complejidades en casi cualquier tipo de datos. Se observa menos sobreajuste en comparación con otros modelos. Desventajas El tiempo de entrenamiento es mayor cuando se calculan grandes conjuntos de datos. Los hiperparámetros suelen ser un desafío al interpretar su impacto. La interpretación general es difícil (black box). 4.7 Ajuste del modelo con R Usaremos las recetas antes implementadas para ajustar tanto el modelo de regresión como el de clasificación. Exploraremos un conjunto de hiperparámetros para elegir el mejor modelo. Recordemos los pasos a seguir al ajustar un modelo Separación inicial de datos ( test, train ) Pre-procesamiento e ingeniería de variables Selección de tipo de modelo con hiperparámetros iniciales Inicialización de workflow o pipeline Creación de grid search Entrenamiento de modelos con hiperparámetros definidos (salvar los modelos entrenados) Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) Selección de modelo a usar Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) Validar poder predictivo con datos de prueba. 4.7.1 Implementación de SVR en R A continuación, revisaremos paso por paso este procedimiento usando SVM como modelo. Los datos corresponden a nuestro ya conocido problema predictivo de precio de casas. Se puede encontrar los datos y documentación en el siguiente enlace Paso 1: Separación inicial de datos ( test, train ) library(tidymodels) data(ames) set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) ames_folds &lt;- vfold_cv(ames_train) Contando con datos de entrenamiento, procedemos a realizar el feature engineering para extraer las mejores características que permitirán realizar las estimaciones en el modelo. Paso 2: Pre-procesamiento e ingeniería de variables receta_casas &lt;- recipe(Sale_Price ~ . , data = ames_train) %&gt;% step_unknown(Alley) %&gt;% step_rename(Year_Remod = Year_Remod_Add) %&gt;% step_rename(ThirdSsn_Porch = Three_season_porch) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod, TotalSF = Gr_Liv_Area + Total_Bsmt_SF, AvgRoomSF = Gr_Liv_Area / TotRms_AbvGrd, Pool = if_else(Pool_Area &gt; 0, 1, 0), Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_predictors(), -all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~ Second_Flr_SF:First_Flr_SF) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% step_rm( First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath, Bsmt_Half_Bath, Kitchen_AbvGr, BsmtFin_Type_1_Unf, Total_Bsmt_SF, Kitchen_AbvGr, Pool_Area, Gr_Liv_Area, Sale_Type_Oth, Sale_Type_VWD ) %&gt;% prep() receta_casas ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 73 ## ## Training data contained 2197 data points and no missing data. ## ## Operations: ## ## Unknown factor level assignment for Alley [trained] ## Variable renaming for Year_Remod [trained] ## Variable renaming for ThirdSsn_Porch [trained] ## Ratios from Bedroom_AbvGr, Gr_Liv_Area [trained] ## Variable mutation for ~Year_Sold - Year_Remod, ~Gr_Liv_Area + To... [trained] ## Re-order factor level to ref_level for Exter_Cond [trained] ## Centering and scaling for Lot_Frontage, Lot_Area, Year_Built, Year_Remod,... [trained] ## Dummy variables from MS_SubClass, MS_Zoning, Street, Alley, Lot_Shape, Land_Co... [trained] ## Interactions with Second_Flr_SF:First_Flr_SF [trained] ## Interactions with (Bsmt_Cond_Fair + Bsmt_Cond_Good + Bsmt_Cond_No_Ba... [trained] ## Variables removed First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath... [trained] Recordemos que la función recipe() solo son los pasos a seguir, necesitamos usar la función prep() que nos devuelve una receta actualizada con las estimaciones y la función juice() que nos devuelve la matriz de diseño. Una vez que la receta de transformación de datos está lista, procedemos a implementar el pipeline del modelo de interés. Existen diversas funciones dentro de tidymodels para implementar estos modelos, entra las cuales se encuentran: Base lineal: svm_lineal() Base polinomial: svm_poly() Base radial: svm_rbf() Paso 3: Selección de tipo de modelo con hiperparámetros iniciales svm_model &lt;- svm_rbf( mode = &quot;regression&quot;, cost = tune(), rbf_sigma = tune(), margin = tune()) %&gt;% set_engine(&quot;kernlab&quot;) svm_model ## Radial Basis Function Support Vector Machine Specification (regression) ## ## Main Arguments: ## cost = tune() ## rbf_sigma = tune() ## margin = tune() ## ## Computational engine: kernlab Paso 4: Inicialización de workflow o pipeline svm_workflow &lt;- workflow() %&gt;% add_recipe(receta_casas) %&gt;% add_model(svm_model) svm_workflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: svm_rbf() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 11 Recipe Steps ## ## • step_unknown() ## • step_rename() ## • step_rename() ## • step_ratio() ## • step_mutate() ## • step_relevel() ## • step_normalize() ## • step_dummy() ## • step_interact() ## • step_interact() ## • ... ## • and 1 more step. ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Radial Basis Function Support Vector Machine Specification (regression) ## ## Main Arguments: ## cost = tune() ## rbf_sigma = tune() ## margin = tune() ## ## Computational engine: kernlab Paso 5: Creación de grid search svm_parameters_set &lt;- svm_workflow %&gt;% hardhat::extract_parameter_set_dials() %&gt;% update( cost = cost(c(0, 0.5)), rbf_sigma = rbf_sigma(c(-3, 3)), margin = svm_margin(c(-2, 2)) ) set.seed(123) svm_grid &lt;- svm_parameters_set %&gt;% grid_max_entropy(size = 100) svm_grid ## # A tibble: 100 × 3 ## cost rbf_sigma margin ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.22 0.504 0.571 ## 2 1.41 1.22 0.329 ## 3 1.33 0.111 1.38 ## 4 1.35 0.0935 -1.65 ## 5 1.04 0.937 1.48 ## 6 1.14 0.0154 -1.91 ## 7 1.39 70.2 -1.98 ## 8 1.07 1.16 -0.487 ## 9 1.17 1.34 0.948 ## 10 1.02 0.0303 0.230 ## # … with 90 more rows ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) Paso 6: Entrenamiento de modelos con hiperparámetros definidos library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) svm1 &lt;- Sys.time() svm_tune_result &lt;- tune_grid( svm_workflow, resamples = ames_folds, grid = svm_grid, metrics = metric_set(rmse, mae, mape), control = ctrl_grid ) svm2 &lt;- Sys.time(); svm2 - svm1 stopCluster(cluster) svm_tune_result %&gt;% saveRDS(&quot;models/svm_model_reg.rds&quot;) Podemos obtener las métricas de cada fold con el siguiente código: svm_tune_result &lt;- readRDS(&quot;models/svm_model_reg.rds&quot;) unnest(svm_tune_result, .metrics) ## # A tibble: 3,000 × 11 ## splits id cost rbf_sigma margin .metric .estimator .estimate ## &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &lt;split [1977/220]&gt; Fold01 1.04 0.00515 0.543 rmse standard 41851. ## 2 &lt;split [1977/220]&gt; Fold01 1.04 0.00515 0.543 mae standard 29737. ## 3 &lt;split [1977/220]&gt; Fold01 1.04 0.00515 0.543 mape standard 19.8 ## 4 &lt;split [1977/220]&gt; Fold01 1.12 688. 1.55 rmse standard 98784. ## 5 &lt;split [1977/220]&gt; Fold01 1.12 688. 1.55 mae standard 86126. ## 6 &lt;split [1977/220]&gt; Fold01 1.12 688. 1.55 mape standard 63.4 ## 7 &lt;split [1977/220]&gt; Fold01 1.15 0.0145 -0.643 rmse standard 41706. ## 8 &lt;split [1977/220]&gt; Fold01 1.15 0.0145 -0.643 mae standard 28971. ## 9 &lt;split [1977/220]&gt; Fold01 1.15 0.0145 -0.643 mape standard 19.7 ## 10 &lt;split [1977/220]&gt; Fold01 1.20 103. -1.51 rmse standard 98356. ## # … with 2,990 more rows, and 3 more variables: .config &lt;chr&gt;, .notes &lt;list&gt;, ## # .predictions &lt;list&gt; Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) collect_metrics(svm_tune_result) ## # A tibble: 300 × 9 ## cost rbf_sigma margin .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.04 0.00515 0.543 mae standard 27905. 10 604. Preprocesso… ## 2 1.04 0.00515 0.543 mape standard 17.2 10 0.579 Preprocesso… ## 3 1.04 0.00515 0.543 rmse standard 39638. 10 1268. Preprocesso… ## 4 1.12 688. 1.55 mae standard 79477. 10 903. Preprocesso… ## 5 1.12 688. 1.55 mape standard 59.3 10 1.23 Preprocesso… ## 6 1.12 688. 1.55 rmse standard 93686. 10 1127. Preprocesso… ## 7 1.15 0.0145 -0.643 mae standard 27913. 10 657. Preprocesso… ## 8 1.15 0.0145 -0.643 mape standard 17.7 10 0.600 Preprocesso… ## 9 1.15 0.0145 -0.643 rmse standard 39121. 10 1424. Preprocesso… ## 10 1.20 103. -1.51 mae standard 78820. 10 912. Preprocesso… ## # … with 290 more rows En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos: svm_tune_result %&gt;% autoplot() svm_tune_result %&gt;% show_best(n = 10, metric = &quot;mape&quot;) ## # A tibble: 10 × 9 ## cost rbf_sigma margin .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.24 0.0175 0.226 mape standard 14.3 10 0.558 Preprocessor1… ## 2 1.13 0.0211 0.319 mape standard 14.8 10 0.569 Preprocessor1… ## 3 1.38 0.0308 0.320 mape standard 14.9 10 0.574 Preprocessor1… ## 4 1.13 0.00549 -0.0512 mape standard 14.9 10 0.584 Preprocessor1… ## 5 1.04 0.00398 -0.0321 mape standard 15.3 10 0.611 Preprocessor1… ## 6 1.10 0.00303 0.0327 mape standard 15.5 10 0.623 Preprocessor1… ## 7 1.17 0.00383 0.353 mape standard 16.1 10 0.615 Preprocessor1… ## 8 1.06 0.00165 0.0561 mape standard 16.2 10 0.628 Preprocessor1… ## 9 1.28 0.00380 -0.388 mape standard 16.2 10 0.609 Preprocessor1… ## 10 1.36 0.00299 0.409 mape standard 16.6 10 0.619 Preprocessor1… Paso 8: Selección de modelo a usar # Selección del mejor modelo según la métrica MAPE svm_regression_best_model &lt;- select_best(svm_tune_result, metric = &quot;mape&quot;) svm_regression_best_model ## # A tibble: 1 × 4 ## cost rbf_sigma margin .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.24 0.0175 0.226 Preprocessor1_Model075 # Selección del modelo más regularizado a menos de una desviación estandar, según la métrica MAPE svm_regression_best_1se_model &lt;- svm_tune_result %&gt;% select_by_one_std_err(metric = &quot;mape&quot;, &quot;mape&quot;) svm_regression_best_1se_model ## # A tibble: 1 × 11 ## cost rbf_sigma margin .metric .estimator mean n std_err .config .best ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.13 0.0211 0.319 mape standard 14.8 10 0.569 Preproces… 14.3 ## # … with 1 more variable: .bound &lt;dbl&gt; Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) # Modelo final svm_regression_final_model &lt;- svm_workflow %&gt;% finalize_workflow(svm_regression_best_1se_model) %&gt;% parsnip::fit(data = ames_train) svm_regression_final_model ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: svm_rbf() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 11 Recipe Steps ## ## • step_unknown() ## • step_rename() ## • step_rename() ## • step_ratio() ## • step_mutate() ## • step_relevel() ## • step_normalize() ## • step_dummy() ## • step_interact() ## • step_interact() ## • ... ## • and 1 more step. ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: eps-svr (regression) ## parameter : epsilon = 0.318837530910969 cost C = 1.1300031747186 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 0.0210599956279017 ## ## Number of Support Vectors : 709 ## ## Objective Function Value : -221.4799 ## Training error : 0.149207 Como hemos hablado anteriormente, este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo library(vip) ames_importance &lt;- svm_regression_final_model %&gt;% extract_fit_parsnip() %&gt;% vi( method = &quot;permute&quot;, nsim = 10, target = &quot;Sale_Price&quot;, metric = &quot;rmse&quot;, pred_wrapper = kernlab::predict, train = juice(receta_casas) ) ames_importance %&gt;% saveRDS(&quot;models/vip_ames_svm.rds&quot;) ames_importance &lt;- readRDS(&quot;models/vip_telco_svm.rds&quot;) ames_importance ## # A tibble: 34 × 3 ## Variable Importance StDev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TotalCharges 0.0637 0.00487 ## 2 MonthlyCharges 0.0539 0.00983 ## 3 SeniorCitizen 0.0219 0.00515 ## 4 gender_Male 0 0 ## 5 Partner_Yes 0 0 ## 6 Dependents_Yes 0 0 ## 7 tenure_X1.2.years 0 0 ## 8 tenure_X2.3.years 0 0 ## 9 tenure_X3.4.years 0 0 ## 10 tenure_X4.5.years 0 0 ## # … with 24 more rows ames_importance %&gt;% mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% slice_max(Importance, n = 20) %&gt;% ggplot(aes(Importance, Variable, color = Variable)) + geom_errorbar(aes(xmin = Importance - StDev, xmax = Importance + StDev), alpha = 0.5, size = 1.3) + geom_point(size = 3) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Variable Importance Measure&quot;) Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: # Predicciones results &lt;- predict(svm_regression_final_model, ames_test) %&gt;% dplyr::bind_cols(truth = ames_test$Sale_Price) %&gt;% dplyr::rename(pred_svm_reg = .pred, Sale_Price = truth) head(results) ## # A tibble: 6 × 2 ## pred_svm_reg Sale_Price ## &lt;dbl&gt; &lt;int&gt; ## 1 143363. 105000 ## 2 183725. 185000 ## 3 179908. 180400 ## 4 127895. 141000 ## 5 217114. 210000 ## 6 196308. 216000 results %&gt;% yardstick::metrics(Sale_Price, pred_svm_reg) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 36200. ## 2 rsq standard 0.798 ## 3 mae standard 24681. Es posible definir nuestro propio conjunto de metricas que deseamos reportar creando este objeto: multi_metric &lt;- metric_set(rmse, rsq, mae, mape, ccc) multi_metric(results, truth = Sale_Price, estimate = pred_svm_reg) %&gt;% mutate(.estimate = round(.estimate, 2)) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 36200. ## 2 rsq standard 0.8 ## 3 mae standard 24681. ## 4 mape standard 14.8 ## 5 ccc standard 0.89 results %&gt;% ggplot(aes(x = pred_svm_reg, y = Sale_Price)) + geom_point() + geom_abline(color = &quot;red&quot;) + xlab(&quot;Prediction&quot;) + ylab(&quot;Observation&quot;) + ggtitle(&quot;Comparisson&quot;) 4.7.2 Implementación de SVM en R Es turno de revisar la implementación de SVM con nuestro bien conocido problema de predicción de cancelación de servicios de telecomunicaciones. Los datos se encuentran disponibles en el siguiente enlace: Los pasos para implementar en R este modelo predictivo son los mismos, cambiando únicamente las especificaciones del tipo de modelo, pre-procesamiento e hiper-parámetros. library(tidyverse) library(tidymodels) library(readr) telco &lt;- read_csv(&quot;data/Churn.csv&quot;) glimpse(telco) ## Rows: 7,043 ## Columns: 21 ## $ customerID &lt;chr&gt; &quot;7590-VHVEG&quot;, &quot;5575-GNVDE&quot;, &quot;3668-QPYBK&quot;, &quot;7795-CFOCW… ## $ gender &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;,… ## $ SeniorCitizen &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Partner &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Dependents &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;… ## $ tenure &lt;dbl&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 58, 49, 2… ## $ PhoneService &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ MultipleLines &lt;chr&gt; &quot;No phone service&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No phone service&quot;, &quot;… ## $ InternetService &lt;chr&gt; &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;Fiber optic&quot;, &quot;Fiber opt… ## $ OnlineSecurity &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;… ## $ OnlineBackup &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;N… ## $ DeviceProtection &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… ## $ TechSupport &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ StreamingTV &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Ye… ## $ StreamingMovies &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Contract &lt;chr&gt; &quot;Month-to-month&quot;, &quot;One year&quot;, &quot;Month-to-month&quot;, &quot;One … ## $ PaperlessBilling &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ PaymentMethod &lt;chr&gt; &quot;Electronic check&quot;, &quot;Mailed check&quot;, &quot;Mailed check&quot;, &quot;… ## $ MonthlyCharges &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, 29.7… ## $ TotalCharges &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820.50, 1949… ## $ Churn &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… Paso 1: Separación inicial de datos ( test, train ) set.seed(1234) telco_split &lt;- initial_split(telco, prop = .70) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) telco_folds &lt;- vfold_cv(telco_train) telco_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [4437/493]&gt; Fold01 ## 2 &lt;split [4437/493]&gt; Fold02 ## 3 &lt;split [4437/493]&gt; Fold03 ## 4 &lt;split [4437/493]&gt; Fold04 ## 5 &lt;split [4437/493]&gt; Fold05 ## 6 &lt;split [4437/493]&gt; Fold06 ## 7 &lt;split [4437/493]&gt; Fold07 ## 8 &lt;split [4437/493]&gt; Fold08 ## 9 &lt;split [4437/493]&gt; Fold09 ## 10 &lt;split [4437/493]&gt; Fold10 Paso 2: Pre-procesamiento e ingeniería de variables binner &lt;- function(x) { x &lt;- cut(x, breaks = c(0, 12, 24, 36,48,60,72), include.lowest = TRUE) as.numeric(x) } telco_rec &lt;- recipe(Churn ~ ., data = telco_train) %&gt;% update_role(customerID, new_role = &quot;id variable&quot;) %&gt;% step_num2factor( tenure, transform = binner, levels = c(&quot;0-1 year&quot;, &quot;1-2 years&quot;, &quot;2-3 years&quot;, &quot;3-4 years&quot;, &quot;4-5 years&quot;, &quot;5-6 years&quot;)) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_rm(customerID, skip=T) %&gt;% prep() telco_rec ## Recipe ## ## Inputs: ## ## role #variables ## id variable 1 ## outcome 1 ## predictor 19 ## ## Training data contained 4930 data points and 10 incomplete rows. ## ## Operations: ## ## Factor variables from tenure [trained] ## Centering and scaling for SeniorCitizen, MonthlyCharges, TotalCharges [trained] ## Dummy variables from gender, Partner, Dependents, tenure, PhoneService, Multip... [trained] ## Median imputation for SeniorCitizen, MonthlyCharges, TotalCharges, ge... [trained] ## Variables removed customerID [trained] Paso 3: Selección de tipo de modelo con hiperparámetros iniciales svm_class_model &lt;- svm_rbf( mode = &quot;classification&quot;, cost = tune(), rbf_sigma = tune(), margin = tune()) %&gt;% set_engine(&quot;kernlab&quot;) svm_class_model ## Radial Basis Function Support Vector Machine Specification (classification) ## ## Main Arguments: ## cost = tune() ## rbf_sigma = tune() ## margin = tune() ## ## Computational engine: kernlab Paso 4: Inicialización de workflow o pipeline svm_class_workflow &lt;- workflow() %&gt;% add_recipe(telco_rec) %&gt;% add_model(svm_class_model) svm_class_workflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: svm_rbf() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 5 Recipe Steps ## ## • step_num2factor() ## • step_normalize() ## • step_dummy() ## • step_impute_median() ## • step_rm() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Radial Basis Function Support Vector Machine Specification (classification) ## ## Main Arguments: ## cost = tune() ## rbf_sigma = tune() ## margin = tune() ## ## Computational engine: kernlab Paso 5: Creación de grid search svm_class_parameters_set &lt;- svm_class_workflow %&gt;% hardhat::extract_parameter_set_dials() %&gt;% update( cost = cost(c(0, 0.5)), rbf_sigma = rbf_sigma(c(-3, 3)), margin = svm_margin(c(-2, 2)) ) set.seed(123) svm_class_grid &lt;- svm_class_parameters_set %&gt;% grid_max_entropy(size = 100) svm_class_grid ## # A tibble: 100 × 3 ## cost rbf_sigma margin ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.22 0.504 0.571 ## 2 1.41 1.22 0.329 ## 3 1.33 0.111 1.38 ## 4 1.35 0.0935 -1.65 ## 5 1.04 0.937 1.48 ## 6 1.14 0.0154 -1.91 ## 7 1.39 70.2 -1.98 ## 8 1.07 1.16 -0.487 ## 9 1.17 1.34 0.948 ## 10 1.02 0.0303 0.230 ## # … with 90 more rows ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) Paso 6: Entrenamiento de modelos con hiperparámetros definidos library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) svm1 &lt;- Sys.time() svm_tune_class_result &lt;- tune_grid( svm_class_workflow, resamples = telco_folds, grid = svm_class_grid, metrics = metric_set(roc_auc, pr_auc), control = ctrl_grid ) svm2 &lt;- Sys.time(); svm2 - svm1 stopCluster(cluster) svm_tune_class_result %&gt;% saveRDS(&quot;models/svm_model_class.rds&quot;) svm_tune_class_result &lt;- readRDS(&quot;models/svm_model_class.rds&quot;) unnest(svm_tune_class_result, .metrics) ## # A tibble: 1,966 × 11 ## splits id cost rbf_sigma margin .metric .estimator .estimate ## &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &lt;split [4437/493]&gt; Fold01 1.22 0.504 0.571 roc_auc binary 0.760 ## 2 &lt;split [4437/493]&gt; Fold01 1.22 0.504 0.571 pr_auc binary 0.854 ## 3 &lt;split [4437/493]&gt; Fold01 1.41 1.22 0.329 roc_auc binary 0.735 ## 4 &lt;split [4437/493]&gt; Fold01 1.41 1.22 0.329 pr_auc binary 0.851 ## 5 &lt;split [4437/493]&gt; Fold01 1.33 0.111 1.38 roc_auc binary 0.756 ## 6 &lt;split [4437/493]&gt; Fold01 1.33 0.111 1.38 pr_auc binary 0.859 ## 7 &lt;split [4437/493]&gt; Fold01 1.35 0.0935 -1.65 roc_auc binary 0.762 ## 8 &lt;split [4437/493]&gt; Fold01 1.35 0.0935 -1.65 pr_auc binary 0.866 ## 9 &lt;split [4437/493]&gt; Fold01 1.04 0.937 1.48 roc_auc binary 0.751 ## 10 &lt;split [4437/493]&gt; Fold01 1.04 0.937 1.48 pr_auc binary 0.859 ## # … with 1,956 more rows, and 3 more variables: .config &lt;chr&gt;, .notes &lt;list&gt;, ## # .predictions &lt;list&gt; Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) collect_metrics(svm_tune_class_result) ## # A tibble: 200 × 9 ## cost rbf_sigma margin .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.22 0.504 0.571 pr_auc binary 0.867 10 0.00769 Preprocessor1_… ## 2 1.22 0.504 0.571 roc_auc binary 0.767 10 0.00534 Preprocessor1_… ## 3 1.41 1.22 0.329 pr_auc binary 0.868 10 0.00710 Preprocessor1_… ## 4 1.41 1.22 0.329 roc_auc binary 0.764 10 0.00778 Preprocessor1_… ## 5 1.33 0.111 1.38 pr_auc binary 0.878 10 0.00588 Preprocessor1_… ## 6 1.33 0.111 1.38 roc_auc binary 0.776 10 0.00505 Preprocessor1_… ## 7 1.35 0.0935 -1.65 pr_auc binary 0.880 10 0.00569 Preprocessor1_… ## 8 1.35 0.0935 -1.65 roc_auc binary 0.779 10 0.00499 Preprocessor1_… ## 9 1.04 0.937 1.48 pr_auc binary 0.871 10 0.00637 Preprocessor1_… ## 10 1.04 0.937 1.48 roc_auc binary 0.769 10 0.00654 Preprocessor1_… ## # … with 190 more rows En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos. svm_tune_class_result %&gt;% autoplot() svm_tune_class_result %&gt;% show_best(n = 10, metric = &quot;roc_auc&quot;) ## # A tibble: 10 × 9 ## cost rbf_sigma margin .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.23 0.00686 -1.20 roc_auc binary 0.808 7 0.00593 Preprocessor1_… ## 2 1.02 0.0303 0.230 roc_auc binary 0.805 9 0.00529 Preprocessor1_… ## 3 1.05 0.00823 0.904 roc_auc binary 0.805 9 0.00624 Preprocessor1_… ## 4 1.14 0.0215 1.11 roc_auc binary 0.804 10 0.00505 Preprocessor1_… ## 5 1.14 0.0154 -1.91 roc_auc binary 0.803 8 0.00625 Preprocessor1_… ## 6 1.19 0.0306 -1.45 roc_auc binary 0.803 10 0.00517 Preprocessor1_… ## 7 1.06 0.0350 -1.54 roc_auc binary 0.803 10 0.00517 Preprocessor1_… ## 8 1.22 0.0360 0.552 roc_auc binary 0.803 10 0.00517 Preprocessor1_… ## 9 1.36 0.0368 0.596 roc_auc binary 0.803 10 0.00515 Preprocessor1_… ## 10 1.11 0.00676 -1.42 roc_auc binary 0.802 9 0.00670 Preprocessor1_… Paso 8: Selección de modelo a usar # Selección del mejor modelo según la métrica ROC AUC svm_classification_best_model &lt;- select_best(svm_tune_class_result, metric = &quot;roc_auc&quot;) svm_classification_best_model ## # A tibble: 1 × 4 ## cost rbf_sigma margin .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1.23 0.00686 -1.20 Preprocessor1_Model043 # Selección del modelo más regularizado a menos de una desviación estandar, según la métrica ROC AUC svm_classification_best_1se_model &lt;- svm_tune_class_result %&gt;% select_by_one_std_err(metric = &quot;roc_auc&quot;, &quot;roc_auc&quot;) svm_classification_best_1se_model ## # A tibble: 1 × 11 ## cost rbf_sigma margin .metric .estimator mean n std_err .config .best ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1.14 0.0154 -1.91 roc_auc binary 0.803 8 0.00625 Preproces… 0.808 ## # … with 1 more variable: .bound &lt;dbl&gt; Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) # Modelo final set.seed(1352) svm_classification_final_model &lt;- svm_class_workflow %&gt;% finalize_workflow(svm_classification_best_model) %&gt;% parsnip::fit(data = telco_train) svm_classification_final_model ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: svm_rbf() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 5 Recipe Steps ## ## • step_num2factor() ## • step_normalize() ## • step_dummy() ## • step_impute_median() ## • step_rm() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1.22744145128527 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 0.00685742851317484 ## ## Number of Support Vectors : 2580 ## ## Objective Function Value : -3090.757 ## Training error : 0.223327 ## Probability model included. Como hemos hablado anteriormente, este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo. churn_importance &lt;- svm_classification_final_model %&gt;% extract_fit_parsnip() %&gt;% vi( method = &quot;permute&quot;, nsim = 10, target = &quot;Churn&quot;, metric = &quot;auc&quot;, reference_class = &quot;Yes&quot;, pred_wrapper = kernlab::predict, train = juice(telco_rec) ) churn_importance %&gt;% saveRDS(&quot;models/vip_telco_svm.rds&quot;) churn_importance &lt;- readRDS(&quot;models/vip_telco_svm.rds&quot;) churn_importance ## # A tibble: 34 × 3 ## Variable Importance StDev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TotalCharges 0.0637 0.00487 ## 2 MonthlyCharges 0.0539 0.00983 ## 3 SeniorCitizen 0.0219 0.00515 ## 4 gender_Male 0 0 ## 5 Partner_Yes 0 0 ## 6 Dependents_Yes 0 0 ## 7 tenure_X1.2.years 0 0 ## 8 tenure_X2.3.years 0 0 ## 9 tenure_X3.4.years 0 0 ## 10 tenure_X4.5.years 0 0 ## # … with 24 more rows churn_importance %&gt;% mutate(Variable = fct_reorder(Variable, Importance)) %&gt;% ggplot(aes(Importance, Variable, color = Variable)) + geom_errorbar(aes(xmin = Importance - StDev, xmax = Importance + StDev), alpha = 0.5, size = 1.3) + geom_point(size = 3) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Variable Importance Measure&quot;) Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: # Predicciones telco_test &lt;- testing(telco_split) results_cla &lt;- predict(svm_classification_final_model, telco_test, type = &quot;prob&quot;) %&gt;% dplyr::bind_cols(truth = telco_test$Churn) %&gt;% mutate(truth = factor(truth, levels = c(&#39;No&#39;, &#39;Yes&#39;), labels = c(&#39;No&#39;, &#39;Yes&#39;))) head(results_cla) ## # A tibble: 6 × 3 ## .pred_No .pred_Yes truth ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.854 0.146 No ## 2 0.247 0.753 Yes ## 3 0.822 0.178 No ## 4 0.871 0.129 No ## 5 0.923 0.0771 No ## 6 0.547 0.453 No roc_curve_data &lt;- roc_curve( results_cla, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39; ) roc_curve_data ## # A tibble: 2,083 × 3 ## .threshold specificity sensitivity ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -Inf 0 1 ## 2 0.0331 0 1 ## 3 0.0339 0.000646 1 ## 4 0.0339 0.00129 1 ## 5 0.0340 0.00194 1 ## 6 0.0348 0.00259 1 ## 7 0.0349 0.00323 1 ## 8 0.0356 0.00323 0.998 ## 9 0.0357 0.00388 0.998 ## 10 0.0361 0.00452 0.998 ## # … with 2,073 more rows roc_curve_plot &lt;- roc_curve_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;)+ theme_minimal() roc_curve_plot pr_curve_data &lt;- pr_curve( results_cla, truth = truth, estimate = .pred_Yes, event_level = &#39;second&#39; ) pr_curve_data ## # A tibble: 2,082 × 3 ## .threshold recall precision ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Inf 0 1 ## 2 0.964 0.00177 1 ## 3 0.946 0.00353 1 ## 4 0.946 0.00530 1 ## 5 0.929 0.00707 1 ## 6 0.921 0.00707 0.8 ## 7 0.918 0.00883 0.833 ## 8 0.918 0.0106 0.857 ## 9 0.915 0.0124 0.875 ## 10 0.908 0.0141 0.889 ## # … with 2,072 more rows pr_curve_plot &lt;- pr_curve_data %&gt;% ggplot(aes(x = recall, y = precision)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;)+ theme_minimal() pr_curve_plot Pueden usar la app de shiny que nos permite jugar con el treshold de clasificación para tomar la mejor decisión. "],["references.html", "References", " References https://towardsdatascience.com/svm-and-kernel-svm-fed02bef1200 https://towardsdatascience.com/support-vector-machine-explained-8bfef2f17e71 https://medium.com/swlh/the-support-vector-machine-basic-concept-a5106bd3cc5f https://towardsdatascience.com/unlocking-the-true-power-of-support-vector-regression-847fd123a4a0#:~:text=Support%20Vector%20Regression%20is%20a,the%20maximum%20number%20of%20points. https://www.mygreatlearning.com/blog/introduction-to-support-vector-machine/ https://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
